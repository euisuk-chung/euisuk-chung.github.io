---
title: "[Paper Review] An Image Is Worth 16x16 Words : Transformers for Image Recognition at Scale (Vision Transformer)"
date: "2021-11-07"
year: "2021"
---

# [Paper Review] An Image Is Worth 16x16 Words : Transformers for Image Recognition at Scale (Vision Transformer)

ì„ ì • ì´ìœ 
=====

ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë…¼ë¬¸ë¦¬ë·°, ì½”ë“œë¦¬ë·°í•´ë³¼ ë…¼ë¬¸ì€ **"An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"** ë¡œ, ì»´í“¨í„° ë¹„ì „ì—ì„œ Transformerì™€ Attentionì´ ì“°ì´ê²Œ ëœ ê²°ì •ì  ê³„ê¸°(?)ê°€ ëœ ë…¼ë¬¸ì…ë‹ˆë‹¤. ìµœê·¼ ì´ìª½ ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ë‹¤ ë³´ë‹ˆ ì˜¤ëŠ˜ì€ ì´ ë…¼ë¬¸ì„ ë¦¬ë·°í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë…¼ë¬¸ë¦¬ë·°
====

Background
----------

### (Self) Attention

* Attentionì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë””ì½”ë”ì—ì„œ ì¶œë ¥ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë§¤ ì‹œì (time step)ë§ˆë‹¤, ì¸ì½”ë”ì—ì„œì˜ ì „ì²´ ì…ë ¥ ë¬¸ì¥ì„ ë‹¤ì‹œ í•œ ë²ˆ ì°¸ê³ í•©ë‹ˆë‹¤.
* ë‹¨, ì „ì²´ ì…ë ¥ ë¬¸ì¥ì„ ì „ë¶€ ë‹¤ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í•´ë‹¹ ì‹œì ì—ì„œ ì˜ˆì¸¡í•´ì•¼ í•  ë‹¨ì–´ì™€ ì—°ê´€ì´ ìˆëŠ” ì…ë ¥ ë‹¨ì–´ ë¶€ë¶„ì„ ì¢€ ë” ì§‘ì¤‘(attention)í•´ì„œ ë³´ê²Œ ë©ë‹ˆë‹¤.

![Q/K/V-1](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fe9f0ecf0-dd8a-4b55-9f89-a53723f9875c%2Fimage.png)

* ì£¼ì–´ì§„ 'ì¿¼ë¦¬(Query)'ì— ëŒ€í•´ì„œ ëª¨ë“  'í‚¤(Key)'ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•©ë‹ˆë‹¤. - ê·¸ë¦¬ê³  êµ¬í•´ë‚¸ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í•˜ì—¬ í‚¤ì™€ ë§µí•‘ë˜ì–´ ìˆëŠ” ê°ê°ì˜ 'ê°’(Value)'ì— ë°˜ì˜í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ 'ê°’(Value)'ì„ ëª¨ë‘ ê°€ì¤‘í•©í•˜ì—¬ ë¦¬í„´í•˜ê²Œ ë©ë‹ˆë‹¤.
* 'ì¿¼ë¦¬(Query)', 'í‚¤(Key)', 'ê°’(Value)'ì˜ ì •ì˜ëŠ” ì˜ì–´ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![Q/K/V-2](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F016b38e9-b646-45a5-98cc-a4be3c33024d%2Fimage.png)

![Q/K/V-3](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fe4da381c-0ab4-42e3-83cf-1ecb3d80193a%2Fimage.png)

### Transformer

* ê¸°ì¡´ RNN ê¸°ë°˜ seq2seq ëª¨ë¸ì—ì„œëŠ” ì´ì „ ì‹œì ì˜ ì—°ì‚°ì´ ëë‚˜ê¸° ì „ì—ëŠ” ë‹¤ìŒ ì‹œì ì˜ ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ì—¬ ë³‘ë ¬í™”(parallelize) ëœ ì—°ì‚°ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.
* RNNêµ¬ì¡°ëŠ” ê³ ì§ˆì  ë¬¸ì œì¸ Long-term dependency ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ê³ , ì´ëŠ” ê³§ íƒ€ì„ ìŠ¤í…(time step)ì´ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
* ì´ëŸ¬í•œ ë¬¸ì œì ë“¤ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ Attentionë§Œìœ¼ë¡œ Encoder, Decoder êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•¨ìœ¼ë¡œì¨ í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë©° ì„±ëŠ¥ë„ ìš°ìˆ˜í•œ Transformerê°€ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

![Encoder/Decoder](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fc9a1d255-e92e-42ef-9b4e-4096b483ff0c%2Fimage.png)

* ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ê°œì˜ Headë¥¼ ì‚¬ìš©í•˜ëŠ” Multi-head Attentionì„ í†µí•´ ë‹¤ì–‘í•œ aspectì— ëŒ€í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

![MHSA](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F19260b3f-3f18-4c03-90dc-5542f22a3dff%2Fimage.png)

### Image Recognition (Classification)

* Image Recognition (Classification)ì€ ì´ë¯¸ì§€ë¥¼ ì•Œê³ ë¦¬ì¦˜ì— ì…ë ¥(input)í•´ì£¼ë©´, ê·¸ ì´ë¯¸ì§€ê°€ ì†í•˜ëŠ” class lableì„ ì¶œë ¥(output)í•´ì£¼ëŠ” taskë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
* ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ê³ ì–‘ì´ ì‚¬ì§„ì„ ë„£ì–´ì£¼ë©´ ê³ ì–‘ì´ ë¼ê³  ì¸ì‹(ë¶„ë¥˜)í•´ëƒ…ë‹ˆë‹¤.  
  ![Image Classification](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F7259726b-7c20-4282-89e6-f867eb3dc394%2Fimage.png)
* ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ ì—¬ì§€ê» CV(Computer Vision) ë„ë©”ì¸ì—ì„œëŠ” CNN(Convolutional Neural Network)ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ë“¤ì´ ë§ì´ ì‚¬ìš©ë˜ì–´ ì˜¤ê³  ìˆì—ˆìŠµë‹ˆë‹¤. (Ex. ResNet, UNet, EfficientNet ë“±)
* í•˜ì§€ë§Œ, NLP(Natural Language Processing) ë„ë©”ì¸ì—ì„œì˜ Self-Attentionê³¼ Transformerì˜ ì„±ì¥ìœ¼ë¡œ ì¸í•´ CNNê³¼ Attentionì„ í•¨ê»˜ ì´ìš©í•˜ë ¤ëŠ” ì¶”ì„¸ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸(ì—°êµ¬) ì—­ì‹œ ê·¸ëŸ¬í•œ ì‹œë„ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

Vision Transformer
------------------

* Vision Transformerì˜ ê°œë…ì€ Transformerê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì•„ëŠ” ì‚¬ëŒë“¤ì´ë¼ë©´ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì´ ì†”ì§íˆ ë³¸ ë…¼ë¬¸ì— ì „ë¶€ì´ê¸° ë•Œë¬¸ì´ì£ .

![ViT](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Ff1b3922c-88d8-4a18-ad4c-08256911ecce%2Fimage.png)

* Vision TransformerëŠ” image recognition taskì— ìˆì–´ì„œ Convolutionì„ ì•„ì˜ˆ ì—†ì• ê³ , Transformer Encoderë§Œì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

> (ì¶”ê¸°) ì•„ë˜ ViT ì‹œê°í™” ìë£Œê°€ ë§¤ìš° ìœ ìµí•´ì„œ ê¸€ ë³¸ë¬¸ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.  
> <https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html?utm_source=pytorchkr&ref=pytorchkr>

ê°ê°ì˜ ìˆœì„œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

### 0. Prerequisites

* í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ IMPORT

```
import torch
import torch.nn as nn
from torch import Tensor
from torchsummary import summary

import torch.optim as optim
from torch.optim import lr_scheduler

import torchvision.datasets as dset
import torchvision.transforms as T
from torchvision.transforms import Compose, Resize, ToTensor
from torch.utils.data import DataLoader
from torch.utils.data import sampler

import numpy as np
import os
import copy
import cv2
import matplotlib.pyplot as plt
from tqdm import tqdm

%pip install einops
from einops import rearrange, repeat, reduce
from einops.layers.torch import Rearrange, Reduce

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dtype = torch.long
```

* Define Helper Function

```
def pair(t):
    return t if isinstance(t, tuple) else (t, t)
```

![Transformer](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F245869c9-6ca7-40dd-bc3e-175f7c574766%2Fimage.png)

* Define `PreNorm` Class

```
# Define PreNorm
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
        
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)
```

* Define `FeedForward` Class

```
# Define FeedForward
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)
```

* Define `Attention` Class

```
# Define Attention
class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)

        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale

        attn = self.attend(dots)

        out = einsum('b h i j, b h j d -> b h i d', attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)
```

* Define `Transformer` Class

```
# Define Transformer
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x
```

> **SOURCE**  
> *all the images are from this [awesome blog - blog.mdturp.ch, visual\_guide\_to\_vision\_transformer](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html?utm_source=pytorchkr&ref=pytorchkr) !!*

### step 1. Splitting Image into fixed-size patches

* ê°€ì¥ ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ê³ ì •ëœ ì‚¬ì´ì¦ˆì˜ íŒ¨ì¹˜ë“¤ë¡œ ë¶„í• í•˜ì—¬ ëª¨ë¸ì— ë„£ì–´ì¤ë‹ˆë‹¤.

![ViT1](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F28c981e3-2d9e-44a6-917d-5bc356ae4e4b%2Fimage.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/78208416-1369-43fd-8160-96f03de1af8a/image.png)

### step 2. Linearly embed each patches

* ê°ê°ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì— ëŒ€í•´ Linear Embeddingì„ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤. (Dì°¨ì›ìœ¼ë¡œ)

![ViT2](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F706a2327-a376-496f-8314-7add1c8ca3f0%2Fimage.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/c70d3657-22b4-42c4-860d-f92363e02601/image.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/d04b2711-7a7b-4cde-8309-355cd8b2d3b9/image.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/f9ce898f-05c4-4b26-beef-e4f3384e8689/image.png)

### step 3. Add positional embedding

* ì´ì œ ê°ê°ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì´ ì–´ë–¤ ìœ„ì¹˜ì— ìˆëŠ”ê°€ì— ëŒ€í•œ ì •ë³´ë„ ëª¨ë¸ì— ë„£ì–´ì£¼ì–´ì•¼ê² ì£ ? ì´ëŸ° ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ìš°ë¦¬ëŠ” position embeddingì´ë¼ê³  í•˜ë©°, ì•ì—ì„œ êµ¬í•œ Embeddingì— ë¶™ì—¬ì£¼ê²Œ ë©ë‹ˆë‹¤.

![ViT3](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F9b45af09-1a4d-4c33-8e95-18f26af9b5ea%2Fimage.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/7ada6856-4856-4cbe-980b-71c58ab03ced/image.png)

> (ì°¸ê³ ) ViT(Vision Transformer)ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ êµ¬ì„±ê³¼ ì‘ë™ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

**1. ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„± ìˆœì„œ (step2)**

ViT ëª¨ë¸ì€ [CLS] í† í°ì„ ì´ë¯¸ì§€ íŒ¨ì¹˜ ì„ë² ë”© ì‹œí€€ìŠ¤ì˜ ê°€ì¥ ì•ì— **ì—°ê²°(Concatenate)**í•©ë‹ˆë‹¤.

InputÂ Sequence=[CLS\_Token,Patch1,Patch2,Patch3,â€¦,PatchN]\text{Input Sequence} = [\text{CLS\\_Token}, \text{Patch}\_1, \text{Patch}\_2, \text{Patch}\_3, \dots, \text{Patch}\_N]InputÂ Sequence=[CLS\_Token,Patch1â€‹,Patch2â€‹,Patch3â€‹,â€¦,PatchNâ€‹]

**2. ìœ„ì¹˜ ì„ë² ë”©(Positional Embedding) ì ìš© (step3)**

ì´í›„, ìœ„ì—ì„œ êµ¬ì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤ì˜ ê° ìš”ì†Œì— **ìœ„ì¹˜ ì„ë² ë”©**ì´ ë”í•´ì§‘ë‹ˆë‹¤.

### step 4. Feed embedding vector into Transformer Encoder

* ê°ê°ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì— ëŒ€í•œ ìœ„ì¹˜ ì •ë³´ì™€ ì„ë°°ë”© ê°’ì„ Transformer Encoderë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. Transformer EncoderëŠ” ì•„ë˜ ê·¸ë¦¼(ìš°ì¸¡)ê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

![ViT4](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F295f5097-2a7b-431e-a02a-0f3fd660acb7%2Fimage.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/5ea11e4f-d38b-4a03-90a0-69449c89bbda/image.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/bc751a4f-a039-48ba-b0fd-0aadbe453ba9/image.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/45f11e59-f0e9-4584-ba24-36e80d9752df/image.png)

### step 5. Use [CLS] token for Classification

* ì´ë¯¸ ëˆˆì¹˜ ì±„ì‹  ë¶„ë“¤ë„ ìˆê² ì§€ë§Œ, Transformer Encoderì— ë“¤ì–´ê°„ ê°ê°ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì— ëŒ€í•œ ìœ„ì¹˜ ì •ë³´ì™€ ì„ë°°ë”© ê°’ ì™¸ì—ë„ ì•ì— `[0,*]`ì´ ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆëŠ” ë° ì´ëŠ” ì „ì²´ ì´ë¯¸ì§€ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” í† í°ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (A.K.A. `[CLS]`í† í°) ì´ë²ˆ ë‹¨ê³„ì—ì„œëŠ” ì´ëŸ¬í•œ `[CLS]`í† í°ì„ ì‚¬ìš©í•˜ì—¬ MLP(Multi-layer Perceptron)ì— íƒœì›Œ Classificatinì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.

![ViT5](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F62d51244-78dc-4d53-bca4-adc30f6a50db%2Fimage.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/b1dc39ee-9b1a-48f4-8743-8e9089b2ae62/image.png)

> ğŸ“Œ ì—¬ê¸°ì„œ ì ê¹! `einops` ë¼ì´ë¸ŒëŸ¬ë¦¬?
>
> * Einstein notation ì€ ë³µì¡í•œ í…ì„œ ì—°ì‚°ì„ í‘œê¸°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì—ì„œ ì“°ì´ëŠ” ë§ì€ ì—°ì‚°ì€ Einstein notation ìœ¼ë¡œ ì‰½ê²Œ í‘œê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> * einops (<https://github.com/arogozhnikov/einops>)ëŠ” pytorch, tensorflow ë“± ì—¬ëŸ¬ í”„ë ˆì„ì›Œí¬ë¥¼ ë™ì‹œì— ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì´ëŸ¬í•œ Einstein notationì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œí•©ë‹ˆë‹¤.

> ğŸ“Œ ì—¬ê¸°ì„œ ì ê¹! `Rearrange` í•¨ìˆ˜?
>
> * Rearrange í•¨ìˆ˜ëŠ” shapeë¥¼ ì‰½ê²Œ ë³€í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.
> * ë°‘ì— ê·¸ë¦¼ìœ¼ë¡œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•´ë³´ì‹œì£ !  
>   ![Rearrangeí•¨ìˆ˜](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F72adaaff-86c1-4a07-a7ad-db1f003ecbb4%2Fimage.png)

> ğŸ“Œ ì—¬ê¸°ì„œ ì ê¹! `einsum` í•¨ìˆ˜?
>
> * Einsum í‘œê¸°ë²•ì€ íŠ¹ìˆ˜í•œ Domain Specific Languageë¥¼ ì´ìš©í•´ ì´ ëª¨ë“  í–‰ë ¬, ì—°ì‚°ì„ í‘œê¸°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
> * ì‰½ê²Œ ë§í•´ ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ í–‰ë ¬ ì—°ì‚°ì„ ì§ê´€ì ìœ¼ë¡œ ì •ì˜í•´ì„œ êµ¬í•˜ê²Œ í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
> * ëª‡ ê°€ì§€ ì˜ˆì‹œë¡œ ì‚´í´ë³´ì‹œì£  (given X(matrix), Y(matrix))  
>   - Transpose : `np.einsum("ij->ji", X)`  
>   - Matrix sum : `np.einsum("ij->", X)`  
>   - Matrix row sum : `np.einsum("ij->i", X)`  
>   - Matrix col sum : `np.einsum("ij->j", X)`  
>   - Matrix Multiplication : `np.einsum('ij,j->i', X, Y)`  
>   - Batched Matrix Multiplication : `np.einsum('bik,bkj->bij', X, Y)`

### Vision Transformer ì½”ë“œ

```
# ViT Class
class ViT(nn.Module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__() # super()ë¡œ ê¸°ë°˜ í´ë˜ìŠ¤ì˜ __init__ ë©”ì„œë“œ í˜¸ì¶œ

        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)

        # assert ë¬¸ : ë’¤ì˜ ì¡°ê±´ì´ Trueê°€ ì•„ë‹ˆë©´ AssertErrorë¥¼ ë°œìƒ
        # patch size ì¡°ê±´
        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width

        # pooling ì¡°ê±´ 
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'

        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width), # 3 -> 2
            nn.Linear(patch_dim, dim), # Linear Projection
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) # position embedding ì´ˆê¸°í™”
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) # [CLS] í† í° ì´ˆê¸°í™”
        self.dropout = nn.Dropout(emb_dropout) # Dropout ì •ì˜

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) # Transformer ì„ ì–¸

        self.pool = pool
        self.to_latent = nn.Identity() # ëœë¤ì„± ì œê±°

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, img):
        x = self.to_patch_embedding(img)
        b, n, _ = x.shape

        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)
        
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)

        x = self.transformer(x)

        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        x = self.to_latent(x)
        
        return self.mlp_head(x)
```

Reference
=========

1. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2019 - Alexey Dosovitskiy et. al.
2. ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ - ìœ ì›ì¤€ ì™¸ 1ì¸ (<https://wikidocs.net/book/2155>)
3. The Illustrated Transformer -  
   Jay Alammar (<https://jalammar.github.io/illustrated-transformer>)
4. ViT Source Code - lucidrains (<https://github.com/lucidrains/vit-pytorch/blob/64a2ef6462bde61db4dd8f0887ee71192b273692/vit_pytorch/vit.py>)
5. <https://discuss.pytorch.kr/t/vision-transformer-a-visual-guide-to-vision-transformers/4158>