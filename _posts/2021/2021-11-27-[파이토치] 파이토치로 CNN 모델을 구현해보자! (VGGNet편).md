---
title: "[íŒŒì´í† ì¹˜] íŒŒì´í† ì¹˜ë¡œ CNN ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ì! (VGGNetí¸)"
date: "2021-11-27"
tags:
  - "PyTorch"
  - "ë”¥ëŸ¬ë‹"
year: "2021"
---

# [íŒŒì´í† ì¹˜] íŒŒì´í† ì¹˜ë¡œ CNN ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ì! (VGGNetí¸)

ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í¬ìŠ¤íŒ…ë¶€í„° ë‹¤ìŒë‹¤ìŒ í¬ìŠ¤íŒ…ê¹Œì§€ëŠ” CNN ëª¨ë¸ì˜ ë¼ˆëŒ€ê°€ ë˜ëŠ” ëª¨ë¸ë“¤ì¸ VGGNet, GoogleNet, ResNetì„ ì†Œê°œí•˜ê³  ì´ë¥¼ êµ¬í˜„í•´ë³´ëŠ” ì‹œê°„ì„ ê°–ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤! :) ì´ë²ˆ í¬ìŠ¤íŒ…ì€ `VGGNet` ê´€ë ¨ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ë¨¼ì € `ILSVRC (Imagenet Large Scale Visual Recognition Challenges)`ì´ë¼ëŠ” ëŒ€íšŒê°€ ìˆëŠ”ë°, ë³¸ ëŒ€íšŒëŠ” ê±°ëŒ€ ì´ë¯¸ì§€ë¥¼ 1000ê°œì˜ ì„œë¸Œì´ë¯¸ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ CNNêµ¬ì¡°ì˜ ëŒ€ì¤‘í™”ë¥¼ ì´ëŒì—ˆë˜ ì´ˆì°½ê¸° ëª¨ë¸ë“¤ë¡œ AlexNet (2012) - VGGNet (2014) - GoogleNet (2014) - ResNet (2015) ìˆœìœ¼ë¡œ ê³„ë³´ë¥¼ ì´ì–´ë‚˜ê°”ìŠµë‹ˆë‹¤.

![ILSVRC](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fb7f6176e-a9b9-435b-9d26-859a241b58df%2Fimage.png)

> Source : <https://icml.cc/2016/tutorials/>

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ layersëŠ” CNN layerì˜ ê°œìˆ˜(ê¹Šì´)ë¥¼ ì˜ë¯¸í•˜ë©° ì§ê´€ì ì¸ ì´í•´ë¥¼ ìœ„í•´ì„œ ì•„ë˜ì²˜ëŸ¼ ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì•˜ìŠµë‹ˆë‹¤.

![Depth Comp](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F83fe8361-b80d-487f-a355-65cbe8c30a05%2Fimage.png)

VGGNet ê°œìš”
---------

### ì†Œê°œ

VGGNetì´ ì†Œê°œëœ ë…¼ë¬¸ì˜ ì œëª©ì€ Very deep convolutional networks for large-scale image recognitionë¡œ, ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ë§í¬](https://arxiv.org/pdf/1409.1556.pdf)

![ì¸ìš©](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F25aa15ea-85ca-49b0-9145-ee0b140b1bae%2Fimage.png)

VGGNetì€ ì‹ ê²½ë§ì˜ ê¹Šì´ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ í•´ë‹¹ ì—°êµ¬ë¥¼ ì‹œì‘í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ 3x3 convolutionì„ ì´ìš©í•œ Deep CNNsë¥¼ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. VGGNetì€ ILSVRC-2014 ëŒ€íšŒì—ì„œ GoogLeNetì— ì´ì–´ 2ë“±ì„ ì°¨ì§€í•˜ì˜€ìœ¼ë‚˜, GoogLeNetì— ë¹„í•´ í›¨ì”¬ ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì¸í•´ 1ë“±ì¸ ëª¨ë¸ë³´ë‹¤ ë”ìš± ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆë‹¤ëŠ” íŠ¹ì§•ì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤.

### ì‹¤í—˜ì„¤ê³„

ëª¨ë¸ì€ 3x3 convolution, Max-pooling, Fully Connected Network 3ê°€ì§€ ì—°ì‚°ìœ¼ë¡œë§Œ êµ¬ì„±ì´ ë˜ì–´ ìˆìœ¼ë©° ì•„ë˜ í‘œì™€ ê°™ì´ A, A-LRN, B, C, D, E 5ê°€ì§€ ëª¨ë¸ì— ëŒ€í•´ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

![VGG](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F88ce391b-98dd-48aa-b214-c454c212ef69%2Fimage.png)

ì´ë•Œ ì‚¬ìš©í•œ ê°ê°ì˜ window\_sizeì™€ activation functionì˜ ì„¤ì •ì„ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

* 3x3 convolution filters (stride: 1)
* 2x2 Max pooling (stride : 2)
* Activation function : ReLU

> ğŸ“¢ ì—¬ê¸°ì„œ ì ê¹!  
> ìœ„ í‘œì—ì„œ conv3-64ë¼ê³  ì¨ìˆëŠ” ê²ƒì€ 3x3ì˜ window\_sizeë¥¼ ê°–ê³  ì‚¬ìš©í•œ windowì˜ ê°œìˆ˜ê°€ 64ê°œì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### ì„±ëŠ¥

ì•„ë˜ ì„±ëŠ¥í‘œë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ê¹Šì´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒê³¼ Local Response Normalization(LRN)ì€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Eval](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Ffd326bed-9335-4eb7-95fa-20310b0b62de%2Fimage.png)

VGGNet êµ¬í˜„
---------

ê·¸ëŸ¼ VGGNetì˜ ê°œìš”ë¥¼ ì‚´í´ë´¤ìœ¼ë‹ˆ ì´ë²ˆì—ëŠ” ì´ë¥¼ êµ¬í˜„í•´ë³¼ê¹Œìš”? êµ¬í˜„ì€ ìœ„ ì‹¤í—˜ ì„¤ê³„ í‘œì˜ Dì—´ì˜ ì…‹íŒ…ì„ êµ¬í˜„í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì¤„ê¸€ë¡œ í•´ë‹¹ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ìë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

* 3x3 í•©ì„±ê³± ì—°ì‚° x2 (ì±„ë„ 64)
* 3x3 í•©ì„±ê³± ì—°ì‚° x2 (ì±„ë„ 128)
* 3x3 í•©ì„±ê³± ì—°ì‚° x3 (ì±„ë„ 256)
* 3x3 í•©ì„±ê³± ì—°ì‚° x3 (ì±„ë„ 512)
* 3x3 í•©ì„±ê³± ì—°ì‚° x3 (ì±„ë„ 512)
* FC layer x3  
  - FC layer 4096
  + FC layer 4096
  + FC layer 1000

![VGG16](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Faa87980c-45f6-4f48-8136-4356a58822a6%2Fimage.png)

ì½”ë”©ì˜ í¸ì˜ë¥¼ ìœ„í•´ ê°ê° conv layerê°€ 2ê°œ ìˆëŠ” blockê³¼ 3ê°œ ìˆëŠ” blockì„ ë”°ë¡œ ì„ ì–¸í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

### conv\_2\_block

```
def conv_2_block(in_dim,out_dim):
    model = nn.Sequential(
        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2,2)
    )
    return model
```

### conv\_3\_block

```
def conv_3_block(in_dim,out_dim):
    model = nn.Sequential(
        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2,2)
    )
    return model
```

### Define VGG16

```
class VGG(nn.Module):
    def __init__(self, base_dim, num_classes=10):
        super(VGG, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3,base_dim), #64
            conv_2_block(base_dim,2*base_dim), #128
            conv_3_block(2*base_dim,4*base_dim), #256
            conv_3_block(4*base_dim,8*base_dim), #512
            conv_3_block(8*base_dim,8*base_dim), #512        
        )
        self.fc_layer = nn.Sequential(
            # CIFAR10ì€ í¬ê¸°ê°€ 32x32ì´ë¯€ë¡œ 
            nn.Linear(8*base_dim*1*1, 4096),
            # IMAGENETì´ë©´ 224x224ì´ë¯€ë¡œ
            # nn.Linear(8*base_dim*7*7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 1000),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(1000, num_classes),
        )

    def forward(self, x):
        x = self.feature(x)
        #print(x.shape)
        x = x.view(x.size(0), -1)
        #print(x.shape)
        x = self.fc_layer(x)
        return x
```

### model, loss, optimizer ì„ ì–¸

```
# device ì„¤ì •
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# VGG í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”
model = VGG(base_dim=64).to(device)

# ì†ì‹¤í•¨ìˆ˜ ë° ìµœì í™”í•¨ìˆ˜ ì„¤ì •
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

### load CIFAR10 dataset

* CIFAR10ì€ â€˜ë¹„í–‰ê¸°(airplane)â€™, â€˜ìë™ì°¨(automobile)â€™, â€˜ìƒˆ(bird)â€™, â€˜ê³ ì–‘ì´(cat)â€™, â€˜ì‚¬ìŠ´(deer)â€™, â€˜ê°œ(dog)â€™, â€˜ê°œêµ¬ë¦¬(frog)â€™, â€˜ë§(horse)â€™, â€˜ë°°(ship)â€™, â€˜íŠ¸ëŸ­(truck)â€™ë¡œ 10ê°œì˜ í´ë˜ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
* CIFAR10ì— í¬í•¨ëœ ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” `3x32x32`ë¡œ, ì´ëŠ” `32x32` í”½ì…€ í¬ê¸°ì˜ ì´ë¯¸ì§€ê°€ 3ê°œ ì±„ë„(channel)ì˜ ìƒ‰ìƒë¡œ ì´ë¤„ì ¸ ìˆë‹¤ëŠ” ê²ƒì„ ëœ»í•©ë‹ˆë‹¤.

**TRAIN/TEST ë°ì´í„°ì…‹ ì •ì˜**

```
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Transform ì •ì˜
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# CIFAR10 TRAIN ë°ì´í„° ì •ì˜
cifar10_train = datasets.CIFAR10(root="../Data/", train=True, transform=transform, target_transform=None, download=True)

# CIFAR10 TEST ë°ì´í„° ì •ì˜
cifar10_test = datasets.CIFAR10(root="../Data/", train=False, transform=transform, target_transform=None, download=True)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

**TRAIN ë°ì´í„°ì…‹ ì‹œê°í™”**

```
import matplotlib.pyplot as plt
import numpy as np

# ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•œ í•¨ìˆ˜

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# í•™ìŠµìš© ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¤ê¸°
dataiter = iter(train_loader)
images, labels = dataiter.next()

# ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°
imshow(torchvision.utils.make_grid(images))

# ì •ë‹µ(label) ì¶œë ¥
print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))
```

> Source : <https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>

![ì‹œê°í™”](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F006eb0ee-0f82-4852-bb97-aae9a61ae5cd%2Fimage.png)

### TRAIN & TEST

ì´ì œ ë°ì´í„°ì…‹ë„ ì •ì˜í•´ì¤¬ìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ í•™ìŠµ ë° ê²€ì¦ì„ ìˆ˜í–‰í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. í•™ìŠµ ì„¤ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.

```
batch_size = 100
learning_rate = 0.0002
num_epoch = 100
```

**TRAIN**

```
loss_arr = []
for i in trange(num_epoch):
    for j,[image,label] in enumerate(train_loader):
        x = image.to(device)
        y_= label.to(device)
        
        optimizer.zero_grad()
        output = model.forward(x)
        loss = loss_func(output,y_)
        loss.backward()
        optimizer.step()

    if i % 10 ==0:
        print(loss)
        loss_arr.append(loss.cpu().detach().numpy())
```

**loss ì‹œê°í™”**

```
plt.plot(loss_arr)
plt.show()
```

![loss](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F54daab90-b8c3-4c23-a3d5-5ff8c39966b1%2Fimage.png)

**test ê²°ê³¼**

```
# ë§ì€ ê°œìˆ˜, ì „ì²´ ê°œìˆ˜ë¥¼ ì €ì¥í•  ë³€ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
correct = 0
total = 0

model.eval()

# ì¸í¼ëŸ°ìŠ¤ ëª¨ë“œë¥¼ ìœ„í•´ no_grad í•´ì¤ë‹ˆë‹¤.
with torch.no_grad():
    # í…ŒìŠ¤íŠ¸ë¡œë”ì—ì„œ ì´ë¯¸ì§€ì™€ ì •ë‹µì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    for image,label in test_loader:
        
        # ë‘ ë°ì´í„° ëª¨ë‘ ì¥ì¹˜ì— ì˜¬ë¦½ë‹ˆë‹¤.
        x = image.to(device)
        y= label.to(device)

        # ëª¨ë¸ì— ë°ì´í„°ë¥¼ ë„£ê³  ê²°ê³¼ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
        output = model.forward(x)
        _,output_index = torch.max(output,1)

        
        # ì „ì²´ ê°œìˆ˜ += ë¼ë²¨ì˜ ê°œìˆ˜
        total += label.size(0)
        correct += (output_index == y).sum().float()
    
    # ì •í™•ë„ ë„ì¶œ
    print("Accuracy of Test Data: {}%".format(100*correct/total))
```

> Accuracy of Test Data: 82.33999633789062%

ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ ^~^