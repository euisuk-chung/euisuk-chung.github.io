---
title: "[νμ΄ν† μΉ] νμ΄ν† μΉ κΈ°μ΄ μ”μ† (Autogradλ€)"
date: "2021-09-16"
tags:
  - "PyTorch"
  - "κ°λ…μ •λ¦¬"
year: "2021"
---

# [νμ΄ν† μΉ] νμ΄ν† μΉ κΈ°μ΄ μ”μ† (Autogradλ€)

μμ „νμ™€ μ—­μ „ν
--------

μ‹ κ²½λ§(Neural Network)μ€ μ–΄λ–¤ μ…λ ¥ λ°μ΄ν„°μ— λ€ν•΄ μ‹¤ν–‰λλ” μ¤‘μ²©λ ν•¨μλ“¤μ μ§‘ν•©μ²΄μ…λ‹λ‹¤. μ‹ κ²½λ§μ„ μ•„λ 2λ‹¨κ³„λ¥Ό κ±°μ³ ν•™μµλ©λ‹λ‹¤ :

1. μμ „ν(Forward Propagation)
2. μ—­μ „ν(Backward Propagation)

![NN-Forward-Backward](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F8585ff31-b35a-4e8f-a3f4-f8e538c9ff8d%2Fimage.png)

### Forward Propagation (μμ „ν)

**Forward Propagation(μμ „ν)** λ‹¨κ³„μ—μ„, μ‹ κ²½λ§μ€ μ •λ‹µμ„ λ§μ¶”κΈ° μ„ν•΄ μµμ„ μ μ¶”μΈ΅(best guess)μ„ ν•©λ‹λ‹¤. μ΄λ ‡κ² μ¶”μΈ΅μ„ ν•κΈ° μ„ν•΄μ„ μ…λ ¥ λ°μ΄ν„°λ¥Ό κ° ν•¨μλ“¤μ—μ„ μ‹¤ν–‰ν•©λ‹λ‹¤.

### Back Propagation (μ—­μ „ν)

**Back Propagation (μ—­μ „ν)** λ‹¨κ³„μ—μ„, μ‹ κ²½λ§μ€ μ¶”μΈ΅ν• κ°’μ—μ„ λ°μƒν• errorμ— λΉ„λ΅€ν•μ—¬ νλΌλ―Έν„°λ“¤μ„ μ μ ν μ—…λ°μ΄νΈν•©λ‹λ‹¤. μ¶λ ¥(output)λ΅λ¶€ν„° μ—­λ°©ν–¥μΌλ΅ μ΄λ™ν•λ©΄μ„ μ¤λ¥μ— λ€ν• ν•¨μλ“¤μ λ§¤κ°λ³€μλ“¤μ λ―Έλ¶„κ°’(gradient)μ„ μμ§‘ν•κ³ , κ²½μ‚¬ν•κ°•λ²•(gradient descent)μ„ μ‚¬μ©ν•μ—¬ λ§¤κ°λ³€μλ“¤μ„ μµμ ν™” ν•©λ‹λ‹¤.

### λ‰΄λ΄λ„¤νΈμ›ν¬ ν•™μµ μ•κ³ λ¦¬μ¦

1. λ¨λ“  κ°€μ¤‘μΉ wλ¥Ό μ„μλ΅ μƒμ„±  
   **[Forward Propagation]**
2. μ…λ ¥λ³€μ κ°’κ³Ό μ…λ ¥μΈµκ³Ό μ€λ‹‰μΈµ μ‚¬μ΄μ wκ°’μ„ μ΄μ©ν•μ—¬ μ€λ‹‰λ…Έλ“μ κ°’μ„ κ³„μ‚°  
   (μ„ ν•κ²°ν•© ν›„ activationν• κ°’)
3. μ€λ‹‰λ…Έλ“μ κ°’κ³Ό μ€λ‹‰μΈµκ³Ό μ¶λ ¥μΈµ μ‚¬μ΄μ wκ°’μ„ μ΄μ©ν•μ—¬ μ¶λ ¥λ…Έλ“μ κ°’μ„ κ³„μ‚°  
   (μ„ ν•κ²°ν•© ν›„ activationν• κ°’)  
   **[Back Propagation]**
4. κ³„μ‚°λ μ¶λ ¥λ…Έλ“μ κ°’κ³Ό μ‹¤μ  μ¶λ ¥λ³€μμ κ°’μ μ°¨μ΄λ¥Ό μ¤„μΌ μ μλ„λ΅ μ€λ‹‰μΈµκ³Ό μ¶λ ¥μΈµ μ‚¬μ΄μ wκ°’μ„ μ—…λ°μ΄νΈ
5. κ³„μ‚°λ μ¶λ ¥λ…Έλ“μ κ°’κ³Ό μ‹¤μ  μ¶λ ¥λ³€μμ κ°’μ μ°¨μ΄λ¥Ό μ¤„μΌ μ μλ„λ΅ μ…λ ¥μΈµκ³Ό μ€λ‹‰μΈµ μ‚¬μ΄μ wκ°’μ„ μ—…λ°μ΄νΈ
6. μ—λ¬κ°€ μ¶©λ¶„ν μ¤„μ–΄λ“¤ λ•κΉμ§€ 2λ² ~ 5λ²μ„ λ°λ³µ

Autograd κ°λ…
-----------

pyTorchλ¥Ό μ΄μ©ν•΄ μ½”λ“λ¥Ό μ‘μ„±ν• λ• μ΄λ¬ν• μ—­μ „νλ¥Ό ν†µν•΄ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•λ” λ°©λ²•μ€ λ°”λ΅  Autograd μ…λ‹λ‹¤. μ°¨κ·Όμ°¨κ·Ό μ½”λ“λ¥Ό ν†µν•΄ μ•μ•„λ³΄λ„λ΅ ν•©μ‹λ‹¤. Autogradμ— λ€ν•΄ μ‚΄ν΄λ³΄κΈ° μ„ν•΄ κ°„λ‹¨ν• MLP(Mulyi-Layer Perceptron)μ„ μμ‹λ΅ μ‚΄ν΄λ³ΌκΉμ”?

### import torch

λ¨Όμ € pyTorchλ¥Ό μ‚¬μ©ν•κΈ° μ„ν•΄μ„ λ‹¤μκ³Ό κ°™μ΄ pyTorchλ¥Ό importν•΄μ¤λ‹λ‹¤. μ΄λ•, `torch.cuda`μ `is_available()`ν•¨μλ¥Ό ν†µν•΄ ν„μ¬ νμ΄μ¬μ΄ μ‹¤ν–‰λκ³  μλ” ν™κ²½μ΄ GPUλ¥Ό μ΄μ©ν•΄μ„ κ³„μ‚°μ„ ν• μ μλ”κ°€λ¥Ό ν™•μΈν•  μ μμµλ‹λ‹¤.

π’» μ½”λ“

```
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device
```

π’» κ²°κ³Ό

```
# GPU μ‚¬μ©μ΄ κ°€λ¥ν•  λ•
device(type='cuda')

# GPU μ‚¬μ©μ΄ λ¶κ°€λ¥ν•  λ•
device(type='cuda')
```

### BATCH\_SIZE

BATCH\_SIZEλ” λ”¥λ¬λ‹ λ¨λΈμ΄ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•  λ• κ³„μ‚°λλ” λ°μ΄ν„° λ¬¶μμ κ°μμ…λ‹λ‹¤. μ•μ—μ„ Neural Networkμ΄ Forward Propagation(μμ „ν)μ™€ Backward Propagation(μ—­μ „ν)λ¥Ό μν–‰ν•λ©΄μ„ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈλ¥Ό ν•λ‹¤κ³  μ†κ° λ“λ Έλ”λ°, μ΄λ¬ν• μ—…λ°μ΄νΈλ¥Ό μν–‰ν•λ” λ° μ‚¬μ©λλ” λ°μ΄ν„° λ‹¨μ„(κ°―μ)κ°€ λλ” κ²ƒμ΄ λ°”λ΅ BATCH\_SIZEμ…λ‹λ‹¤. μ•„λ μμ‹μ—μ„ BATCH\_SIZEλ΅ 32λ¥Ό μ§€μ •ν•΄μ¤¬λ”λ°, μ΄λ” μ½”λ“ μ‘μ„±μ λ§μλ€λ΅(?) μ •ν•΄μ£Όλ” ν•μ΄νΌνλΌλ―Έν„°μ…λ‹λ‹¤.

π’» μ½”λ“

```
# ν•μ΄νΌνλΌλ―Έν„° μ§€μ •
BATCH_SIZE = 32
```

### INPUT\_SIZE, HIDDEN\_SIZE, OUTPUT\_SIZE, LEARNING\_RATE

![Input_Hidden_Ouput](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F38054636-dab5-4ac6-b287-3c601ebf3a14%2Fimage.png)

* `INPUT_SIZE`λ” λ”¥λ¬λ‹ λ¨λΈμ μ…λ ¥κ°’μ ν¬κΈ°μ΄λ©°, μ…λ ¥μΈµμ λ…Έλ“ μλ¥Ό μλ―Έν•©λ‹λ‹¤.
* `HIDDEN_SIZE`λ” μ…λ ¥κ°’μ— λ‹¤μμ νλΌλ―Έν„°λ¥Ό μ‚¬μ©ν•μ—¬ κ³„μ‚°λλ” κ°’μ κ°μλ΅, μ€λ‹‰ μΈµμ λ…Έλ“ μλ¥Ό μλ―Έν•©λ‹λ‹¤.
* `OUTPUT_SIZE`λ” μ€λ‹‰κ°’μ— λ‹¤μμ νλΌλ―Έν„°λ¥Ό μ‚¬μ©ν•μ—¬ κ³„μ‚°λλ” κ²°κ³Όκ°’μ κ°μλ΅, μ¶λ ¥ μΈµμ λ…Έλ“ μλ¥Ό μλ―Έν•©λ‹λ‹¤.
* `LEARNING_RATE`μ€ Gradientλ¥Ό μ—…λ°μ΄νΈν•  λ• κ³±ν•΄μ£Όλ” 0κ³Ό 1μ‚¬μ΄μ— μ΅΄μ¬ν•λ” κ°’μ…λ‹λ‹¤. μΆ€ λ” λλ¦¬μ§€λ§ μ„¬μ„Έν•κ³  μ΄μ΄ν μ—…λ°μ΄νΈλ¥Ό μ›ν•λ©΄ μ‘μ€ rateμ„, μΆ€ λ” λΉ λ¥΄κ² μ—…λ°μ΄νΈλ¥Ό μ›ν•λ©΄ ν° rateλ¥Ό μ¤„ μ μμµλ‹λ‹¤.

π’» μ½”λ“

```
# ν•μ΄νΌνλΌλ―Έν„° μ§€μ •
INPUT_SIZE = 1000
HIDDEN_SIZE = 100
OUTPUT_SIZE = 2
LEARNING_RATE = 1e-6
```

* ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ„ μ–Έν–μΌλ©΄ μ‹¤ν—μ„ ν•΄λ΄μ•Όκ² μ£ ? μΌλ‹¨ μ‹¤ν—ν™κ²½μ„ μ„ν•΄ λ‹¤μκ³Ό κ°™μ΄ μ„μμ κ°’μΌλ΅ input(X), output(Y), Weights(W1, W2)λ¥Ό μ •μν•΄μ¤λ‹λ‹¤.
* μ΄λ•, `requires_grad=True`λ” autograd μ— λ¨λ“  μ—°μ‚°(operation)λ“¤μ„ μ¶”μ ν•΄μ•Ό ν•λ‹¤κ³  μ•λ ¤μ¤λ‹λ‹¤.

π’» μ½”λ“

```
# μ„μμ X, Y, Weight μ •μ
# x : input κ°’ >> (32, 1000)
x = torch.randn(BATCH_SIZE, 
                INPUT_SIZE, 
                device = device, 
                dtype = torch.float, 
                requires_grad = False)
                
# y : output κ°’ >> (32, 2)
y = torch.randn(BATCH_SIZE, 
                OUTPUT_SIZE, 
                device = device,
                dtype = torch.float, 
                requires_grad = False) 
                
# w1 : input -> hidden >> (1000, 100)
w1 = torch.randn(INPUT_SIZE, 
                 HIDDEN_SIZE, 
                 device = device, 
                 dtype = torch.float,
                 requires_grad = True)  

# w2 : hidden -> output >> (100, 2)
w2 = torch.randn(HIDDEN_SIZE,
                 OUTPUT_SIZE, 
                 device = device,
                 dtype = torch.float,
                 requires_grad = True)
```

### Train Model (iteration = 500)

* λ³Έ ν¬μ¤νΈμ€ Autogradλ¥Ό ν™•μΈν•΄λ³΄λ” ν¬μ¤νΈμ΄λ―€λ΅, λ‹¨μν•κ² forλ¬Έμ„ μ΄μ©ν•μ—¬ 500λ² iterationμ„ μν–‰ν•λ„λ΅ μ½”λ“λ¥Ό μ‘μ„±ν•μ€μµλ‹λ‹¤.
* `torch.mm()` : mmμ€ matrix multiplicationμ μ¤„μ„λ§μΌλ΅, ν–‰λ ¬μ κ³±μ…μ„ μλ―Έν•©λ‹λ‹¤.
* `torch.nn.ReLU()` : ReLUν•¨μ, ReLUλ” max(0, x)λ¥Ό μλ―Έν•λ” ν•¨μμΈλ°, 0λ³΄λ‹¤ μ‘μ•„μ§€κ² λλ©΄ 0μ΄ λκ³ , κ·Έ μ΄μƒμ€ κ°’μ„ μ μ§€ν•λ‹¤λ” νΉμ§•μ„ κ°€μ§€κ³  μμµλ‹λ‹¤.
* `loss.backward()` : lossμ— λ€ν•μ—¬ `.backward()`λ¥Ό νΈμ¶ν• κ²ƒμΌλ΅, autogradλ” κ° νλΌλ―Έν„° κ°’μ— λ€ν•΄ λ―Έλ¶„κ°’(gradient)μ„ κ³„μ‚°ν•κ³  μ΄λ¥Ό κ° ν…μ„μ `.grad` μ†μ„±(attribute)μ— μ €μ¥ν•©λ‹λ‹¤.
* `with torch.no_grad()` : λ―Έλ¶„κ°’(gradient) κ³„μ‚°μ„ μ‚¬μ©ν•μ§€ μ•λ„λ΅ μ„¤μ •ν•λ” μ»¨ν…μ¤νΈ-κ΄€λ¦¬μ(Context-manager)μ…λ‹λ‹¤. ν•΄λ‹Ή λ¨λ“λ” μ…λ ¥μ— requires\_grad=Trueκ°€ μμ–΄λ„, μ΄λ¥Ό requires\_grad=Falseλ΅ λ°”κΏ”μ¤λ‹λ‹¤.

π’» μ½”λ“

```
from torch import nn

# 500 iteration
for t in range(1, 501):
    # μ€λ‹‰κ°’
    hidden = nn.ReLU(x.mm(w1))
    
    # μμΈ΅κ°’
    y_pred = hidden.mm(w2)
    
    # μ¤μ°¨μ κ³±ν•© κ³„μ‚°
    loss = (y_pred - y).pow(2).sum()
    
    # iteration 100 λ§λ‹¤ κΈ°λ΅ν•λ„λ΅
    if t % 100 == 0:
        print(t, "th Iteration: ", sep = "")
        print(">>>> Loss: ", loss.item())
    
    # Lossμ Gradient κ³„μ‚°
    loss.backward()                                           
	
    # ν•΄λ‹Ή μ‹μ μ Gradientκ°’μ„ κ³ μ •
    with torch.no_grad():
    	# Weight μ—…λ°μ΄νΈ
        w1 -= LEARNING_RATE * w1.grad                          
        w2 -= LEARNING_RATE * w2.grad                          
		
        # Weight Gradient μ΄κΈ°ν™”(0)
        w1.grad.zero_()                                      
        w2.grad.zero_()
```

* 500λ²μ λ°λ³µλ¬Έμ„ μ‹¤ν–‰ν•λ©΄μ„ μ μ  Lossκ°€ μ¤„μ–΄λ“λ” κ²ƒμ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

π’» κ²°κ³Ό

```
100th Iteration: 
>>>> Loss:  926.969116210
200th Iteration: 
>>>> Loss:  6.41975164413
300th Iteration: 
>>>> Loss:  0.06706248223
400th Iteration: 
>>>> Loss:  0.00112969405
500th Iteration: 
>>>> Loss:  0.00011484944
```

μ‹¬ν™” κ°λ…
-----

### Computational Graph (μ—°μ‚° κ·Έλν”„)

* `autograd`λ” λ°μ΄ν„°(ν…μ„)μ λ° μ‹¤ν–‰λ λ¨λ“  μ—°μ‚°λ“¤μ κΈ°λ΅μ„ κ°μ²΄λ΅ κµ¬μ„±λ λ°©ν–¥μ„± λΉ„μν™ κ·Έλν”„(DAG; Directed Acyclic Graph)μ— μ €μ¥(keep)ν•©λ‹λ‹¤.
* λ°©ν–¥μ„± λΉ„μν™ κ·Έλν”„(DAG)μ NNμ μ „λ°μ μΈ κ³„μ‚°κ³Όμ •μ„ κ·Έλν”„λ΅ λ‚νƒ€λ‚Έ κ²ƒμΌλ΅, μ(leave)μ€ μ…λ ¥ ν…μ„(λ°μ΄ν„°)μ΄κ³ , λΏλ¦¬(root)λ” κ²°κ³Ό ν…μ„(λ°μ΄ν„°)μ…λ‹λ‹¤.
* μ΄λ¬ν• λ°©ν–¥μ„± λΉ„μν™ κ·Έλν”„(DAG)λ¥Ό λΏλ¦¬μ—μ„λ¶€ν„° μκΉμ§€ μ¶”μ ν•λ©΄ μ—°μ‡„ λ²•μΉ™(chain rule)μ— λ”°λΌ κΈ°μΈκΈ°(gradient)λ¥Ό μλ™μΌλ΅ κ³„μ‚°ν•  μ μλ” κµ¬μ΅°μ…λ‹λ‹¤.

**μμ „ν λ‹¨κ³„** μ—μ„, autogradλ” μ•„λ λ‘ κ°€μ§€ μ‘μ—…μ„ λ™μ‹μ— μν–‰ν•©λ‹λ‹¤.

1. μ”μ²­λ μ—°μ‚°μ„ μν–‰ν•μ—¬ κ²°κ³Ό ν…μ„λ¥Ό κ³„μ‚°ν•κ³ ,
2. DAGμ— μ—°μ‚°μ gradient functionμ„ μ μ§€(maintain)ν•©λ‹λ‹¤.

**μ—­μ „ν λ‹¨κ³„** λ” DAG λΏλ¦¬(root)μ—μ„ `.backward()` κ°€ νΈμ¶λ  λ• μ‹μ‘λ©λ‹λ‹¤. autograd λ” μ•„λ μ„Έ κ°€μ§€ μ‘μ—…μ„ μμ°¨μ μΌλ΅ μν–‰ν•©λ‹λ‹¤.

1. κ° `.grad_fn`μΌλ΅λ¶€ν„° gradientλ¥Ό κ³„μ‚°
2. κ° ν…μ„μ `.grad` μ†μ„±μ— κ³„μ‚° κ²°κ³Όλ¥Ό μ €μ¥(accumulate)
3. μ—°μ‡„ λ²•μΉ™μ„ μ‚¬μ©ν•μ—¬, λ¨λ“  μ(leaf) ν…μ„λ“¤κΉμ§€ μ „ν(propagate)

![Computational Graph](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F59e03166-17c2-4d15-8656-92f0a1f52baf%2Fimage.png)

μ°Έκ³  κ°λ…
-----

> μ¶μ² : <https://tutorials.pytorch.kr/beginner/nn_tutorial.html>

* μ—°μ‚° κ·Έλν”„μ™€ autogradλ” λ³µμ΅ν• μ—°μ‚°μλ¥Ό μ •μν•κ³  λ„ν•¨μ(derivative)λ¥Ό μλ™μΌλ΅ κ³„μ‚°ν•λ” λ§¤μ° κ°•λ ¥ν• ν¨λ¬λ‹¤μ„(paradigm)μ…λ‹λ‹¤. ν•μ§€λ§ λ€κ·λ¨ μ‹ κ²½λ§μ—μ„λ” autograd κ·Έ μμ²΄λ§μΌλ΅λ” λ„λ¬΄ μ €μμ¤€(low-level)μΌ μ μμµλ‹λ‹¤.

### torch.nn, torch.optim

* PyTorchλ” μ‹ κ²½λ§(neural network)λ¥Ό μƒμ„±ν•κ³  ν•™μµμ‹ν‚¤λ” κ²ƒμ„ λ„μ™€μ£ΌκΈ° μ„ν•΄μ„ `torch.nn`, `torch.optim`μ΄ μ κ³µλ©λ‹λ‹¤.

  + torch.nn : λ‹¤μ–‘ν• λ‰΄λ΄ λ„¤νΈμ›ν¬λ¥Ό μƒμ„±ν•  μ μλ” ν¨ν‚¤μ§€μ…λ‹λ‹¤.

    1. `torch.nn.Module`: ν•¨μμ²λΌ λ™μ‘ν•μ§€λ§, λν• μƒνƒ(state)λ¥Ό ν¬ν•¨ν•  μ μλ” νΈμ¶ κ°€λ¥ν• μ¤λΈμ νΈλ¥Ό μƒμ„±ν•©λ‹λ‹¤. μ΄λ” ν¬ν•¨λ Parameterλ“¤μ΄ μ–΄λ–¤ κ²ƒμΈμ§€ μ•κ³ , λ¨λ“  κΈ°μΈκΈ°λ¥Ό 0μΌλ΅ μ„¤μ •ν•κ³  κ°€μ¤‘μΉ μ—…λ°μ΄νΈ λ“±μ„ μ„ν•΄ λ°λ³µν•  μ μμµλ‹λ‹¤.
    2. `torch.nn.Parameter`: Module μ— μ—­μ „ν λ™μ• μ—…λ°μ΄νΈκ°€ ν•„μ”ν• κ°€μ¤‘μΉκ°€ μμμ„ μ•λ ¤μ£Όλ” ν…μ„μ© λνΌμ…λ‹λ‹¤. requires\_grad μ†μ„±μ΄ μ„¤μ •λ ν…μ„λ§ μ—…λ°μ΄νΈ λ©λ‹λ‹¤.
    3. `torch.nn.functional`: ν™μ„±ν™” ν•¨μ, μ†μ‹¤ ν•¨μ λ“±μ„ ν¬ν•¨ν•λ” λ¨λ“μ΄κ³ , λ¬Όλ΅  μ»¨λ³Όλ£¨μ… λ° μ„ ν• λ μ΄μ–΄ λ“±μ— λ€ν•΄μ„ μƒνƒλ¥Ό μ €μ¥ν•μ§€μ•λ”(non-stateful) λ²„μ „μ λ μ΄μ–΄λ¥Ό ν¬ν•¨ν•©λ‹λ‹¤.
  + torch.optim: μ•μ—μ„λ” `torch.no_grad()`λ΅ ν•™μµ κ°€λ¥ν• λ§¤κ°λ³€μλ¥Ό κ°–λ” ν…μ„λ“¤μ„ μ§μ ‘ μ΅°μ‘ν•μ—¬ λ¨λΈμ κ°€μ¤‘μΉ(weight)λ¥Ό κ°±μ‹ ν•μ€μµλ‹λ‹¤. μ΄λ” κ°„λ‹¨ν• μµμ ν™” μ•κ³ λ¦¬μ¦μ—μ„λ” ν¬κ² λ¶€λ‹΄μ΄ λμ§€ μ•μ§€λ§, μ‹¤μ λ΅ μ‹ κ²½λ§μ„ ν•™μµν•  λ•λ” AdaGrad, RMSProp, Adam λ“±κ³Ό κ°™μ€ λ” μ •κµν• μµν‹°λ§μ΄μ €(optimizer)λ¥Ό μ‚¬μ©ν•κ³¤ ν•©λ‹λ‹¤. μ΄μ— PyTorchμ optim ν¨ν‚¤μ§€λ” μµμ ν™” μ•κ³ λ¦¬μ¦μ— λ€ν• μ•„μ΄λ””μ–΄λ¥Ό μ¶”μƒν™”ν•κ³  μΌλ°μ μΌλ΅ μ‚¬μ©ν•λ” μµμ ν™” μ•κ³ λ¦¬μ¦μ κµ¬ν„μ²΄(implementation)λ¥Ό μ κ³µν•©λ‹λ‹¤.

### Dataset, DataLoader

* λ°μ΄ν„° μƒν”μ„ μ²λ¦¬ν•λ” μ½”λ“λ” μ§€μ €λ¶„ν•κ³  μ μ§€λ³΄μκ°€ μ–΄λ ¤μΈ μ μμµλ‹λ‹¤. λ” λ‚μ€ κ°€λ…μ„±(readability)κ³Ό λ¨λ“μ„±(modularity)μ„ μ„ν•΄ λ°μ΄ν„°μ…‹ μ½”λ“λ¥Ό λ¨λΈ ν•™μµ μ½”λ“λ΅λ¶€ν„° λ¶„λ¦¬ν•λ” κ²ƒμ΄ μ΄μƒμ μ…λ‹λ‹¤.
* PyTorchλ” `torch.utils.data.DataLoader` μ™€ `torch.utils.data.Dataset`μ λ‘ κ°€μ§€ λ°μ΄ν„° κΈ°λ³Έ μ”μ†λ¥Ό μ κ³µν•μ—¬ λ―Έλ¦¬ μ¤€λΉ„ν•΄λ(pre-loaded) λ°μ΄ν„°μ…‹ λΏλ§ μ•„λ‹λΌ κ°€μ§€κ³  μλ” λ°μ΄ν„°λ¥Ό μ‚¬μ©ν•  μ μλ„λ΅ ν•©λ‹λ‹¤.

  + torch.utils.data.Dataset: μƒν”κ³Ό μ •λ‹µ(label)μ„ μ €μ¥ν•κ³ , **len** λ° **getitem** μ΄ μλ” κ°μ²΄μ μ¶”μƒ μΈν„°νμ΄μ¤μ…λ‹λ‹¤.
  + torch.utils.data.DataLoader: λ¨λ“  μΆ…λ¥μ Datasetμ„ κΈ°λ°μΌλ΅ λ°μ΄ν„°μ λ°°μΉλ“¤μ„ μ¶λ ¥ν•λ” λ°λ³µμ(iterator)λ¥Ό μƒμ„±ν•©λ‹λ‹¤.

κΈ΄ κΈ€ μ½μ–΄μ£Όμ…”μ„ κ°μ‚¬ν•©λ‹λ‹¤ ^~^