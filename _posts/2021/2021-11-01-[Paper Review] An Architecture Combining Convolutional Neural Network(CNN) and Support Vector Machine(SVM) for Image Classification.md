---
title: "[Paper Review] An Architecture Combining Convolutional Neural Network(CNN) and Support Vector Machine(SVM) for Image Classification"
date: "2021-11-01"
tags:
  - "CV"
  - "paper-review"
year: "2021"
---

# [Paper Review] An Architecture Combining Convolutional Neural Network(CNN) and Support Vector Machine(SVM) for Image Classification

ì˜¤ëŠ˜ `ë¦¬ë·°/ë²ˆì—­/êµ¬í˜„`í•  ë…¼ë¬¸ì€ "Abien Fred M. Agarap" ì €ìê°€ ì“´ ë…¼ë¬¸ìœ¼ë¡œ, "Yichuan Tang"ì˜ "Deep Learning using Linear Support Vector Machines"ì„ ë³´ê³  inspiredë˜ì–´ ì—°êµ¬í•˜ê²Œ ë˜ì—ˆë‹¤ê³  í•œë‹¤. í•˜ë‹¨ì˜ ì°¸ê³  ë…¼ë¬¸ ì†ŒìŠ¤ì— í•´ë‹¹ ë…¼ë¬¸ ë§í¬ì™€ ì´ë²ˆ ë…¼ë¬¸ì˜ ë§í¬ë¥¼ ì²¨ë¶€ì˜€ë‹¤.

**(ì°¸ê³ ) ë…¼ë¬¸ ì†ŒìŠ¤**

* Deep Learning using Linear Support Vector Machines ([í´ë¦­](https://arxiv.org/pdf/1306.0239v4.pdf))
* An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification ([í´ë¦­](https://arxiv.org/pdf/1712.03541v2.pdf))

---

Abstract
========

* CNN(í•©ì„±ê³±ì‹ ê²½ë§)ì€ Hidden layerë“¤ê³¼ learnable parameterë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ë‰´ëŸ°ì—ì„œëŠ” inputì„ ë°›ìœ¼ë©´ ì´ë¥¼ ë‚´ì í•˜ê³ , ë¹„ì„ í˜•ì„±ì„ ë”í•´ì¤€ë‹¤. Raw Imageì™€ í•´ë‹¹ class scoreë¥¼ ì´ì–´ì£¼ëŠ” ë§¤ê°œì²´ì˜ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. (ì£¼ë¡œ CNN ë§ˆì§€ë§‰ ë‹¨ì—ëŠ” softmaxí•¨ìˆ˜ê°€ ì´ìš©ì´ ëœë‹¤.
* í•˜ì§€ë§Œ, ëª‡ëª‡ ë…¼ë¬¸ë“¤ì€ ìœ„ì™€ ê°™ì€ ë°©ë²•ë¡ ì— ë¬¸ì œë¥¼ ì œê¸°í•˜ì˜€ë‹¤:

  + Abien Fred Agarap. 2017. A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data. arXiv preprint arXiv:1709.03082 (2017).
  + Abdulrahman Alalshekmubarak and Leslie S Smith. 2013. A novel approach combining recurrent neural network and support vector machines for time series classification. In Innovations in Information Technology (IIT), 2013 9th International Conference on. IEEE, 42â€“47
  + Yichuan Tang. 2013. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239 (2013).
* ìœ„ì—ì„œ ë³´ì—¬ì¤€ ë…¼ë¬¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ linear SVMì„ ì´ìš©í•˜ëŠ” ê²ƒì„ ì œì•ˆí•œë‹¤. ì´ì— ì €ìëŠ” ***CNNë‹¨ì— Softmax ëŒ€ì‹  SVMì„ ì´ìš©í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰***í•œë‹¤.
* MNIST

  + CNN-SVM : 99.04%
  + CNN-Softmax : 99.23%
* MNIST-Fasion

  + CNN-SVM : 90.72%
  + CNN-Softmax : 91.86%
* ì €ìëŠ” ì„±ëŠ¥ì€ ë¹„ë¡ ì¡°ê¸ˆ ë‚®ì„ ìˆ˜ ìˆì„ì§€ë¼ë„, ì¢€ ë” ê³ ë„í™”ëœ CNNì„ ì´ìš©í•˜ë©´ ì„±ëŠ¥ì„ ë”ìš± ë” í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ì£¼ì¥í•œë‹¤.

> ğŸ’¡ **ë¦¬ë·° ë…¼ë¬¸ ì„ ì • ì´ìœ **  
> í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ì´ìš©í•˜ì—¬ State-of-the-art(SOTA)ë¥¼ ì°ì§€ëŠ” ì•Šì§€ë§Œ, í›„ì— ë‹¤ì–‘í•œ Vision ë¶„ì•¼ì—ì„œ ë§ˆì§€ë§‰ ë‹¨ì— SVM Classifierë¥¼ ì‚¬ìš©í•˜ê¸°ì— ê·¼ê°„ì´ ëœ ë…¼ë¬¸ì„ ì„ ì •í•˜ê²Œ ë˜ì—ˆë‹¤. ìµœê·¼ ì—°êµ¬ì— ìˆì–´ì„œ ëª¨ë¸ì— ê°„ë‹¨í•œ ë³€í™”ë¥¼ (ë”í•´)ì¤Œìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ í•˜ëŠ” ê³ ë¯¼ì— ì°¾ì•„ë³´ê³  ì •ë¦¬í•´ë³´ê²Œ ë˜ì—ˆë‹¤.

---

1. Introduction
===============

* ìœ„ì— Abstractì—ì„œ ê°„ë‹¨íˆ ì†Œê°œí–ˆë“¯ì´ NeuralNet(ì¸ê³µì‹ ê²½ë§)ì— softmaxì´ì™¸ì— ë‹¤ë¥¸ ë°©ë²•ë¡ (Ex. SVM)ì„ ì ìš©í•˜ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì–´ ì™”ë‹¤.
  + Abien Fred Agarap. 2017. A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data. arXiv preprint arXiv:1709.03082 (2017).
  + Abdulrahman Alalshekmubarak and Leslie S Smith. 2013. A novel approach combining recurrent neural network and support vector machines for time series classification. In Innovations in Information Technology (IIT), 2013 9th International Conference on. IEEE, 42â€“47
  + Yichuan Tang. 2013. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239 (2013).
* ì´ëŸ¬í•œ ì—°êµ¬ë“¤ì—ì„œ ANNì— softmaxë¥¼ ì ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, SVMì„ ì ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤ëŠ” ê²°ê³¼ë“¤ì´ ë‚˜ì™”ë‹¤. (ì´ì§„ íŒë³„(binary classification) í•œì •, multinomial caseì˜ ê²½ìš° one-versus-all ë°©ì‹ ì±„ìš©)
* í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” 2013ë…„ì— ë‚˜ì˜¨ "Deep learning using linear support vector machines" ë…¼ë¬¸ì—ì„œ CNNëª¨ë¸ì„ ì¢€ ë” ì‰½ê³  ê°„í¸í•œ 2-Conv Layer with Max Poolingëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.

2. Metodology
=============

2.1 Machine Intelligence Library
--------------------------------

* í•´ë‹¹ ë…¼ë¬¸ì€ Googleì˜ Tensorflowì„ ì´ìš©í•˜ì—¬ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.
* ì´ë²ˆ ë…¼ë¬¸ êµ¬í˜„ì— ìˆì–´ì„œëŠ” ìµœê·¼ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” PyTorchë¥¼ ì´ìš©í•˜ì—¬ ë…¼ë¬¸êµ¬í˜„ì„ ìˆ˜í–‰í•´ë³´ì•˜ë‹¤.

```
# Load Libraries
import torch
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn.init
from torch.utils.data import Dataset
from torch.autograd import Variable
from PIL import Image
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import helper

# GPU ì„¤ì •
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ëœë¤ ì‹œë“œ ê³ ì •
torch.manual_seed(123)

# GPU ì‚¬ìš© ê°€ëŠ¥ì¼ ê²½ìš° ëœë¤ ì‹œë“œ ê³ ì •
if device == 'cuda':
    torch.cuda.manual_seed_all(123)

# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])
```

2.2 The Dataset
---------------

* **MNIST** : 10-class classification problem having 60,000 training examples, and 10,000 test cases â€“ all in grayscale

![MNIST](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F9eb5d6a1-3019-43cf-8bde-50650d0402aa%2Fimage.png)

* **Fashion-MNIST** : the same number of classes, and the same color profile as MNIST

![Fashion-MNIST](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fc828ddae-c49b-4d63-ac54-97a705706678%2Fimage.png)

![Table 1](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fb42795a5-c851-49ae-b73e-0b6279cbde57%2Fimage.png)  
Table 1: Dataset distribution for both MNIST and Fashion-MNIST

### Import fashion-MINIST

```
# Download and load the training data
fashion_trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)
fashion_trainloader = torch.utils.data.DataLoader(fashion_trainset, batch_size=128, shuffle=True)

# Download and load the test data
fashion_testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)
fashion_testloader = torch.utils.data.DataLoader(fashion_testset, batch_size=128, shuffle=True)
```

### Import MINIST

```
# Download and load the training data
mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)
mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, shuffle=True)

# Download and load the test data
mnist_testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)
mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=128, shuffle=True)
```

* ë³„ë„ì˜ ì „ì²˜ë¦¬ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. (No normalization or dimensionality reduction)

2.3 Support Vector Machine(SVM)
-------------------------------

* Support Vector Machine(SVM)ì€ C. Cortes and V. Vapnikì— ì˜í•´ ê°œë°œëœ ì´ì§„ë¶„ë¥˜ ë°©ë²•ë¡ ìœ¼ë¡œ, ìµœì ì˜ ì´ˆí‰ë©´(***f (w, x) = w Â· x + b***)ì„ ì°¾ëŠ” ë°ì— ì˜ì˜ë¥¼ ë‘”ë‹¤. ì´ˆí‰ë©´ì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ classë¥¼ ë¶„ë¥˜í•´ì¤€ë‹¤.
* SVMì€ í•´ë‹¹ ì‹ì„ ìµœì í™”í•˜ì—¬ W parameterë¥¼ í•™ìŠµí•œë‹¤.
  + L1-SVM  
    ![L1-SVM](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F3bc89ea6-8acc-4841-854f-e92d14de9e41%2Fimage.png)
  + wTww^{T}wwTwëŠ” Manhattan norm(L1 norm), CëŠ” penalty parameter, y'ëŠ” ì‹¤ì œ yê°’, wTww^{T}wwTw+bëŠ” ì˜ˆì¸¡ yê°’ì´ë‹¤.
  + L2-SVM  
    ![L2-SVM](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F6672f586-2591-44a6-8d16-ec50b60b387f%2Fimage.png)
  + âˆ£wâˆ£2|w|^{2}âˆ£wâˆ£2ëŠ” Euclidean norm(L2 norm), CëŠ” penalty parameter, y'ëŠ” ì‹¤ì œ yê°’, wTww^{T}wwTw+bëŠ” ì˜ˆì¸¡ yê°’ì´ë‹¤.

```
class SVM:
		# set learning_rate, lambda, n iterations
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None

		# SVM fit function
    def fit(self, X, y):
        n_samples, n_features = X.shape
        
        y_ = np.where(y <= 0, -1, 1)
        
        self.w = np.zeros(n_features)
        self.b = 0

        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if condition:
                    self.w -= self.lr * (2 * self.lambda_param * self.w)
                else:
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]

		# SVM predict function
    def predict(self, X):
        approx = np.dot(X, self.w) - self.b
        return np.sign(approx)
```

2.4 Convolutional Neural Network(CNN)
-------------------------------------

* Convolutional Neural Network(CNN)ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ë§ì´ ì“°ì´ëŠ” deep feed-forward artificial neural networkë¡œ, MLP ë¿ë§Œ ì•„ë‹ˆë¼ convolutional layers, pooling, ê·¸ë¦¬ê³  ë¹„ì„ í˜• activation functionì¸ tanh, sigmoid, ReLU ë“±ì´ ì“°ì¸ë‹¤.
* ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ë³¸ CNNëª¨ë¸ì„ ì´ìš©í•œë‹¤.
  + 5x5x1 size filter
  + 2x2 max pooling
  + RELU as activation function (threshold = 0)  
    ![RELU](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F33b4def9-68a1-491e-9586-35e63038d8ed%2Fimage.png)
  + 10ë²ˆì§¸ layerë‹¨ì—ì„œ convolutional softmax ëŒ€ì‹  **L2-SVM**ì„ ì´ìš©í•œë‹¤. ( y âˆˆ {-1, +1}, adam optimizer ì´ìš©)

![ëª¨ë¸ êµ¬ì¡°1](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F137a4713-bc86-444f-abf6-b2e79fe857f1%2Fimage.png)  
ì €ìê°€ ì´ìš©í•œ ëª¨ë¸ êµ¬ì¡°(ì§ì ‘ ì œì‘)

![ëª¨ë¸ êµ¬ì¡°2](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fa14846ef-7caf-4d82-9fb1-7e3a3fcaa5d0%2Fimage.png)  
ì €ìê°€ ì´ìš©í•œ ëª¨ë¸ êµ¬ì¡°(ë…¼ë¬¸ ìˆ˜ë¡)

![ëª¨ë¸ êµ¬ì¡°3](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F7534242e-2f59-4fe4-98ec-174eed194efa%2Fimage.png)  
ì €ìê°€ ì´ìš©í•œ ëª¨ë¸ êµ¬ì¡°(ì§ì ‘ êµ¬í˜„)

### CNN model

```
class CNN(torch.nn.Module):

    def __init__(self):
        super(CNN, self).__init__()
        self.drop_prob = 0.5

        # define layer1
        self.layer1 = torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, kernel_size=5, stride=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=1))

        # define layer2
        self.layer2 = torch.nn.Sequential(
            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=1))

        # define fully connected layer (1024)
        self.fc1 = torch.nn.Linear(18 * 18 * 64, 1024, bias=True)
        torch.nn.init.xavier_uniform_(self.fc1.weight)
        self.layer3 = torch.nn.Sequential(
            self.fc1,
            torch.nn.Dropout(p= self.drop_prob))

            
        # define fully connected layer (10 classes)
        self.fc2 = torch.nn.Linear(1024, 10, bias=True)
        torch.nn.init.xavier_uniform_(self.fc2.weight)

    # define feed-forward
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(out.size(0), -1)   # Flatten them for FC
        out = self.layer3(out)
        out = self.fc2(out)
        return out
```

### CNN + SVM model (multi-Class Hinge Loss)

```
class multiClassHingeLoss(nn.Module):
    def __init__(self, p=1, margin=1, weight=None, size_average=True):
        super(multiClassHingeLoss, self).__init__()
        self.p=p
        self.margin=margin
        self.weight=weight
        self.size_average=size_average

    # define feed-forward		
    def forward(self, output, y):`
        output_y=output[torch.arange(0,y.size()[0]).long().cuda(),y.data.cuda()].view(-1,1)

        # output - output(y) + output(i)
        loss=output-output_y+self.margin

        # remove i=y items
        loss[torch.arange(0,y.size()[0]).long().cuda(),y.data.cuda()]=0
        
				# apply max function
        loss[loss<0]=0
        
				# apply power p function
        if(self.p!=1):
            loss=torch.pow(loss,self.p)

        # add weight
        if(self.weight is not None):
            loss=loss*self.weight

        # sum up
        loss=torch.sum(loss)

        if(self.size_average):
            loss/=output.size()[0]

        return loss
```

> ğŸ’¡ ì ê¹!! **hinge lossë€?**
>
> * í•™ìŠµë°ì´í„° ê°ê°ì˜ ë²”ì£¼ë¥¼ êµ¬ë¶„í•˜ë©´ì„œ ë°ì´í„°ì™€ì˜ ê±°ë¦¬ê°€ ê°€ì¥ ë¨¼ ê²°ì •ê²½ê³„(decision boundary)ë¥¼ ì°¾ê¸° ìœ„í•´ ê³ ì•ˆëœ ì†ì‹¤í•¨ìˆ˜ì˜ í•œ ë¶€ë¥˜. ì´ë¡œì¨ ë°ì´í„°ì™€ ê²½ê³„ ì‚¬ì´ì˜ ë§ˆì§„(margin)ì´ ìµœëŒ€í™”ëœë‹¤.
> * ì´ì§„ ë¶„ë¥˜ë¬¸ì œì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’Â yâ€²(ìŠ¤ì¹¼ë¼), í•™ìŠµë°ì´í„°ì˜ ì‹¤ì œê°’Â y (-1 ë˜ëŠ” 1) ì‚¬ì´ì˜ hinge lossëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.  
>   loss=max(0,1âˆ’(yâ€²Ã—y))loss=max( 0, 1 âˆ’ (y' Ã— y))loss=max(0,1âˆ’(yâ€²Ã—y))

2.5 Data Analysis
-----------------

* 2ê°œì˜ phase(train/test)
* 2ê°œì˜ dataset(MNIST, fashion-MNIST)

3. Experiments
==============

* ì•„ë˜ ê·¸ë¦¼ì€ ê°ê°ì˜ ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ ì„¤ì •í•´ì¤€ Hyper parameter ì •ë³´ë“¤ì´ë‹¤.

![Hyper-parameters](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fb88bc082-19c0-47a7-83d0-943869fd668d%2Fimage.png)  
Table 2: Hyper-parameters used for CNN-Softmax and CNNSVM models.

### Set hyper-parameter

```
learning_rate = 0.001
training_epochs = 50
# training_epochs = 10000
# í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ë§Œë²ˆì˜ epochë¥¼ ìˆ˜í–‰í–ˆì§€ë§Œ computation powerë¡œ ì¸í•´ epoch 50íšŒ ìˆ˜í–‰
batch_size = 128
```

### Make Model for MNIST Data (CNN)

```
# MNIST CNN ëª¨ë¸ ì •ì˜
mnist_model = CNN().to(device)

criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)

total_batch = len(mnist_trainloader)
print('ì´ ë°°ì¹˜ì˜ ìˆ˜ : {}'.format(total_batch))
```

### Make Model for MNIST Data (CNN + SVM)

```
# MNIST CNN+SVM ëª¨ë¸ ì •ì˜
minst_SVM_model = CNN().to(device)

criterion = multiClassHingeLoss().to(device)
optimizer = torch.optim.Adam(minst_SVM_model.parameters(), lr=learning_rate)

total_batch = len(mnist_trainloader)
print('ì´ ë°°ì¹˜ì˜ ìˆ˜ : {}'.format(total_batch))
```

### Make Model for fashion-MNIST Data (CNN)

```
# fashion-MNIST CNN ëª¨ë¸ ì •ì˜
fashion_model = CNN().to(device)

criterion = torch.nn.CrossEntropyLoss().to(device)    # ë¹„ìš© í•¨ìˆ˜ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ í¬í•¨ë˜ì–´ì ¸ ìˆìŒ.
optimizer = torch.optim.Adam(fashion_model.parameters(), lr=learning_rate)

total_batch = len(fashion_trainloader)
print('ì´ ë°°ì¹˜ì˜ ìˆ˜ : {}'.format(total_batch))
```

### Make Model for fashion-MNIST Data (CNN + SVM)

```
# fashion-MNIST CNN + SVM ëª¨ë¸ ì •ì˜
fashion_SVM_model = CNN().to(device)

criterion = multiClassHingeLoss().to(device)
optimizer = torch.optim.Adam(fashion_SVM_model.parameters(), lr=learning_rate)

total_batch = len(fashion_trainloader)
print('ì´ ë°°ì¹˜ì˜ ìˆ˜ : {}'.format(total_batch))
```

### Train Models

```
# mnist_model(CNN)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in mnist_trainloader: 
        X = X.to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = mnist_model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))

# minst_SVM_model(CNN + SVM)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in mnist_trainloader: 
        X = X.to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = minst_SVM_model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))
```

```
# fashion_model(CNN)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in fashion_trainloader: 
        X = X.to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = fashion_model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))

# fashion_SVM_model(CNN + SVM)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in fashion_trainloader: 
        X = X.to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = fashion_SVM_model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))
```

### Test Models

```
# mnist_model(CNN)
with torch.no_grad():
    correct = 0
    total = 0
    for X_test, Y_test in mnist_testloader:
        X_test = X_test.to(device)
        Y_test = Y_test.to(device)
        prediction = mnist_model(X_test)
        predicted = torch.argmax(prediction, 1)
        total += Y_test.size(0)
        correct += (predicted == Y_test).sum().item()

print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))

# mnist_SVM_model(CNN+SVM)
with torch.no_grad():
    correct = 0
    total = 0
    for X_test, Y_test in mnist_testloader:
        X_test = X_test.to(device)
        Y_test = Y_test.to(device)
        prediction = mnist_SVM_model(X_test)
        predicted = torch.argmax(prediction, 1)
        total += Y_test.size(0)
        correct += (predicted == Y_test).sum().item()

print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))
```

```
# fashion_model(CNN)
with torch.no_grad():
    correct = 0
    total = 0
    for X_test, Y_test in fashion_testloader:
        X_test = X_test.to(device)
        Y_test = Y_test.to(device)
        prediction = fashion_model(X_test)
        predicted = torch.argmax(prediction, 1)
        total += Y_test.size(0)
        correct += (predicted == Y_test).sum().item()

print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))

# fashion_SVM_model(CNN + SVM)
with torch.no_grad():
    correct = 0
    total = 0
    for X_test, Y_test in fashion_testloader:
        X_test = X_test.to(device)
        Y_test = Y_test.to(device)
        prediction = fashion_SVM_model(X_test)
        predicted = torch.argmax(prediction, 1)
        total += Y_test.size(0)
        correct += (predicted == Y_test).sum().item()

print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))
```

* ë°‘ì˜ ê·¸ë¦¼ì€ ë°ì´í„° ë¶„ì„ì˜ ê²°ê³¼í‘œì´ë‹¤.
  + Figure2 : CNN-Softmaxì™€ CNN-SVMì˜ Training Accuracyë¥¼ ì‹œê°í™”í•œ í‘œ  
    (MNIST)
  + Figure3 : CNN-Softmaxì™€ CNN-SVMì˜ Training lossë¥¼ ì‹œê°í™”í•œ í‘œ  
    (MNIST)
  + Figure4 : CNN-Softmaxì™€ CNN-SVMì˜ Training Accuracyë¥¼ ì‹œê°í™”í•œ í‘œ  
    (fashion-MNIST)
  + Figure5 : CNN-Softmaxì™€ CNN-SVMì˜ Training lossë¥¼ ì‹œê°í™”í•œ í‘œ  
    (fashion-MNIST)

![2/5](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F5090722f-b5ae-4db0-b3f8-9fa1966d26b3%2Fimage.png)

![3/4](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2F51ee38ac-0f11-4bc7-bf99-437bfb0b5992%2Fimage.png)

* **ëª¨ë¸ ì„±ëŠ¥ (epoch = 10000)**

![Table 3](https://velog.velcdn.com/images%2Feuisuk-chung%2Fpost%2Fffadbe78-f169-410e-9645-bc35e6b14934%2Fimage.png)  
Table 3: Test accuracy of CNN-Softmax and CNN-SVM on image classification using MNIST and Fashion-MNIST

* **ì§ì ‘ êµ¬í˜„í•œ ëª¨ë¸ ì„±ëŠ¥ (epoch = 50)**
  + í•™ìŠµì— ì´ìš©í•œ epochìˆ˜ê°€ ìƒì´í•˜ì—¬ ì„±ëŠ¥ì— ì¡°ê¸ˆì˜ ì°¨ì´ê°€ ìˆì—ˆì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í—˜í™˜ê²½ì„ ë™ì¼í•˜ê²Œ êµ¬ì¶•í•´ë³¼ ìˆ˜ ìˆì—ˆë‹¤.

| Dataset | CNN-softmax | CNN-SVM |
| --- | --- | --- |
| MNIST | 98.47% | 98.77% |
| FASHION-MNIST | 88.13% | 87.84% |

4. Conclusion and Rcommendation
===============================

* ë³¸ ì—°êµ¬ ê²°ê³¼ëŠ” "Deep Learning using Linear Support Vector Machines"ì˜ ì œì•ˆëœ CNN-SVMì— ëŒ€í•œ ê²€í† ë¥¼ ë”ìš± ê²€ì¦í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ì˜ ê°œì„ ì„ ë³´ì¦í•˜ëŠ”ë° ì˜ì˜ë¥¼ ë‘”ë‹¤.
* "Deep Learning using Linear Support Vector Machines"ì˜ ì¡°ì‚¬ ê²°ê³¼ì™€ ëª¨ìˆœë¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì–‘ì ìœ¼ë¡œ ë§í•˜ë©´, CNN-ì†Œí”„íŠ¸ë§¥ìŠ¤ì™€ CNN-SVMì˜ ì‹œí—˜ ì •í™•ë„ëŠ” ê´€ë ¨ ì—°êµ¬ì™€ ê±°ì˜ ê°™ë‹¤.
* ë”°ë¼ì„œ, ì¶”ê°€ì ì¸ ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬ ë° ë¹„êµì  ì •êµí•œ base CNN ëª¨ë¸ì„ ì´ìš©í•˜ë©´ ì¶©ë¶„íˆ í•´ë‹¹ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.