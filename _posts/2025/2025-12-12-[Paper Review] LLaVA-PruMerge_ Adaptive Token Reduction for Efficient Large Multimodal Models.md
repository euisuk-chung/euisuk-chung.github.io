---
title: "[Paper Review] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models"
date: "2025-12-12"
tags:
  - "paper-review"
year: "2025"
---

# [Paper Review] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

ì›ë³¸ ê²Œì‹œê¸€: https://velog.io/@euisuk-chung/LLaVA-PruMerge-Adaptive-Token-Reduction-for-Efficient-Large-Multimodal-Models

![](https://velog.velcdn.com/images/euisuk-chung/post/544908e7-c545-4f9b-b1fc-2ae9a671dd01/image.png)

> <https://arxiv.org/pdf/2403.15388>

ë…¼ë¬¸ ì •ë³´
-----

* **ì œëª©**: LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
* **ì €ì**: Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
* **ì†Œì†**: Illinois Institute of Technology, University of Wisconsinâ€“Madison
* **ë°œí‘œ**: ICCV 2025 (arXiv: 2024ë…„ 3ì›”)
* **ë…¼ë¬¸ ë§í¬**: <https://arxiv.org/abs/2403.15388>
* **GitHub**: <https://github.com/42Shawn/LLaVA-PruMerge>
* **Project Page**: <https://llava-prumerge.github.io>

---

1. Introduction: LMM íš¨ìœ¨ì„±ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„
----------------------------------

### 1.1 Large Multimodal Models (LMMs)ì˜ ë“±ì¥

* **Large Language Models (LLMs)**ì€ GPT-4, LLaMA, Mistral ë“±ì—ì„œ ë³´ë“¯ ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ LLMì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „í•™ìŠµëœ ê³ ìš©ëŸ‰ **Transformer ì•„í‚¤í…ì²˜**ì…ë‹ˆë‹¤.
* **Large Multimodal Models (LMMs)**ì€ LLMì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì„ ê³„ìŠ¹í•˜ë©´ì„œ, CLIP-ViT ê°™ì€ Vision Encoderë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ visual tokensìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ visual tokensì€ LLMì˜ prefix contextë¡œ ì…ë ¥ë˜ì–´ ì‹œê°ì  ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

```
[Vision Encoder] â†’ Visual Tokens (prefix) â†’ [LLM] â†’ í…ìŠ¤íŠ¸ ì‘ë‹µ
(CLIP-ViT)         (576ê°œ)                  (Vicuna/LLaMA)
```

---

### 1.2 LMMì˜ ê³„ì‚° ë¹„ìš© ë¬¸ì œ

LMMì€ **ì¶”ë¡ (inference)ì— ìƒë‹¹í•œ ê³„ì‚° ë¹„ìš©**ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì´ ë¹„ìš©ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ë©´:

| êµ¬ì„±ìš”ì†Œ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë¹„ê³  |
| --- | --- | --- |
| Vision Encoder (ViT-L) | ~0.3B | ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŒ |
| **LLM (LLaMA/Vicuna)** | **7B~13B** | ì£¼ìš” ë¹„ìš© ì›ì¸ |

> ğŸ” **í•µì‹¬ í†µì°°:** Vision EncoderëŠ” LLMì— ë¹„í•´ ë§¤ìš° ì‘ìœ¼ë¯€ë¡œ, **LLMì˜ ì¶”ë¡  ë¹„ìš©ì„ ì¤„ì´ëŠ” ê²ƒì´ ì „ì²´ LMM íš¨ìœ¨í™”ì˜ í•µì‹¬**ì…ë‹ˆë‹¤.

---

### 1.3 ê¸°ì¡´ ì ‘ê·¼ë²•ê³¼ í•œê³„

ì´ì „ ì—°êµ¬ë“¤ì€ ì´ëŸ¬í•œ LLM ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ì‹œë„ë“¤ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

| ì ‘ê·¼ë²• | ë°©ë²• | í•œê³„ |
| --- | --- | --- |
| **Small LLM ì‚¬ìš©** | Phi-2 ê¸°ë°˜ MobileVLM, TinyGPT-V | LLM ì¶”ë¡  ëŠ¥ë ¥ í¬ìƒ, VQAv2/MMBenchì—ì„œ í° ì„±ëŠ¥ ê²©ì°¨ |
| **Quantization** | 4-bit, 8-bit ì••ì¶• | íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì¤„ì§€ë§Œ ë‹¤ë¥¸ ë¬¸ì œ ë¯¸í•´ê²° |

---

### 1.4 ê°„ê³¼ëœ ë¹„ìš© ì›ì²œ: Input Context Length

í•˜ì§€ë§Œ, ìœ„ ì—°êµ¬ë“¤ì—ì„œ ê°„ê³¼í•œ ë‚´ìš©ìœ¼ë¡œ "LLMì˜ ë¹„ìš©ì€ **íŒŒë¼ë¯¸í„° ìˆ˜**ë¿ë§Œ ì•„ë‹ˆë¼ **ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**ì—ì„œë„ ë°œìƒí•œë‹¤."ë¼ëŠ” ì‚¬ì‹¤ì„ ì§€ì í•©ë‹ˆë‹¤.

**LLM = Transformer ì•„í‚¤í…ì²˜:**  
LLMì€ Transformer ê¸°ë°˜ì´ë©°, í•µì‹¬ ì—°ì‚°ì¸ Self-Attentionì€ ì…ë ¥ëœ ëª¨ë“  í† í° ìŒ ê°„ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

Attention(Q,K,V)=softmax(QKTdk)â‹…V\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d\_k}}\right) \cdot VAttention(Q,K,V)=softmax(dkâ€‹â€‹QKTâ€‹)â‹…V

ì—¬ê¸°ì„œ QKTQK^TQKT ì—°ì‚°ì€ **N Ã— N ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (N = ì…ë ¥ í† í° ìˆ˜). ë”°ë¼ì„œ Self-Attentionì˜ ê³„ì‚° ë³µì¡ë„ëŠ” ì…ë ¥ ê¸¸ì´ì— ëŒ€í•´ **O(NÂ²)**ì…ë‹ˆë‹¤.

**LMMì—ì„œì˜ ë¬¸ì œ:**

* LMMì€ **ê³ ì •ëœ ëŒ€ëŸ‰ì˜ visual tokens**ì„ prefixë¡œ ì‚¬ìš©
  + LLaVA-1.5: 576 visual tokens
  + Video-LLaVA: 2,048+ tokens (ê³ í•´ìƒë„/ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œ)
* ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¡œ ì¸í•˜ì—¬ Visual tokens ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ LLMì˜ ì–´í…ì…˜ ì—°ì‚°ëŸ‰ì´ **ì œê³±ìœ¼ë¡œ ì¦ê°€**

> ğŸ” **í•µì‹¬ ì§ˆë¬¸:** *Prefix visual tokensì˜ ìˆ˜ë¥¼ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ”ê°€?*

---

### 1.5 í•µì‹¬ ê´€ì°°: Visual Tokensì˜ Redundancy

ë³¸ ì—°êµ¬ì—ì„œ ë°œê²¬í•œ ì¤‘ìš”í•œ í˜„ìƒ:

**ê´€ì°° 1: Sparse Attention Distribution**

Vision Encoderì˜ self-attentionì—ì„œ **[CLS] í† í°ê³¼ spatial patches ê°„ì˜ ì–´í…ì…˜ì´ sparse**í•©ë‹ˆë‹¤. ì´ëŠ” ì†Œìˆ˜ì˜ visual tokensë§Œì´ í•µì‹¬ ì‹œê° ì •ë³´ì™€ ì—°ê´€ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**ê´€ì°° 2: ëŒ€ë¶€ë¶„ì˜ Visual Tokensì€ Redundant**

ê¸°ì¡´ ì—°êµ¬(Bolya et al., 2023; Liu et al., 2022)ì™€ ì¼ê´€ë˜ê²Œ, ëŒ€ë¶€ë¶„ì˜ visual tokensì€ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì œê±°(prune)ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### 1.6 ì œì•ˆ ë°©ë²•: PruMerge ê°œìš”

ì´ëŸ¬í•œ sparse similarityë¥¼ í™œìš©í•˜ì—¬ ì¤‘ìš”í•œ visual tokensì„ **ì ì‘ì ìœ¼ë¡œ ì„ íƒ**í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/3fabb360-07bd-40f0-9156-da1172ffceb4/image.png)

**PruMergeì˜ í•µì‹¬ ì•„ì´ë””ì–´:**

1. **Adaptive Token Selection (Prune)**

   * Interquartile Range (IQR) ê¸°ë°˜ outlier detection
   * [CLS] ì–´í…ì…˜ ê°’ì´ ë†’ì€ í† í°ì„ ì¤‘ìš” í† í°ìœ¼ë¡œ ì„ ë³„
2. **Token Merging (Merge)**

   * IQRë¡œ 32ê°œ í† í°ë§Œ ì„ íƒí•˜ë©´, ë‚˜ë¨¸ì§€ 544ê°œ í† í°ì˜ ì •ë³´ê°€ ì™„ì „íˆ ì†ì‹¤ë  ìˆ˜ ìˆìŒ
   * ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ k-nearest neighbor ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
   * ì„ íƒëœ í† í°ì„ weighted averagingìœ¼ë¡œ ì—…ë°ì´íŠ¸
   * ì œê±°ëœ í† í°ì˜ ì •ë³´ë¥¼ ë³´ì¡´ì„ ëª©ì ìœ¼ë¡œ í•¨
3. **PruMerge+ (í™•ì¥)**

   * ë„ˆë¬´ ê³µê²©ì ì¸ ì••ì¶•ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„± ì¡´ì¬
   * Spatial-uniform sampling ì¶”ê°€
   * ë” í¬ê´„ì ì´ê³  ëŒ€í‘œì„± ìˆëŠ” í† í° ì„ íƒ ë³´ì¥

> ğŸ“· **ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬ì **  
> 1. **Visual token redundancy ë¶„ì„**: [CLS]-spatial attentionì˜ sparsity ê´€ì°°  
> 2. **PruMerge ì œì•ˆ**: ì ì‘ì  í† í° ì„ íƒ ë° ë³‘í•© ì „ëµ  
> 3. **Plug-and-play ì ìš©**: ê¸°ì¡´ LMMì— ì¶”ê°€ í•™ìŠµ ì—†ì´ ì ìš© ê°€ëŠ¥  
> 4. **ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° í™•ì¥**: ì´ë¯¸ì§€ë¿ ì•„ë‹ˆë¼ ë¹„ë””ì˜¤(Video-LLaVA)ì—ë„ ì ìš© ê°€ëŠ¥

---

2.1 Efficient Large Multimodal Models
-------------------------------------

### 1. Compact Architecture (ì‘ì€ ëª¨ë¸ ì‚¬ìš©)

#### MobileVLM / MobileVLM-v2

**ëª©í‘œ:** ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ LMM

> <https://arxiv.org/abs/2402.03766>

```
ì¼ë°˜ LMM:     Vision Encoder â†’ [Vicuna-7B] â†’ ì‘ë‹µ
MobileVLM:   Vision Encoder â†’ [MobileLLaMA-1.4B] â†’ ì‘ë‹µ
                                    â†‘
                              5ë°° ì‘ì€ LLM
```

**íŠ¹ì§•:**

* ëª¨ë°”ì¼ ìµœì í™” LLM backbone ì‚¬ìš©
* ê²½ëŸ‰í™”ëœ projector ì„¤ê³„

---

#### TinyGPT-V

**ëª©í‘œ:** ì‘ì€ LLMìœ¼ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±

> <https://arxiv.org/abs/2312.16862>

```
ê¸°ì¡´ LLaVA:  Vision Encoder â†’ [Vicuna-7B] â†’ ì‘ë‹µ
TinyGPT-V:  Vision Encoder â†’ [Phi-2 (2.7B)] â†’ ì‘ë‹µ
                                   â†‘
                            Microsoftì˜ ì†Œí˜• LLM
```

**íŠ¹ì§•:**

* Phi-2ì˜ ê°•ë ¥í•œ reasoning ëŠ¥ë ¥ í™œìš©
* 7B ëŒ€ë¹„ ì•½ 3ë°° ì‘ì€ ëª¨ë¸

---

#### LLaVA-Phi

**ëª©í‘œ:** Phi ê¸°ë°˜ íš¨ìœ¨ì  LMM

> <https://dl.acm.org/doi/abs/10.1145/3688863.3689575>

```
Vision Encoder â†’ [Phi-2] â†’ ì‘ë‹µ
```

**íŠ¹ì§•:**

* ì‘ì€ backbone + í–¥ìƒëœ vocabulary
* ë” ë‚˜ì€ ì¼ë°˜í™” ì„±ëŠ¥ ì¶”êµ¬

---

#### TinyLLaVA

**ëª©í‘œ:** ì•„í‚¤í…ì²˜ ì„ íƒê³¼ í•™ìŠµ ìµœì í™” ì—°êµ¬

> <https://arxiv.org/abs/2402.14289>

**íƒêµ¬ ë‚´ìš©:**

* ì–´ë–¤ Vision Encoderê°€ ìµœì ì¸ê°€?
* ì–´ë–¤ Projectorê°€ ìµœì ì¸ê°€?
* ì–´ë–¤ í•™ìŠµ ì „ëµì´ ìµœì ì¸ê°€?

**ê²°ë¡ **: ì‘ì€ ëª¨ë¸ë„ ìµœì í™”í•˜ë©´ í° ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ ê°€ëŠ¥

---

#### MoE-LLaVA

**ëª©í‘œ:** Mixture of Expertsë¡œ íš¨ìœ¨ì„± í–¥ìƒ

> <https://arxiv.org/abs/2401.15947>

```
ì¼ë°˜ LLM:    ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ í•­ìƒ í™œì„±í™”

MoE-LLM:    Expert 1  Expert 2  Expert 3  Expert 4
                â†‘         â†‘
            Routerê°€ ì„ íƒí•œ Expertë§Œ í™œì„±í™” (sparse)
```

**íŠ¹ì§•:**

* ì „ì²´ íŒŒë¼ë¯¸í„°ëŠ” ë§ì§€ë§Œ, ì¶”ë¡  ì‹œ **ì¼ë¶€ë§Œ ì‚¬ìš©**
* ê³„ì‚°ëŸ‰ ê°ì†Œ + ì„±ëŠ¥ ìœ ì§€

---

### 2. Quantization & Compression

#### 4/8-bit Quantization

**ëª©í‘œ:** íŒŒë¼ë¯¸í„° ì •ë°€ë„ë¥¼ ë‚®ì¶° ë©”ëª¨ë¦¬/ì—°ì‚° ì ˆì•½

```
ê¸°ì¡´ (FP16):
W = [0.1234, -0.5678, 0.9012, ...]  â† ê° ìˆ«ìê°€ 16 bits

INT8 Quantization:
W = [0.12, -0.57, 0.90, ...]        â† ê° ìˆ«ìê°€ 8 bitsë¡œ ê·¼ì‚¬

INT4 Quantization:  
W = [0.1, -0.6, 0.9, ...]           â† ê° ìˆ«ìê°€ 4 bitsë¡œ ê·¼ì‚¬
```

**í•œê³„:**

* ì••ì¶• ëŒ€ìƒ: ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° (weights)
* í† í° ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ â†’ attention ì—°ì‚°ëŸ‰ ë™ì¼

---

### 3. Vision-Language Connectors

Vision Encoder ì¶œë ¥ì„ LLM ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆë“¤ì…ë‹ˆë‹¤.

#### MLP Projector (LLaVA)

> <https://arxiv.org/abs/2304.08485>

```
Visual Token (1024-dim) â†’ [Linear â†’ GELU â†’ Linear] â†’ LLM Token (4096-dim)
```

**íŠ¹ì§•:**

* ê°€ì¥ ë‹¨ìˆœí•œ êµ¬ì¡°
* **í† í° ìˆ˜ ë³€í™” ì—†ìŒ** (576 â†’ 576)

---

#### Q-Former (BLIP-2)

> <https://arxiv.org/abs/2301.12597>

```
Visual Tokens (576ê°œ)
        â†“
   [Q-Former]  â† Learnable Query Tokens (32ê°œ)ì™€ cross-attention
        â†“
Query Outputs (32ê°œ)
```

**íŠ¹ì§•:**

* Learnable queryê°€ visual ì •ë³´ë¥¼ "ì§ˆì˜"
* **í† í° ìˆ˜ ê°ì†Œ** (576 â†’ 32)
* í•˜ì§€ë§Œ ê³ ì •ëœ ìˆ˜ì˜ query ì‚¬ìš© (adaptive ì•„ë‹˜)

---

#### Resampler (Flamingo)

> <https://arxiv.org/abs/2204.14198>

```
Visual Tokens (ê°€ë³€)
        â†“
   [Perceiver Resampler]  â† Latent Tokensì™€ cross-attention
        â†“
Fixed-size Output (64ê°œ)
```

**íŠ¹ì§•:**

* ë‹¤ì–‘í•œ í•´ìƒë„ ì…ë ¥ ì²˜ë¦¬ ê°€ëŠ¥
* ê³ ì •ëœ ìˆ˜ì˜ ì¶œë ¥ í† í°

---

### Connector ë¹„êµ

| Connector | ì…ë ¥ í† í° | ì¶œë ¥ í† í° | Adaptive? |
| --- | --- | --- | --- |
| MLP (LLaVA) | 576 | 576 | âœ— |
| Q-Former (BLIP-2) | 576 | 32 | âœ— (ê³ ì •) |
| Resampler (Flamingo) | ê°€ë³€ | 64 | âœ— (ê³ ì •) |
| **PruMerge** | 576 | **ì•½ 32**(ìœ ë™ì ) | **âœ“ (ì ì‘ì )** |

---

2.2 Token Reduction Methods
---------------------------

### Sparse Attention

#### Linformer

* **ë¬¸ì œ:** Self-attentionì˜ O(NÂ²) ë³µì¡ë„
* **í•´ê²°:** Key, Valueë¥¼ ì €ì°¨ì›ìœ¼ë¡œ **projection**

> <https://arxiv.org/abs/2006.04768>

```
ê¸°ì¡´ Attention:
Q (NÃ—d) @ K^T (dÃ—N) = NÃ—N í–‰ë ¬  â†’ O(NÂ²)

Linformer:
K' = E @ K  (kÃ—d, where k << N)
V' = F @ V  (kÃ—d)
Q @ K'^T = NÃ—k í–‰ë ¬  â†’ O(NÃ—k) â‰ˆ O(N)
```

![](https://velog.velcdn.com/images/euisuk-chung/post/729d060a-eed9-4718-bc8f-b27da6f7a08e/image.png)

**í•œê³„:** LMMì— ì§ì ‘ ì ìš© ì–´ë ¤ì›€ (prefix êµ¬ì¡°)

---

#### ReFormer (Reformer)

* **ë¬¸ì œ:** ê¸´ ì‹œí€€ìŠ¤ì˜ attention ë¹„ìš©
* **í•´ê²°:** **Locality-Sensitive Hashing (LSH)** ì‚¬ìš©

> <https://arxiv.org/pdf/2001.04451>

```
ê¸°ì¡´: ëª¨ë“  í† í° ìŒì˜ attention ê³„ì‚°

ReFormer: 
  1. LSHë¡œ ìœ ì‚¬í•œ í† í°ë¼ë¦¬ bucket ë¶„ë¥˜
  2. ê°™ì€ bucket ë‚´ì—ì„œë§Œ attention ê³„ì‚°
```

![](https://velog.velcdn.com/images/euisuk-chung/post/64d6fb26-a4ee-44d8-b0bd-1f66e7d756e0/image.png)

```
[í† í°ë“¤] â†’ [LSH Hashing] â†’ [Bucket 1] [Bucket 2] [Bucket 3]
                              â†“          â†“          â†“
                           Attention  Attention  Attention
                           (ë‚´ë¶€ë§Œ)   (ë‚´ë¶€ë§Œ)   (ë‚´ë¶€ë§Œ)
```

**í•œê³„:** ì—¬ì „íˆ ëª¨ë“  í† í° ìœ ì§€, ì—°ì‚° ë°©ì‹ë§Œ ë³€ê²½

---

### Token Merging

#### ToMe (Bolya et al., 2023)

* **ëª©í‘œ:** ViT ë‚´ë¶€ì—ì„œ ì ì§„ì ìœ¼ë¡œ í† í° ìˆ˜ ê°ì†Œ
* **ë°©ë²•:** Bipartite Matchingìœ¼ë¡œ ìœ ì‚¬í•œ í† í° ë³‘í•©

> <https://arxiv.org/abs/2210.09461>

```
ViT Block 1: 576 tokens
      â†“ (merge)
ViT Block 2: 500 tokens
      â†“ (merge)
ViT Block 3: 450 tokens
      â†“ (merge)
...
ìµœì¢…: 1 token (class token)
```

**Bipartite Matching:**

```
í† í°ë“¤ì„ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• :

Group A: [T1, T3, T5, ...]
Group B: [T2, T4, T6, ...]

ìœ ì‚¬í•œ ìŒ ë§¤ì¹­ í›„ ë³‘í•©:
T1 + T2 â†’ T'1
T3 + T4 â†’ T'2
...
```

---

### ê¸°ì¡´ Token Reduction vs PruMerge ë¹„êµ

| í•­ëª© | ToMe (ê¸°ì¡´) | PruMerge |
| --- | --- | --- |
| **ì ìš© ìœ„ì¹˜** | ViT ë‚´ë¶€ (layer-by-layer) | ViT ì¶œë ¥ í›„ (í•œ ë²ˆì—) |
| **ëª©í‘œ** | ViT ì—°ì‚° ê°€ì† | **LLM ì—°ì‚° ê°€ì†** |
| **ì¶œë ¥** | Single [CLS] token | **Multiple visual tokens** |
| **ê°ì†Œ ë°©ì‹** | ì ì§„ì  (576â†’500â†’450â†’...) | í•œ ë²ˆì— (576â†’32) |
| **Adaptive** | âœ— (ê³ ì • ë¹„ìœ¨) | **âœ“ (ì´ë¯¸ì§€ë³„ ë‹¤ë¦„)** |

---

3. Method: Token Pru-Merging
----------------------------

### 3.1 Preliminaries

#### Vision Transformers (ViTs)

![](https://velog.velcdn.com/images/euisuk-chung/post/122988b2-0a77-413d-bf36-888547728b13/image.png)

**êµ¬ì¡°**:

```
Input Image
  â†“ (Patch embedding)
Patch Tokens (576 tokens for 336Ã—336 image with 14Ã—14 patches)
  + Class Token ([CLS])
  â†“
Transformer Blocks (Ã—24 for ViT-L/14)
  â”œâ”€ Multi-head Self-Attention (MSA)
  â”œâ”€ Feed-Forward Network (FFN)
  â”œâ”€ Skip connections
  â””â”€ Layer Normalization
  â†“
Output Tokens
```

**Self-Attention ë©”ì»¤ë‹ˆì¦˜**:

```
# Query, Key, Value ê³„ì‚°
Q = X Â· Wq
K = X Â· Wk
V = X Â· Wv

# Attention ê³„ì‚°
A = softmax(Q Â· K^T / âˆšdk)
Y = A Â· V
```

**Class Token Attention**:

```
# [CLS] tokenê³¼ visual tokens ê°„ì˜ attention
a_cls = softmax(q_cls Â· K^T / âˆšdk)
```

> **í•µì‹¬ ê´€ì°°**:
>
> * `a_cls`ì˜ ë¶„í¬ê°€ **ë§¤ìš° sparse**
> * ì†Œìˆ˜ì˜ visual tokensë§Œ ë†’ì€ attention ê°’
> * ëŒ€ë¶€ë¶„ì˜ tokensëŠ” near-zero attention

#### Large Multimodal Models (LMMs)

![](https://velog.velcdn.com/images/euisuk-chung/post/68aa5123-7656-42d1-bbc7-1b8d156563c1/image.png)

**Pipeline**:

```
Image X_v â†’ [Vision Encoder] â†’ Z_v â†’ [Projector W] â†’ H_v
                                                       â†“
Text X_q  â†’ [Tokenizer] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  H_q
                                                       â†“
                                              [LLM f_Î¸] â†’ Response Y_a
```

**Computational Cost**:

* N tokens â†’ N Ã— N attention matrix
* **Quadratic complexity**: O(NÂ²)
* Visual tokensê°€ ë§ì„ìˆ˜ë¡ ë¹„ìš© ê¸‰ì¦

> ğŸ’¼ **LLaVa PruMerge ëª©í‘œ**: Visual tokens ìˆ˜ë¥¼ ì¤„ì—¬ LLMì˜ computational cost ê°ì†Œ

![](https://velog.velcdn.com/images/euisuk-chung/post/cae273e6-bbf3-44ad-8901-27621829a3a3/image.png)

---

### 3.2 Adaptive Important Token Selection via Outlier Detection

> **í•µì‹¬ ì§ˆë¬¸**: "ê° visual tokenì˜ ì¤‘ìš”ë„ë¥¼ ì–´ë–»ê²Œ íŒë‹¨í•˜ëŠ”ê°€?"

---

#### ë‘ ê°€ì§€ ê·¹ë‹¨ì  íŒ¨ëŸ¬ë‹¤ì„

| íŒ¨ëŸ¬ë‹¤ì„ | í† í° ìˆ˜ | íŠ¹ì§• |
| --- | --- | --- |
| **LMM** | 576ê°œ (ì „ë¶€ ì‚¬ìš©) | ìƒì„¸í•œ ì‹œê° ì •ë³´ í‘œí˜„ |
| **CLIP** | 1ê°œ ([CLS]ë§Œ ì‚¬ìš©) | ê°€ì¥ ì••ì¶•ëœ ì •ë³´ í‘œí˜„ |

---

#### ê· í˜•ì  íƒìƒ‰: [CLS]-Visual Attention ì¡°ì‚¬

ì´ ë‘ ê·¹ë‹¨ì˜ **ê· í˜•ì **ì„ ì°¾ê¸° ìœ„í•´, `[CLS] token`ê³¼ `visual tokens` ê°„ì˜ attentionì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/269c494c-2413-49ca-bbdb-f104e2767b44/image.png)

**ê´€ì°° ê²°ê³¼ (Figure 3a):**

![](https://velog.velcdn.com/images/euisuk-chung/post/b4393a88-ae25-432b-b1b5-11dafabfd8f6/image.png)

* Yì¶•: Log(Class Attention Value)
* Xì¶•: Visual Token Index (0-575)

**ë¶„í¬ íŠ¹ì„±**:

* ëŒ€ë¶€ë¶„ì˜ tokens: near-zero attention
* ì†Œìˆ˜ì˜ tokens: ë§¤ìš° ë†’ì€ attention (outliers)

> **í•µì‹¬ ë°œê²¬:** Attention ë¶„í¬ê°€ **ë§¤ìš° sparse**í•¨  
> â†’ ì†Œìˆ˜ì˜ visual tokensë§Œ í•µì‹¬ ì‹œê° ì •ë³´ì™€ ì—°ê´€ë¨

**ê´€ì°° ê²°ê³¼ (Figure 3b):**

![](https://velog.velcdn.com/images/euisuk-chung/post/9bbf3b03-8510-4162-ba3b-af7b5a630bab/image.png)

* `PruMerge`: ì •ë³´ê°€ ì¤‘ìš”í•œ ê³³ë§Œ ì„ íƒ â†’ íš¨ìœ¨ì ì´ì§€ë§Œ ì¼ë¶€ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥
* `PruMerge+`: ì¤‘ìš”í•œ ê³³ + ê· ë“± ìƒ˜í”Œë§ â†’ ì•½ê°„ì˜ í† í° ì¦ê°€ë¡œ ì»¤ë²„ë¦¬ì§€ ë³´ì¥

---

#### IQR (Interquartile Range) ê¸°ë°˜ Outlier Detection

Sparseí•œ attention ë¶„í¬ì—ì„œ **outlier = ì¤‘ìš”í•œ í† í°**ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

**ì•Œê³ ë¦¬ì¦˜:**

```
# 1. Attention ê°’ì˜ quartiles ê³„ì‚°
Q1 = percentile(a_cls, 25)  # 1ì‚¬ë¶„ìœ„ìˆ˜
Q3 = percentile(a_cls, 75)  # 3ì‚¬ë¶„ìœ„ìˆ˜

# 2. IQR ê³„ì‚°
IQR = Q3 - Q1

# 3. Upper fence (threshold) ê³„ì‚°
upper_fence = Q3 + 1.5 * IQR

# 4. Outliers = Important tokens
important_indices = where(a_cls > upper_fence)
```

![](https://velog.velcdn.com/images/euisuk-chung/post/2a174ade-41cb-4bed-baa4-d43577580666/image.png)

> <https://docs.oracle.com/cloud/help/ko/pbcs_common/PFUSU/insights_metrics_IQR.htm#PFUSU-GUID-CF37CAEA-730B-4346-801E-64612719FF6B>

**ì™œ IQRì¸ê°€?**

* Attention scoreëŠ” ì–‘ìˆ˜ì´ë¯€ë¡œ **upper fenceë§Œ ì‚¬ìš©**
* ê° ì´ë¯¸ì§€ì˜ ë¶„í¬ì— ë”°ë¼ **thresholdê°€ ìë™ ì¡°ì ˆ**
* í†µê³„ì ìœ¼ë¡œ ê²€ì¦ëœ robustí•œ outlier detection

---

#### Adaptive Selectionì˜ íŠ¹ì„±

**ì´ë¯¸ì§€ ë³µì¡ë„ì— ë”°ë¥¸ ìë™ ì¡°ì ˆ:**

| ì´ë¯¸ì§€ ìœ í˜• | íŠ¹ì„± | ì„ íƒ í† í° ìˆ˜ |
| --- | --- | --- |
| ë³µì¡í•œ ì´ë¯¸ì§€ (í…ìŠ¤íŠ¸ å¤š) | Attention outlier å¤š | ë§ìŒ (40-50ê°œ) |
| ë‹¨ìˆœí•œ ì´ë¯¸ì§€ (í•˜ëŠ˜+ê°„íŒ) | Attention outlier å°‘ | ì ìŒ (10-20ê°œ) |

![](https://velog.velcdn.com/images/euisuk-chung/post/2db9c26f-e611-4d91-81f0-272a3928f10b/image.png)

**ë²¤ì¹˜ë§ˆí¬ë³„ í‰ê·  í† í° ìˆ˜ (Table 4):**

ë¹„êµ ëŒ€ìƒ (ë™ì¼í•œ í† í° ìˆ˜ì—ì„œ):

| ë°©ë²• | ì„¤ëª… |
| --- | --- |
| **LLaVA-PruMerge** | IQR ê¸°ë°˜ adaptive selection |
| **Sequential** | ì•ì—ì„œë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ Nê°œ ì„ íƒ |
| **Spatial** | ê³µê°„ì ìœ¼ë¡œ ê· ë“±í•˜ê²Œ Nê°œ ì„ íƒ (ì˜ˆ: 5Ã—8, 8Ã—5) |

```
Sequential ì„ íƒ:
[T1, T2, T3, ..., T40] â† ì•ìª½ 40ê°œë§Œ ì„ íƒ

ì´ë¯¸ì§€ íŒ¨ì¹˜ ìˆœì„œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T1  T2  T3  ... T24    â”‚ â† ìƒë‹¨ë§Œ ì„ íƒë¨
â”‚ T25 T26 T27 ... T48    â”‚
â”‚ ...                    â”‚ â† í•˜ë‹¨ì€ ì™„ì „ ë¬´ì‹œ
â”‚ T553 ... T576          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
Spatial ì„ íƒ (5Ã—8 = 40):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–   Â·  Â·  Â·  â–   Â·  Â·  Â· â”‚
â”‚ Â·  Â·  Â·  Â·  Â·  Â·  Â·  Â· â”‚
â”‚ â–   Â·  Â·  Â·  â–   Â·  Â·  Â· â”‚
â”‚ Â·  Â·  Â·  Â·  Â·  Â·  Â·  Â· â”‚
â”‚ â–   Â·  Â·  Â·  â–   Â·  Â·  Â· â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
PruMerge ì„ íƒ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Â·  Â·  â–   â–   â–   Â·  Â·  Â· â”‚
â”‚ Â·  Â·  â–   â–   â–   Â·  Â·  Â· â”‚  â† í…ìŠ¤íŠ¸/ê°ì²´ ì˜ì—­ì— ì§‘ì¤‘
â”‚ Â·  Â·  â–   â–   â–   Â·  Â·  Â· â”‚
â”‚ Â·  Â·  Â·  Â·  Â·  Â·  Â·  Â· â”‚
â”‚ Â·  Â·  Â·  Â·  Â·  Â·  Â·  Â· â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> Task: TextVQA (40 tokens)

| Approach | Performance |
| --- | --- |
| **LLaVA-PruMerge** | **54.00** |
| Sequential | 42.72 |
| Spatial (5Ã—8) | 46.85 |
| Spatial (8Ã—5) | 47.42 |

> Task: MME (40 tokens)

| Approach | Performance |
| --- | --- |
| **LLaVA-PruMerge** | **1250.07** |
| Sequential | 703.60 |
| Spatial (5Ã—8) | 1180.23 |
| Spatial (8Ã—5) | 1142.32 |

> Task: POPE (35 tokens)

| Approach | Performance |
| --- | --- |
| **LLaVA-PruMerge** | **76.2** |
| Sequential | 11.7 |
| Spatial (5Ã—7) | 69.8 |
| Spatial (7Ã—5) | 71.1 |

> Task: ScienceQA (16 tokens)

| Approach | Performance |
| --- | --- |
| **LLaVA-PruMerge** | **68.07** |
| Sequential | 64.20 |
| Spatial (4Ã—4) | 66.29 |

---

#### Penultimate Layer ì‚¬ìš©

**ì™œ ë§ˆì§€ë§‰ layerê°€ ì•„ë‹Œ penultimate (ëì—ì„œ ë‘ ë²ˆì§¸) layer?**

* ë§ˆì§€ë§‰ layer: Classificationì— íŠ¹í™”
* Penultimate layer: **ë” richí•œ feature representation ë³´ìœ **

---

### 3.3 Token Supplement via Similar Key Clustering

> *"While pruned tokens may initially seem extraneous, they hold potential value for the perception capabilities of the LLM backbone."*

#### ë¬¸ì œ: Pruned Tokensì˜ ì •ë³´ ì†ì‹¤

Pruned tokensë¥¼ ì™„ì „íˆ ë²„ë¦¬ë©´:

* í° ê°ì²´ê°€ sceneì„ ì§€ë°°í•˜ëŠ” ê²½ìš° ì •ë³´ ì†ì‹¤
* ëª¨ë¸ì˜ representation ëŠ¥ë ¥ ì €í•˜ ê°€ëŠ¥

ì˜ˆì‹œ: í° ê°ì²´ê°€ í™”ë©´ì„ ì§€ë°°í•˜ëŠ” ê²½ìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜      â”‚
â”‚  ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜      â”‚
â”‚  ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜      â”‚  â† ì½”ë¼ë¦¬ê°€ ì´ë¯¸ì§€ ëŒ€ë¶€ë¶„ ì°¨ì§€
â”‚  ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜      â”‚
â”‚  ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿ ğŸŒ¿      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

IQR Outlier Selection ê²°ê³¼:

* ì½”ë¼ë¦¬ì˜ ëˆˆ, ê·€ ë“± íŠ¹ì§•ì ì¸ ë¶€ë¶„ë§Œ ì„ íƒ (5-10ê°œ)
* ë‚˜ë¨¸ì§€ ì½”ë¼ë¦¬ ëª¸í†µ ë¶€ë¶„ì€ prunedë¨

ë¬¸ì œ: ì½”ë¼ë¦¬ ì „ì²´ë¥¼ í‘œí˜„í•˜ê¸°ì— ì •ë³´ ë¶€ì¡±

**í•´ê²°ì±…:** Pruned tokensë¥¼ ë²„ë¦¬ì§€ ì•Šê³  ì„ íƒëœ í† í°ì— **ë³‘í•©(merge)**í•´ì£¼ë©´, ê·¸ íŠ¹ì§•ì„ ì‚´ë ¤ì¤„ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

---

#### Token Similarity ì¸¡ì •: Key Vector í™œìš©

> *"Since the key vector of each patch token already contains information summarized in the self-attention module, the final layer's key vector serves as the representation."*

**ì™œ Key vectorì¸ê°€?**

* Self-attentionì—ì„œ key vectorëŠ” ì´ë¯¸ í•´ë‹¹ í† í°ì˜ ì •ë³´ë¥¼ ìš”ì•½
* ë³„ë„ ê³„ì‚° ì—†ì´ ì¬ì‚¬ìš© ê°€ëŠ¥

**Similarity ê³„ì‚°:**

Sim(yi,yj)=kiâ‹…kjT\text{Sim}(y\_i, y\_j) = \mathbf{k}\_i \cdot \mathbf{k}\_j^TSim(yiâ€‹,yjâ€‹)=kiâ€‹â‹…kjTâ€‹

ì „ì²´ í† í°ì— ëŒ€í•´ ë²¡í„°í™”: KKT\mathbf{K}\mathbf{K}^TKKT

```
Similarity Matrix (576 Ã— 576):
# K: ëª¨ë“  í† í°ì˜ Key vectors [576, d_k]
# d_k: key dimension (ì˜ˆ: 64)

              T0     T1     T2     T3    ...   T575
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   T0   â”‚  1.0    0.3    0.1    0.8   ...   0.2  â”‚
   T1   â”‚  0.3    1.0    0.7    0.2   ...   0.4  â”‚
   T2   â”‚  0.1    0.7    1.0    0.1   ...   0.3  â”‚
   T3   â”‚  0.8    0.2    0.1    1.0   ...   0.5  â”‚
   ...  â”‚  ...    ...    ...    ...   ...   ...  â”‚
   T575 â”‚  0.2    0.4    0.3    0.5   ...   1.0  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

similarity_matrix[i][j] = token iì™€ token jì˜ ìœ ì‚¬ë„
```

---

#### K-Nearest Neighbor Clustering & Weighted Merge

**ê³¼ì •:**

```
def token_merge(K, Y, a_cls, unpruned_indices, k=32):
    """
    K: Key vectors [576, d_k]
    Y: Token features [576, d]
    a_cls: Class attention [576]
    unpruned_indices: IQRë¡œ ì„ íƒëœ ì¸ë±ìŠ¤ [m]
    k: neighbors ìˆ˜
    """
    
    # Step 1: ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    similarity_matrix = K @ K.T  # [576, 576]
    
    # Step 2: ê° centerì— ëŒ€í•´ merge
    merged_tokens = []
    
    for p in unpruned_indices:
        # ìœ ì‚¬ë„ ê¸°ë°˜ k-nearest neighbors
        sims = similarity_matrix[p]            # [576]
        neighbor_idx = argsort(sims)[-k:]      # top-k indices
        
        # Class attention ê°€ì¤‘ì¹˜
        weights = a_cls[neighbor_idx]          # [k]
        
        # Weighted sum
        merged = (weights @ Y[neighbor_idx]) / weights.sum()
        merged_tokens.append(merged)
    
    return stack(merged_tokens)  # [m, d]
```

**í•µì‹¬:**

* ì„ íƒëœ í† í° = Cluster center
* Pruned tokens = ê°€ì¥ ìœ ì‚¬í•œ centerì— ë³‘í•©
* **Class attention**ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© â†’ ì¤‘ìš”í•œ ì •ë³´ ë” ë§ì´ ë°˜ì˜

> (ê°œì¸ì˜ê²¬) ì½”ë“œ êµ¬í˜„ì²´ì—ì„œëŠ” kë¥¼ 32ë¡œ ê³ ì •í•´ë‘ëŠ”ë°, ì´ë¥¼ dynamicí•˜ê²Œ ë°”ê¾¸ëŠ”ê²Œ ë” ë§ì§€ ì•Šì„ê¹Œ? ğŸ¤” [(ë§í¬)](https://github.com/42Shawn/LLaVA-PruMerge/blob/main/llava/model/multimodal_encoder/clip_encoder.py)

```
# 1. Cosine Similarity ê³„ì‚° (KK^T ëŒ€ì‹  normalized dot product)
cos_sim_matrix = torch.bmm(key_others_norm, rest_Keys.transpose(1, 2))
## bmm : Batch ë‹¨ìœ„ë¡œ í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜

# 2. Top-k Nearest Neighbors ì„ íƒ â† ì´ê²Œ KNN!
_, cluster_indices = torch.topk(cos_sim_matrix, k=int(32), dim=2, largest=True)
## topk: Tensorì—ì„œ ê°€ì¥ í° (ë˜ëŠ” ì‘ì€) kê°œì˜ ê°’ê³¼ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
```

---

### 3.4 PruMerge+: Bridging the Efficiency-Performance Gap

#### ë¬¸ì œ: PruMergeì˜ ì„±ëŠ¥ ê²©ì°¨

PruMergeëŠ” **~14ë°° ì••ì¶• (5.5% tokens)**ì„ ë‹¬ì„±í•˜ì§€ë§Œ:

* ì›ë³¸ LLaVA ëŒ€ë¹„ **marginal performance drop** ë°œìƒ
* íŠ¹ì • ì˜ì—­ì— í† í°ì´ í¸ì¤‘ë  ìˆ˜ ìˆìŒ

---

#### í•´ê²°ì±…: Spatial Uniform Sampling ì¶”ê°€

**PruMerge+ ì „ëµ:**

```
Final Tokens = Attention-based Outliers + Spatially-uniform Samples
```

**ì•Œê³ ë¦¬ì¦˜:**

```
# Step 1: IQRë¡œ outlier ratio ê³„ì‚°
if if_adaptive:
    reduction_ratio = outlier_dectection(cls_attn)  # ì˜ˆ: 0.05 (5%)

# Step 2: Top-kë¡œ outlier ì„ íƒ
_, idx = torch.topk(cls_attn, int(N * reduction_ratio), dim=1, largest=True)
# idx: [B, ~32] â† IQR ê¸°ë°˜ ì„ íƒëœ ì¸ë±ìŠ¤

# Step 3: Spatial Uniform Sampling
if if_adaptive:
    step_length = int(1 / reduction_ratio)  # ì˜ˆ: 1/0.05 = 20
    
    # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§ (step_length/3 ê°„ê²©)
    arithmetic_sequence = torch.arange(0, 575, int(step_length / 3))
    # ì˜ˆ: step=20 â†’ step/3â‰ˆ6 â†’ [0, 6, 12, 18, 24, ..., 570]
    
    # ì´ë¯¸ ì„ íƒëœ ì¸ë±ìŠ¤ ì œì™¸ (ì¤‘ë³µ ì œê±°)
    original_tensor_1d = idx.flatten()
    filtered_sequence = [x for x in arithmetic_sequence if x not in original_tensor_1d]
    
    # Step 4: Union (í•©ì§‘í•©)
    concatenated_tensor = torch.cat((idx, filtered_sequence.unsqueeze(0)), dim=1)
    idx = concatenated_tensor  # ìµœì¢… ì¸ë±ìŠ¤
```

**íš¨ê³¼:**

* ê³µê°„ì ìœ¼ë¡œ underrepresented ì˜ì—­ ë³´ì™„
* ë” comprehensiveí•œ visual representation

---

#### PruMerge vs PruMerge+ ë¹„êµ

| í•­ëª© | PruMerge | PruMerge+ |
| --- | --- | --- |
| **ì••ì¶•ë¥ ** | ~14Ã— (5.5%) | ~4Ã— (25%) |
| **ì„ íƒ ë°©ì‹** | IQR Outlierë§Œ | Outlier + Spatial Uniform |
| **ê³µê°„ ì»¤ë²„ë¦¬ì§€** | í¸ì¤‘ ê°€ëŠ¥ | ê· ë“± ë³´ì¥ |

**ì„±ëŠ¥ ë¹„êµ (Vicuna-7B):**

| Metric | LLaVA-1.5 | PruMerge | PruMerge+ |
| --- | --- | --- | --- |
| VQAv2 | 78.5 | 72.0 | **76.8** |
| ScienceQA | 66.8 | 68.5 | 68.3 |
| TextVQA | 58.2 | 56.0 | 57.1 |
| POPE | 85.9 | 76.3 | **84.0** |
| MME | 1510.7 | 1350.3 | **1462.4** |
| MMBench | 64.3 | 60.9 | **64.9** |

**Trade-off:**

* **PruMerge**: ìµœëŒ€ íš¨ìœ¨ì„± (14Ã— ì••ì¶•), ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜
* **PruMerge+**: íš¨ìœ¨ì„± + ì„±ëŠ¥ ê· í˜• (4Ã— ì••ì¶•, ê±°ì˜ ì›ë³¸ ì„±ëŠ¥)

---

### 3.5 Algorithm Summary

**Algorithm 1: Token PruMerge and PruMerge+**

```
# Input: K, Q (penultimate layer), Y (output tokens), n (token count)
# Output: Y' (m tokens, m << n)

def token_prumerge(K, Q, Y, n):
    # Step 1: Calculate class attention
    a_cls = calculate_attention(Q_cls, K)  # Eq 3.2

    # Step 2: Adaptive token selection via IQR
    indices = IQR_outlier_detection(a_cls)  # Sec 3.2
    m = len(indices)
    selected_indices = indices

    # Step 3 (Optional - PruMerge+): Spatial sampling
    if PRUMERGE_PLUS:
        r_o = m / n
        spatial_indices = spatial_uniform_sample(r_o)
        selected_indices = indices + spatial_indices
        m = len(selected_indices)

    # Step 4: Token merging via k-NN clustering
    Y_prime = []
    for p in selected_indices:
        # Calculate similarity
        similarities = cosine_similarity(
            K[p],
            K[others]
        )

        # Find k nearest neighbors
        neighbor_indices = topk(similarities, k=32)

        # Weighted averaging
        weights = a_cls[neighbor_indices]
        y_p_prime = weighted_average(
            Y[neighbor_indices],
            weights=weights
        )

        Y_prime.append(y_p_prime)

    return Y_prime  # m tokens
```

**í•µì‹¬ ë‹¨ê³„**:  
1. **AITS**: IQRë¡œ ì¤‘ìš” í† í° ì„ íƒ  
2. **(Optional)** Spatial sampling  
3. **TS**: k-NN clustering + weighted merging

---

4. Experiments
--------------

### 4.1 Main Results

#### ì‹¤í—˜ ì„¤ì •

**Base Model**: LLaVA-1.5 (7B, 13B)

* CLIP ViT-L/14 vision encoder
* Vicuna-7B / Vicuna-13B LLM
* 336Ã—336 resolution
* ì›ë³¸: 576 visual tokens

**Training**:

* LoRA fine-tuning (1 epoch)
* LLaVA-1.5 instruction data ì‚¬ìš©
* Reduced visual tokensë¡œ í•™ìŠµ

**Evaluation Benchmarks**:  
1. **VQAv2**: Visual question answering  
2. **ScienceQA (SQAI)**: Multimodal reasoning  
3. **TextVQA (VQAT)**: OCR-based QA  
4. **POPE**: Hallucination evaluation  
5. **MME**: Perception & cognition  
6. **MMBench (MMB)**: Comprehensive evaluation

#### ì„±ëŠ¥ ë¹„êµ

**Table 1: 6ê°œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼**

| Method | LLM | VQAv2 | SQAI | VQAT | POPE | MME | MMB |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **Existing Methods** |  |  |  |  |  |  |  |
| BLIP-2 | Vicuna-13B | 41.0 | 61.0 | 42.5 | 85.3 | 1293.8 | - |
| InstructBLIP | Vicuna-13B | - | 63.1 | 50.7 | 78.9 | 1212.8 | - |
| Qwen-VL-Chat | Qwen-7B | 78.2 | 68.2 | 61.5 | - | 1487.5 | 60.6 |
| **LLaVA-1.5 Baselines** |  |  |  |  |  |  |  |
| LLaVA-1.5 | Vicuna-7B | 78.5 | 66.8 | 58.2 | 85.9 | 1510.7 | 64.3 |
| + PruMerge (5.5%) | Vicuna-7B | 72.0 | **68.5** | 56.0 | 76.3 | 1350.3 | 60.9 |
| + PruMerge+ (25%) | Vicuna-7B | 76.8 | **68.3** | 57.1 | 84.0 | 1462.4 | 64.9 |
| LLaVA-1.5 | Vicuna-13B | 80.0 | 71.6 | 61.3 | 85.9 | 1531.3 | 67.7 |
| + PruMerge (5.5%) | Vicuna-13B | 72.8 | 71.0 | 58.4 | 78.5 | 1428.2 | 62.3 |
| + PruMerge+ (25%) | Vicuna-13B | 77.8 | 71.0 | 58.6 | 84.4 | 1485.5 | 65.7 |

**ì£¼ìš” ë°œê²¬**:

1. **PruMerge+ (25% tokens)**:

   * VQAv2: 76.8 (ì›ë³¸ 78.5 ëŒ€ë¹„ -1.7)
   * **ScienceQA: 68.3** (ì›ë³¸ 66.8 ëŒ€ë¹„ **+1.5**)
   * MME: 1462.4 (ì›ë³¸ 1510.7 ëŒ€ë¹„ -48.3)
   * MMBench: 64.9 (ì›ë³¸ 64.3 ëŒ€ë¹„ +0.6)
   * **â†’ Comparable performance**
2. **PruMerge (5.5% tokens)**:

   * **ScienceQA: 68.5** (ì›ë³¸ ëŒ€ë¹„ **+1.7**)
   * POPE: 76.3 (ì›ë³¸ 85.9 ëŒ€ë¹„ -9.6)
   * **â†’ ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒ!**
3. **vs. Previous Methods**:

   * BLIP-2, InstructBLIP ëŒ€ë¹„ **í›¨ì”¬ ìš°ìˆ˜**
   * Qwen-VL-Chatê³¼ comparable

#### ì™œ ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒ?

**ScienceQAì—ì„œ í–¥ìƒ ì´ìœ **:

* ì¤‘ìš”í•œ ì‹œê° ì •ë³´ì— **ì§‘ì¤‘**
* Redundant tokens ì œê±°ë¡œ **signal-to-noise ë¹„ìœ¨ í–¥ìƒ**
* ì¶”ë¡ ì— í•„ìš”í•œ í•µì‹¬ featuresë§Œ ì„ íƒ

**POPEì—ì„œ PruMergeê°€ ì•½í•œ ì´ìœ **:

* Object presence detection í•„ìš”
* Spatial coverage ì¤‘ìš”
* Aggressive pruning (5.5%)ìœ¼ë¡œ ì¼ë¶€ ê°ì²´ ì •ë³´ ì†ì‹¤
* â†’ PruMerge+ê°€ ì´ ë¬¸ì œ í•´ê²° (84.0)

### 4.2 Efficiency Analysis

#### Computational Cost (Table 2)

**ì‹¤í—˜ í™˜ê²½**: Tesla V100 GPU  
**ë°©ë²•ë¡ **: Roofline model ê¸°ë°˜ theoretical analysis

**LLaVA-1.5 (Vicuna-7B)**:

| Config | FLOPs (TB) | Prefill Time (ms) | Total Memory (GB) | Activation (GB) |
| --- | --- | --- | --- | --- |
| **FP16** |  |  |  |  |
| Original | 9.3 | 88.6 | 23.3 | 4.60 |
| + PruMerge | **0.91** | **15.3** | **13.7** | **0.28** |
| **Speedup** | **10.2Ã—** | **5.8Ã—** | **1.7Ã—** | **16.4Ã—** |
| **INT4** |  |  |  |  |
| Original | 2.3 | 151.6 | 5.9 | 1.20 |
| + PruMerge | **0.28** | **14.9** | **3.5** | **0.07** |
| **Speedup** | **8.2Ã—** | **10.2Ã—** | **1.7Ã—** | **17.1Ã—** |

**LLaVA-1.5 (Vicuna-13B)**:

| Config | FLOPs (TB) | Prefill Time (ms) | Total Memory (GB) | Activation (GB) |
| --- | --- | --- | --- | --- |
| **FP16** |  |  |  |  |
| Original | 18.2 | 170.5 | 41.6 | 7.30 |
| + PruMerge | **1.80** | **29.5** | **26.6** | **0.44** |
| **Speedup** | **10.1Ã—** | **5.8Ã—** | **1.6Ã—** | **16.6Ã—** |
| **INT4** |  |  |  |  |
| Original | 4.6 | 294.9 | 10.5 | 1.80 |
| + PruMerge | **0.45** | **29.0** | **6.8** | **0.11** |
| **Speedup** | **10.2Ã—** | **10.2Ã—** | **1.5Ã—** | **16.4Ã—** |

**í•µì‹¬ íš¨ìœ¨ì„± í–¥ìƒ**:

1. **FLOPs ê°ì†Œ**: ~10ë°°

   * Quadratic complexity íš¨ê³¼: O(nÂ²) â†’ O(mÂ²)
   * 576Â² â†’ 40Â² â‰ˆ 331,776 â†’ 1,600
2. **Prefill Time**: 5.8~10.2ë°° ë¹¨ë¼ì§

   * FP16: 88.6ms â†’ 15.3ms
   * INT4: 151.6ms â†’ 14.9ms
   * **INT4 + PruMergeê°€ ê°€ì¥ ë¹ ë¦„!**
3. **Memory ì ˆê°**:

   * Total: 1.5~1.7ë°° ê°ì†Œ
   * **Activation: 16ë°° ì´ìƒ ê°ì†Œ**
4. **Quantizationê³¼ì˜ ì‹œë„ˆì§€**:

   * INT4 quantization ì ìš© ì‹œ ë” ë¹ ë¥¸ ì†ë„
   * **Orthogonal techniques**ë¡œ ê²°í•© ê°€ëŠ¥

#### Scenario Analysis

**ê°€ì •**:

* Image: 336Ã—336 (576 visual tokens)
* Text prompt: 40 tokens
* PruMerge ì ìš© í›„: 40 visual tokens

**Token ìˆ˜ ë¹„êµ**:

```
Original:  576 (visual) + 40 (text) = 616 tokens
PruMerge:   40 (visual) + 40 (text) =  80 tokens

Reduction: 616 â†’ 80 (7.7Ã— fewer tokens)
```

**Attention Computation**:

```
Original:  616Â² = 379,456 operations
PruMerge:   80Â² =   6,400 operations

Speedup: 59.3Ã— in attention matrix computation
```

### 4.3 Generalization on Video-LLM

#### Video-LLaVA í†µí•©

**Video-LLaVA íŠ¹ì„±**:

* 8 frames per video clip
* 16Ã—16 patches per frame
* **2048 visual tokens** (8 Ã— 256)
* LLaVA-1.5 ëŒ€ë¹„ **4ë°° ë§ì€ tokens**

**PruMerge ì ìš©** (Training-free):

* Inference ì‹œì—ë§Œ ì ìš©
* ì¶”ê°€ í•™ìŠµ ë¶ˆí•„ìš”
* **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥**

#### ê²°ê³¼ (Table 3)

**Video QA Benchmarks**:

| Method | LLM | MSVD-QA |  | MSRVT-QA |  | ActivityNet-QA |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | Acc | Score | Acc | Score | Acc | Score |
| **Baselines** |  |  |  |  |  |  |  |
| FrozenBiLM | 1B | 32.2 | - | 16.8 | - | 24.7 | - |
| VideoChat | 7B | 56.3 | 2.8 | 45.0 | 2.5 | - | 2.2 |
| LLaMA-Adapter | 7B | 54.9 | 3.1 | 43.8 | 2.7 | 34.2 | 2.7 |
| Video-LLaMA | 7B | 51.6 | 2.5 | 29.6 | 1.8 | 12.4 | 1.1 |
| Video-ChatGPT | 7B | 64.9 | 3.3 | 49.3 | 2.8 | 35.2 | 2.7 |
| **Video-LLaVA** |  |  |  |  |  |  |  |
| Original | 7B | 70.7 | 3.9 | 59.2 | 3.5 | 45.3 | 3.3 |
| + PruMerge (12.5%) | 7B | **71.1** | 3.9 | 58.4 | 3.5 | **48.3** | **3.4** |
| + PruMerge+ (25%) | 7B | **71.1** | 3.9 | **59.3** | **3.6** | 47.7 | **3.4** |

**ë†€ë¼ìš´ ë°œê²¬**:

1. **ì„±ëŠ¥ í–¥ìƒ**:

   * MSVD-QA: 70.7 â†’ 71.1 (+0.4)
   * ActivityNet-QA: 45.3 â†’ 48.3 (+3.0)
   * **Token ê°ì†Œí–ˆëŠ”ë° ì„±ëŠ¥ í–¥ìƒ!**
2. **í† í° ì••ì¶•**:

   * Original: 2048 tokens
   * PruMerge: **256 tokens (12.5%)**
   * PruMerge+: **512 tokens (25%)**
   * **8ë°° / 4ë°° ì••ì¶•**
3. **Training-free**:

   * Video ë°ì´í„°ë¡œ ì¬í•™ìŠµ ë¶ˆí•„ìš”
   * Inference ì‹œì—ë§Œ ì ìš©
   * **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥**

**Insight**:

* Video tokensì—ë„ **significant redundancy** ì¡´ì¬
* Temporal + spatial redundancy ëª¨ë‘ í™œìš© ê°€ëŠ¥
* **Future direction**: Temporal token reduction íƒêµ¬

### 4.4 Ablation Study

#### ìš©ì–´ ì •ë¦¬

**PruMergeì˜ ë‘ ëª¨ë“ˆ**

* **AITS** : Adaptive Important Token Selection
  + IQRë¡œ ì¤‘ìš” í† í° ì„ íƒ
* **TS**: Token Supplement
  + KNNìœ¼ë¡œ pruned ì •ë³´ ë³‘í•©

#### 4.4.1 Token Sampling Strategy Analysis (Table 4)

**ë¹„êµ ì „ëµ**:

1. **LLaVA-PruMerge**: IQR-based adaptive sampling
2. **Sequential**: ì²˜ìŒ Nê°œ í† í° ì„ íƒ
3. **Spatial**: Nê°œ í† í°ì„ ê³µê°„ì ìœ¼ë¡œ ê· ë“± ë°°ì¹˜

**ê²°ê³¼ (ë™ì¼í•œ í† í° ìˆ˜ë¡œ ë¹„êµ)**:

**TextVQA (40 tokens)**:

* **PruMerge: 54.00**
* Sequential: 42.72
* Spatial 5Ã—8: 46.85
* Spatial 8Ã—5: 47.42
* **â†’ PruMergeê°€ 11.3% ë” ë†’ìŒ**

**MME (40 tokens)**:

* **PruMerge: 1250.07**
* Sequential: 703.60
* Spatial 5Ã—8: 1180.23
* Spatial 8Ã—5: 1142.32
* **â†’ PruMergeê°€ 77.7% ë” ë†’ìŒ**

**POPE (35 tokens)**:

* **PruMerge: 76.2**
* Sequential: 11.7 (!)
* Spatial 5Ã—7: 69.8
* Spatial 7Ã—5: 71.1
* Spatial 6Ã—6: 67.9
* **â†’ PruMergeê°€ 6.5ë°° ë†’ìŒ**

**ScienceQA (16 tokens)**:

* **PruMerge: 68.07**
* Sequential: 64.20
* Spatial 4Ã—4: 66.29
* **â†’ PruMergeê°€ 3.87% ë” ë†’ìŒ**

**ë¶„ì„**:

> **Sequentialì˜ ë¬¸ì œ**:

* ì²˜ìŒ Nê°œ í† í° = ì´ë¯¸ì§€ íŠ¹ì • ì˜ì—­ë§Œ
* Spatial bias ì‹¬ê°
* POPEì—ì„œ ê±°ì˜ random guess (11.7)

> **Spatialì˜ ì¥ì **:

* ì „ì²´ ì´ë¯¸ì§€ ì»¤ë²„ë¦¬ì§€
* ê· í˜•ì¡íŒ representation
* Sequentialë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜

> **PruMergeì˜ ìš°ìˆ˜ì„±**:

* **Attention-guided** selection
* ì •ë³´ ë°€ë„ ë†’ì€ ì˜ì—­ ì§‘ì¤‘
* **Adaptive** to image complexity
* íŠ¹íˆ **TextVQA (OCR)**ì—ì„œ í° ì°¨ì´
  + í…ìŠ¤íŠ¸ ì˜ì—­ì— í† í° ì§‘ì¤‘
  + ì„¸ë°€í•œ ì •ë³´ ë³´ì¡´

#### 4.4.2 Effectiveness of Each Module (Table 5)

**ì‹¤í—˜ ì„¤ì •**:

* ê³ ì •: 40 tokens (6.9%)
* Vicuna-7B ëª¨ë¸
* 4ê°œ ë²¤ì¹˜ë§ˆí¬

**Module ì¡°í•©**:

| Method | SQAI | VQAT | POPE | MME |
| --- | --- | --- | --- | --- |
| LLaVA-1.5 (baseline) | 66.8 | 58.2 | 85.9 | 1510.7 |
| **w. AITS only** | 66.5 | 54.8 | 75.7 | 1221.6 |
| **w. AITS & TS** | **68.5** | **56.0** | **76.3** | **1350.3** |

**ë¶„ì„**:

> **AITS (Adaptive Important Token Selection) ë‹¨ë…**:

* SQA: 66.5 (baseline 66.8)
* TextVQA: 54.8 (baseline 58.2)
* POPE: 75.7 (baseline 85.9)
* MME: 1221.6 (baseline 1510.7)
* **â†’ í† í° ì„ íƒë§Œìœ¼ë¡œëŠ” ì„±ëŠ¥ ì €í•˜**

> **AITS + TS (Token Supplement)**:

* **SQA: 68.5** (baseline ëŒ€ë¹„ **+1.7**)
* TextVQA: 56.0 (baseline ëŒ€ë¹„ -2.2)
* POPE: 76.3 (baseline ëŒ€ë¹„ -9.6)
* MME: 1350.3 (baseline ëŒ€ë¹„ -160.4)
* **â†’ Token mergingì´ í•„ìˆ˜ì !**

> **TSì˜ íš¨ê³¼**:

* SQA: +2.0 (66.5 â†’ 68.5)
* TextVQA: +1.2 (54.8 â†’ 56.0)
* POPE: +0.6 (75.7 â†’ 76.3)
* MME: +128.7 (1221.6 â†’ 1350.3)
* **â†’ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ê°œì„ **

> **í•µì‹¬ Insight**:

* Token selectionë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
* **Mergingì´ pruned tokens ì •ë³´ ë³´ì¡´**
* k-NN clustering + weighted averaging íš¨ê³¼

#### 4.4.3 Training Analysis (Table 6)

**ë¹„êµ**:  
1. **Training-free**: PruMergeë§Œ ì ìš©, í•™ìŠµ X  
2. **LoRA fine-tuning**: PruMerge + LoRA 1 epoch

**ê²°ê³¼ (40 tokens, Vicuna-7B)**:

| Method | SQAI | VQAT | POPE | MME |
| --- | --- | --- | --- | --- |
| LLaVA-1.5 (baseline) | 66.8 | 58.2 | 85.9 | 1510.7 |
| **w.o. LoRA-FT** | 68.0 | 54.0 | 76.2 | 1250.1 |
| **w. LoRA-FT** | **68.5** | **56.0** | **76.3** | **1350.3** |

**ë¶„ì„**:

> **Training-free ì„±ëŠ¥**:

* **SQA: 68.0** (baseline ëŒ€ë¹„ **+1.2**)
* TextVQA: 54.0 (baseline ëŒ€ë¹„ -4.2)
* POPE: 76.2 (baseline ëŒ€ë¹„ -9.7)
* MME: 1250.1 (baseline ëŒ€ë¹„ -260.6)
* **â†’ ì¼ë¶€ íƒœìŠ¤í¬ëŠ” ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥**

> **Fine-tuning íš¨ê³¼**:

* SQA: +0.5 (68.0 â†’ 68.5)
* TextVQA: +2.0 (54.0 â†’ 56.0)
* POPE: +0.1 (76.2 â†’ 76.3)
* MME: +100.2 (1250.1 â†’ 1350.3)
* **â†’ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ê°œì„ **

> **Trade-off**:

* **Training-free**: ë¹ ë¥¸ ì ìš©, ì¼ë¶€ ì„±ëŠ¥ ì €í•˜
* **Fine-tuning**: ìµœê³  ì„±ëŠ¥, ì¶”ê°€ í•™ìŠµ í•„ìš” (1 epoch)

> **ì‹¤ìš©ì  ì„ íƒ**:

* Resource ì¶©ë¶„: Fine-tuning ê¶Œì¥
* ë¹ ë¥¸ ì ìš© í•„ìš”: Training-freeë¡œ ì‹œì‘

---

5. ìš”ì•½
-----

### 5.1 Adaptive Token Selection

**í•µì‹¬ í˜ì‹ **:

* **IQR-based outlier detection**: í†µê³„ì ìœ¼ë¡œ ê²€ì¦ëœ ë°©ë²•
* **Image-specific adaptation**: ì´ë¯¸ì§€ë§ˆë‹¤ ë‹¤ë¥¸ ìˆ˜ì˜ í† í°
* **Learned importance**: ëª¨ë¸ì´ í•™ìŠµí•œ attention pattern í™œìš©

**ì¥ì **:

* Manual threshold ë¶ˆí•„ìš”
* Robust to different image types
* Computation-efficient (ë‹¨ìˆœ í†µê³„ ê³„ì‚°)

### 5.2 Token Merging via k-NN

**í•µì‹¬ í˜ì‹ **:

* **Information preservation**: Pruned tokens ì •ë³´ ë³´ì¡´
* **Similarity-based clustering**: Semantic ìœ ì‚¬ë„ ê¸°ë°˜
* **Weighted aggregation**: Attentionìœ¼ë¡œ ê°€ì¤‘

**ì¥ì **:

* Losslessì— ê°€ê¹Œìš´ ì••ì¶•
* Semantic consistency ìœ ì§€
* Large objects ì •ë³´ ë³´ì¡´

### 5.3 PruMerge+ Hybrid Strategy

**í•µì‹¬ í˜ì‹ **:

* **Attention + Spatial**: ë‘ ê°€ì§€ ì›ì¹™ ê²°í•©
* **Balanced coverage**: ì „ì²´ ì´ë¯¸ì§€ ì»¤ë²„ë¦¬ì§€
* **Performance-efficiency trade-off**: ì„ íƒ ê°€ëŠ¥

**ì¥ì **:

* Minimal performance drop
* Spatial bias ë°©ì§€
* Flexible deployment

### 5.4 Plug-and-Play Design

**í•µì‹¬ í˜ì‹ **:

* **Vision encoder level**: ì•„í‚¤í…ì²˜ ë…ë¦½ì 
* **Training-free option**: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
* **Modular implementation**: ì‰¬ìš´ í†µí•©

**ì¥ì **:

* LLaVA-1.5, Video-LLaVA ë“± ì¦‰ì‹œ ì ìš©
* Minimal code changes
* Research-friendly

---

6. Limitations ë° í–¥í›„ ë°©í–¥
----------------------

### í˜„ì¬ í•œê³„ (ë…¼ë¬¸ ê¸°ì¤€)

**1. Not Entirely Lossless**

* Visual token compressionì´ ì™„ì „íˆ losslessí•˜ì§€ ì•ŠìŒ
* ì›ë³¸ LLaVA ëŒ€ë¹„ marginal performance gap ì¡´ì¬
* PruMerge+ (25%)ë¡œ ëŒ€ë¶€ë¶„ í•´ê²°ë˜ë‚˜ ì™„ì „í•˜ì§€ ì•ŠìŒ

**2. Large-Scale Model ê²€ì¦ ë¶€ì¡±**

* Academic settingì˜ computational resources í•œê³„
* LLaVA-Next with Yi-34B ë“± ëŒ€ê·œëª¨ ëª¨ë¸ì— ëŒ€í•œ ê²€ì¦ ë¯¸ì™„ë£Œ

### í–¥í›„ ì—°êµ¬ ë°©í–¥ (ë…¼ë¬¸ ê¸°ì¤€)

**1. Fully Lossless Compression**

* ì™„ì „ ë¬´ì†ì‹¤ í† í° ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ê°œë°œ
* Performance gap ì™„ì „ ì œê±° ëª©í‘œ

**2. Larger-Scale Models í™•ì¥**

* LLaVA-Next with Yi-34B backbone ë“± ëŒ€ê·œëª¨ ëª¨ë¸ ì ìš©
* Generalization ë° broader impact ê²€ì¦

---

7. Conclusion
-------------

**LLaVA-PruMergeëŠ” Large Multimodal Modelsì˜ íš¨ìœ¨ì„±ì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ **:

**í•µì‹¬ ê¸°ì—¬**:  
1. **Adaptive token selection**: IQR-based outlier detection  
2. **Information-preserving merging**: k-NN clustering + weighted averaging  
3. **PruMerge+**: Attention + spatial hybrid strategy  
4. **14ë°° / 4ë°° ì••ì¶•**: ì„±ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ëŒ€í­ ì••ì¶•

**ì˜ì˜**:

* **Visual token ìˆ˜** ê´€ì ì˜ ìµœì´ˆ íš¨ìœ¨í™” ì—°êµ¬
* Plug-and-play ë°©ì‹ìœ¼ë¡œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
* Training-free optionìœ¼ë¡œ ë¹ ë¥¸ ë°°í¬
* Video-LLMì—ë„ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥

**ì‹¤ìš©ì„±**:

* 10ë°° FLOPs ê°ì†Œ
* 5.8~10.2ë°° ë¹ ë¥¸ prefill
* 50% ë©”ëª¨ë¦¬ ì ˆê°
* Quantizationê³¼ orthogonal (ê²°í•© ê°€ëŠ¥)

LLaVA-PruMergeëŠ” **íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•**ì„ ì´ë£¨ë©°, LMMì˜ ì‹¤ìš©ì  ë°°í¬ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.