---
title: "OpenAI, 새로운 에이전트 개발 도구 발표"
date: "2025-03-12"
tags:
  - "OpenAI"
  - "chatGPT"
year: "2025"
---

# OpenAI, 새로운 에이전트 개발 도구 발표

원본 게시글: https://velog.io/@euisuk-chung/OpenAI-새로운-에이전트-개발-도구-발표



![](https://velog.velcdn.com/images/euisuk-chung/post/8ed0339c-3099-43fa-9bd2-dc94aa0c1a8a/image.png)

OpenAI는 개발자와 기업이 보다 쉽게 **에이전트(Agent)를 구축할 수 있도록 돕기 위해 새로운 API와 도구**를 2025년 3월 11일(현지 일자)에 발표했습니다.

이번 업데이트의 핵심은 **Responses API, 내장 도구(`Web Search`, `File Search`, `Computer Use`), Agents SDK**이며, 이를 통해 더욱 강력하고 신뢰할 수 있는 AI 에이전트를 개발할 수 있을 것이라고 기대하고 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/914456c7-3fa9-4a0b-b9c3-97121448f913/image.png)

> 본 게시글은 **OpenAI 공식 블로그** 및 **공식 YouTube** 내용을 읽고, 듣고나서 정리한 내용입니다. (완벽하지 않을 수 있습니다)
> 
> * [New tools for building agents - YouTube](https://www.youtube.com/live/hciNKcLwSes?si=odJoX7xRiI9V0eUh)
> * [New tools for building agents - OpenAI Blog](https://openai.com/index/new-tools-for-building-agents/)

---

1. Responses API: Chat Completions + Tool Use 결합
================================================

**Responses API**는 기존의 **Chat Completions API**와 **Assistants API**의 기능을 결합한 새로운 API로, 하나의 API 호출만으로 여러 도구와 모델을 활용하여 복잡한 작업을 수행할 수 있도록 설계되었습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/f4d323c4-b578-45ec-ae97-201ffa349e3c/image.png)

**(Chat Completions API)**

> 이전에는 OpenAI의 `Chat Completions API`와 `Assistants API`가 각각 독립적으로 작동하면서, **개발자들이 복잡한 작업을 수행하려면 여러 API를 결합**해야 했습니다.
> 
> * 예를 들어, 대화를 유지하면서 **파일 검색**이나 **웹 검색** 같은 외부 도구를 사용하려면 별도의 API 호출이나 설정이 필요했죠.
>   + 이로 인해 **상태 관리**(대화의 흐름을 유지하는 것)와 **도구 결합**이 복잡하고 번거로웠습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/f6531fc8-6794-49cb-b2a7-bbd95d219773/image.png)

**(Responses API)**

> 하지만, `Responses API`가 도입되면서 이런 부분이 간소화되었습니다.
> 
> * Responses API는 단순한 텍스트 생성(Chat Completions API) 기능뿐만 아니라, **웹 검색, 파일 검색, 코드 실행, 시스템 상호작용** 같은 복잡한 작업을 수행할 수 있도록 설계되었습니다.
>   + 이제 **AI가 대화의 맥락을 자동으로 기억**하고, **여러 도구를 한 번의 API 호출로 사용**할 수 있게 되었어요.
>   + 즉, 개발자가 **직접 대화 상태를 관리하거나 도구를 호출할 필요가 없게되었다**고 합니다.

용어정리
----

본 게시글에는 OpenAI의 **Chat Completions API**, **Assistants API**, **Responses API** 3가지 API에 대해서 이야기하고 있습니다.

* 본격적인 이야기로 넘어가기 전에, 각각의 API에 대한 개념/역할/목표를 정리하고 가는 것이 필요하다고 판단하여 아래와 같이 정리해보았습니다.

| **API** | **개념 및 목표** | **현재 상태** | **향후 계획** |
| --- | --- | --- | --- |
| **Chat Completions API** | 단순한 대화형 텍스트 생성 및 응답 생성에 초점을 맞춘 기본 API | 지속적인 지원 중이며, Responses API의 기능도 포함 가능 | Responses API로 기능 통합 중, 도구 사용 옵션 제공 |
| **Assistants API** | 상태 유지형 AI 에이전트 구축을 위한 API로, 도구 사용 및 대화 맥락 관리 가능 | 2026년 상반기까지 지원 예정, 이후 Responses API로 대체됨 | Responses API로 기능 통합 및 폐지 예정 |
| **Responses API** | Chat Completions와 Assistants API의 기능을 통합한 차세대 AI 에이전트 구축 표준 API | 현재 제공 중, AI 에이전트 구축에 적합한 강력한 도구 제공 | 향후 OpenAI의 표준 API로 자리 잡을 예정, Assistants API 대체 및 지속적인 개선 |

현재 OpenAI의 `Chat Completions API`, `Assistants API`, `Responses API`는 각각의 역할과 목표가 다르며, OpenAI는 이들을 통합하거나 대체하는 방향으로 발전시키고 있습니다.

* (해당 게시글 작성일 기준) OpenAI는 `Responses API`를 중심으로 API 전략을 재정비하고 있으며, 기존의 `Assistants API`와 `Chat Completions API`의 기능을 통합하여 더 강력하고 유연한 AI 애플리케이션 개발 환경을 제공하고 있습니다.

---

### Chat Completions API 🗣

✔ **개념 및 목표**

* **Chat Completions API**는 OpenAI의 가장 기본적인 대화형 API로, GPT 모델을 호출하여 텍스트 기반의 응답을 생성하는 데 사용됩니다.
  + 이 API는 사용자가 입력한 프롬프트에 따라 모델이 **자연스러운 대화나 텍스트**를 생성합니다.

✔ **현재 상태**

* **지속적인 지원**:
  + Chat Completions API는 여전히 OpenAI에서 가장 널리 사용되는 API로, 특히 단순한 대화형 텍스트 생성에 적합합니다.
* **도구 사용 제한**:
  + 도구 사용은 기본적으로 지원되지 않으며, 단순한 텍스트 생성에 초점이 맞춰져 있습니다.

✔ **향후 발전 계획**

* OpenAI는 Chat Completions API에 새로운 모델과 기능을 지속적으로 추가할 예정이지만, 도구 사용이나 상태 관리 기능은 **포함하지 않습니다**.
* 도구와 상태 관리가 필요한 경우에는 Responses API를 사용하는 것이 권장됩니다.

---

### Assistants API 🤝

> 2026년 상반기 폐지 예정

✔ **개념 및 목표**

* **Assistants API**는 상태 유지형 AI 에이전트를 구축하는 데 최적화된 API로, 대화의 문맥을 지속적으로 유지하며 다양한 작업을 수행할 수 있도록 설계되었습니다.
  + 기존 Chat Completions API와 달리, **Threads(스레드)**와 **Messages(메시지)** 구조를 통해 지속적인 대화를 유지할 수 있습니다.
  + AI 에이전트는 단순한 대화형 챗봇을 넘어 복잡한 작업을 자동화하고 사용자 요청을 지능적으로 처리할 수 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/83a746aa-57f5-4357-8fe3-6acf14a446fa/image.png)

> <https://platform.openai.com/docs/assistants/overview>

✔ **주요 기능 및 활용 가능 도구**

* **Code Interpreter(코드 실행기)**:
  
  + Python 코드 실행이 가능하며, 데이터 분석, 그래프 생성, 수치 연산 등의 작업을 자동화할 수 있습니다.
* **Retrieval(파일 검색)**:
  
  + 업로드된 문서를 기반으로 정보를 검색하고 문서 내용을 활용하여 질문에 답변할 수 있습니다.
* **Function Calling(함수 호출)**:
  
  + `외부 API 호출` 또는 `사용자 정의 함수`를 실행할 수 있어, 외부 데이터베이스 연동이나 특정 로직을 처리하는 데 활용됩니다.

✔ **현재 상태**

* **폐지 예정**:
  + Assistants API는 2026년 상반기까지 Responses API로 대체될 예정이며, 현재는 기능 지원이 유지되고 있습니다.
* **개발자 전환 준비**:
  + 폐지 발표 전까지 새로운 모델과 기능이 추가되며, 개발자들은 Responses API로 전환을 준비해야 합니다.

✔ **향후 발전 계획**

* **Responses API로 통합**:
  + Assistants API의 모든 기능은 Responses API로 이전되며, 개발자들은 Responses API로 전환해야 합니다.
    - OpenAI는 이 과정에서 데이터 보존 및 마이그레이션 가이드를 추후 제공할 예정입니다.

---

### Responses API 💬

> 미래 지향적인 표준 API

✔ **개념 및 목표**

* **Responses API**는 `Chat Completions API`와 `Assistants API`의 **기능을 통합**한 차세대 API로, AI 에이전트를 구축하기 위한 강력한 도구를 제공합니다.
  + 대화 생성뿐만 아니라, 웹 검색, 파일 검색, 컴퓨터 사용 등 다양한 도구를 통합하여 AI가 실제 세상과 상호작용할 수 있도록 지원합니다.

✔ **현재 상태**

* **현재 사용 가능**:
  + Responses API는 이미 제공되고 있으며, AI 에이전트를 보다 효율적으로 구축할 수 있도록 지원하고 있습니다.

✔ **향후 발전 계획**

* **표준 API로 자리 잡음**:
  + OpenAI는 Responses API를 AI 에이전트 구축의 **표준 API**로 삼고 있으며, 향후에도 새로운 모델과 기능이 추가될 예정입니다.
* **Assistants API 대체**:
  + Assistants API는 Responses API로 완전히 대체될 예정이며, 2026년 상반기까지 개발자들이 전환할 수 있도록 지원할 것입니다.

---

아래는 이번 발표에서 언급된 `Responses API`의 주요 특징 및 변경 사항에 대해서 소개해드리겠습니다.

주요 특징
-----

`Responses API`는 아래와 같은 특징을 가지고 있습니다:

* **Chat Completions API**처럼 쉽게 사용할 수 있음
* `Web Search, File Search, Computer Use 등의 **내장 도구** 지원
* API의 출력 방식이 직관적이며 스트리밍 지원
* **트레이싱 및 평가 기능 제공**으로 에이전트 성능 분석 가능
* 다중 툴 호출을 지원하여 여러 작업을 한 번에 실행 가능
* 데이터 저장 및 추적 기능을 내장하여 성능 모니터링 용이

주요 변경 사항
--------

1. **도구 자동 선택**
   
   * **Chat Completions API**에서는 웹 검색을 활성화하면 **모델이 항상 검색을 수행**
   * **Responses API**에서는 모델이 **필요할 때만 자동으로 웹 검색 도구를 호출**
   * **불필요한 API 요청이 줄어들어 비용 절감 & 성능 향상**

2. **다중 도구 통합**
   
   * **Chat Completions API**에서는 웹 검색 외 다른 기능(파일 검색, 코드 실행)을 추가할 수 없음
   * **Responses API**에서는 **`웹 검색` + `파일 검색` + `코드 실행`을 하나의 요청에서 조합** 가능

```
response = client.responses.create(
    model="gpt-4o",
    tools=[
        {"type": "web_search_preview"},
        {"type": "file_search"},
        {"type": "code_interpreter"}  # (추후 지원 예정)
    ],
    input="내가 올린 파일에서 최근 데이터를 분석하고, 관련 뉴스를 찾아줘."
)
```
> ✅ **이런 방식이 가능**:  
> 
> → `file_search`로 업로드된 데이터 찾고  
> 
> → `web_search_preview`로 최신 뉴스 검색 후  
> 
> → 최종 결과를 `code_interpreter`로 분석

3. **상태 유지 기능 (Stateful API)**
   
   * **Chat Completions API**는 상태를 수동으로 관리해야 함
   * **Responses API**에서는 `previous_response_id`를 통해 **이전 대화의 상태를 자동으로 유지**가 가능함

```
response = client.responses.create(
    model="gpt-4o",
    previous_response_id="msg_1234567890",  # 이전 응답과 연결된 상태 유지
    input="이전 뉴스 기사에 대한 추가 정보를 찾아줘."
)
```
> ✅ 장점: **대화의 연속성을 유지할 수 있음**
> 
> * 이전에 **Chat Completions API**에서 여러 도구를 동시에 활용하는 데 제한이 있었기 때문에, 개발자들은 이러한 제한을 극복하기 위해 **LangChain**과 같은 프레임워크를 사용했습니다.

*(참고) **LangChain**은 언어 모델과 다양한 도구(예: 데이터베이스, 웹 검색, 파일 시스템 등)를 연결하여 복잡한 작업을 수행할 수 있도록 지원하는 프레임워크입니다.*

> 🐦 **LangChain**의 주요 기능은 다음과 같습니다:

* **도구 및 리소스 통합**: 언어 모델이 외부 API, 데이터베이스, 파일 시스템 등과 상호작용할 수 있도록 지원합니다.
* **체인 구성**: 여러 개의 언어 모델 호출이나 도구 사용을 체인 형태로 구성하여 복잡한 워크플로우를 구현할 수 있습니다.
* **메모리 관리**: 대화나 작업의 상태를 추적하여 컨텍스트를 유지합니다.

❗  *그러나 **Responses API**의 도입으로 이러한 기능이 API 자체에서 지원되기 시작했습니다.*

* 이제는 LangChain과 같은 **외부 프레임워크 없이도** Responses API를 통해 **여러 도구를 통합하고, 상태를 관리하며, 복잡한 작업을 수행**할 수 있습니다.

> 🖐️ **Responses API**의 주요 특징:

* **내장 도구 지원**: 웹 검색, 파일 검색, 코드 실행 등의 도구를 API 요청 시에 직접 지정할 수 있습니다.
* **상태 관리**: 이전 응답의 ID를 통해 대화나 작업의 상태를 유지할 수 있습니다.
* **구조화된 출력**: 모델의 응답을 구조화된 형식으로 받아서 처리할 수 있습니다.

*(결론) 이전에는 LangChain과 같은 **외부 프레임워크를 사용하여 언어 모델의 기능을 확장하고 다양한 도구와 통합**했지만, 이제는 **Responses API**를 통해 이러한 기능을 **직접 활용**할 수 있게 되었습니다.*

![](https://velog.velcdn.com/images/euisuk-chung/post/ca197f8a-8c95-47da-864f-0079dfd60461/image.png)

> OpenAI API만 사용하는 유저들은 랭체인 더이상 안 써도 될듯..!?

기존 API와 비교
----------

각각 `Responses API`와 `Chat Completions API`에서 웹 검색을 활용하는 코드를 예시로 살펴보겠습니다.

### **코드 비교**

✅ **기존 방식 (`Chat Completions API`)**

`GPT model for web search in Chat Completions`인 [gpt-4o-search-preview](https://platform.openai.com/docs/models/gpt-4o-search-preview)이 이번 업데이트에 릴리즈되면서 Chat Completions API에서도 빌트인으로 웹서치가 가능해졌습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/841b1cad-e12b-4e96-b12f-2c9ec6a3ac64/image.png)

> 출처: <https://platform.openai.com/docs/models/gpt-4o-search-preview>

![](https://velog.velcdn.com/images/euisuk-chung/post/77f5384e-709b-4c74-bc53-8715ac4321c6/image.png)

> 출처: <https://openai.com/index/new-tools-for-building-agents/>

코드도 비교적 간단해졌는데요! (model="gpt-4o-search-preview"만 호출하면 끝!)

```
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o-search-preview",  # 웹 검색 기능을 포함한 특정 모델 필요
    web_search_options={},  # 웹 검색 활성화 (설정 값이 없어도 자동 적용)
    messages=[
        {
        "role": "user", 
        "content": "What was a positive news story from today?"
        }
    ],
)

print(completion.choices[0].message.content)
```

원래 더 이전에는 아래와 같은 방식으로 코드를 작성해야 했습니다:

* 사용자의 질의를 받아 DuckDuckGo(외부 API)를 통해 검색을 수행하고, 검색 결과의 요약을 생성합니다.
* 이 요약을 OpenAI 모델의 프롬프트에 포함시켜, 모델이 최신 정보에 기반한 응답을 생성하도록 합니다.

```
import requests
import openai

# DuckDuckGo Instant Answer API 호출 함수
def duckduckgo_search(query):
    url = 'https://api.duckduckgo.com/'
    params = {
        'q': query,
        'format': 'json',
        'no_html': 1,
        'skip_disambig': 1
    }
    response = requests.get(url, params=params)
    return response.json()

# OpenAI Chat Completions API 호출 함수
def get_openai_response(prompt):
    openai.api_key = 'YOUR_OPENAI_API_KEY'
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo',
        messages=[{'role': 'user', 'content': prompt}]
    )
    return response.choices[0].message['content']

# 사용자 질의
user_query = "오늘의 주요 뉴스는 무엇인가요?"

# DuckDuckGo를 통한 웹 검색
search_results = duckduckgo_search(user_query)

# 검색 결과 요약 생성
if 'Abstract' in search_results and search_results['Abstract']:
    search_summary = search_results['Abstract']
else:
    search_summary = "죄송합니다만, 관련된 최신 정보를 찾을 수 없습니다."

# OpenAI 모델에 검색 결과 포함하여 응답 생성
prompt = f"사용자 질문: {user_query}\n\n검색 결과 요약: {search_summary}\n\n이 정보를 바탕으로 사용자에게 답변해 주세요."
response = get_openai_response(prompt)

print(response)
```

**함수 설명**

* `duckduckgo_search` 함수:
  + DuckDuckGo Instant Answer API를 호출하여 사용자의 질의에 대한 검색 결과를 가져옵니다.
* `get_openai_response` 함수:
  + OpenAI의 Chat Completions API를 호출하여 모델의 응답을 생성합니다.

ℹ️ **기존 방식 (`Chat Completions API`)의 웹 검색 특징**

* 기존(외부 API 사용) 방식은 `DuckDuckGo`, `Google Search` 등의 **외부 API를 직접 호출**해야 했음
  
  + 호출하여 검색을 수행한 내용을 바탕으로, 이를 요약하는데 LLM 모델을 사용함
* 이번 업데이트를 통해 웹 검색 활성화 가능해짐.
  
  + 단, **특정 모델(`gpt-4o-search-preview`, `gpt-4o-mini-search-preview`)만 지원**
  + 단, **항상 웹 검색 실행**됨 (검색이 필요하지 않아도 수행됨)
  + **검색 결과가 자동으로 응답(`message.content`)에 포함되며, 인라인 citation 제공**
  + **검색 컨텍스트 크기(`search_context_size`) 조절 가능 (low, medium, high)**

---

✅ **새로운 방식 (Responses API)**

```
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o",  # 웹 검색 전용 모델이 아니라 일반 모델도 사용 가능
    tools=[
    		{
            "type": "web_search_preview"
            }
          ],  # 웹 검색 도구 활성화
    input="오늘의 긍정적인 뉴스는?"
)

print(response.output_text)
```

ℹ️ **새로운 방식 (Responses API)의 웹 검색 특징**

* **모든 모델에서 웹 검색 사용 가능** (`gpt-4o` 포함)
* **`tools=[{"type": "web_search_preview"}]`**:
  + 웹 검색을 **도구(tool)로 등록**하여 필요할 때만 실행됨
* **모델이 필요하다고 판단**할 때만 **웹 검색을 수행**함 (불필요한 검색 방지)
* **웹 검색 결과**는 `output_text`에 포함되며, **citation 정보**도 제공함
* **추후 다양한 도구(File Search, Code Execution 등)와 함께** 사용할 수 있도록 **확장 가능**함

> 📚 `Responses API`와 `Chat Completions API`는 OpenAI 모델과 상호작용하는 두 가지 방식입니다.  
> 
> 아래는 각각의 API가 지원하는 기능입니다.
> 
> * 출처: <https://platform.openai.com/docs/guides/responses-vs-chat-completions>  
>   
>   ![](https://velog.velcdn.com/images/euisuk-chung/post/29a4864b-724b-42b3-b30b-e475810b7f1b/image.png)

---

2. 내장 도구(Built-in Tools)
========================

![](https://velog.velcdn.com/images/euisuk-chung/post/fac2f476-b5ca-4849-8125-b46227ed1c8b/image.png)

> 그림. `Responses API`와 `Chat Completions API` 내장 도구(Built-in Tools)

**Function calling**은 단순히 내장 도구로 분류되기보다는, 사용자가 정의한 **외부 도구(custom tool)**를 호출하는 기능으로 이해할 수 있습니다.

* 이는 모델이 기본적으로 제공하는 내장 도구(예: `웹 검색`, `파일 검색`, `컴퓨터 사용` 등)와는 다르게, 사용자가 직접 작성한 코드를 실행하고 그 결과를 모델 응답에 통합할 수 있도록 돕는 역할을 합니다.
* 따라서, 이는 내장 도구(Built-in Tools)가 아닙니다. ❌

> 💦 아직 잘 모르겠어요... 그래서 **내장 도구(Built-in Tools)**의 정의가 뭐라고요?

* **내장 도구(Built-in Tools)**는 OpenAI API에서 기본적으로 제공하는 기능으로, 예를 들어 `웹 검색`, `파일 검색`, `컴퓨터 사용` 등이 포함됩니다.
  
  + 이 도구들은 API 호출 시 자동으로 사용될 수 있도록 설계되어 있습니다.
* 반면, **Function calling**은 사용자가 `직접 정의한 외부 함수나 코드`를 실행할 수 있도록 하는 기능입니다.
  
  + 이 기능을 통해 모델은 기본 내장 도구만으로는 **제공하기 어려운 맞춤형 데이터나 기능을 활용**할 수 있습니다.

`Responses API`는 세 가지 강력한 내장 도구를 제공합니다.

(1) Web Search
--------------

* 실시간 정보를 검색하여 최신 데이터를 기반으로 응답을 생성
* 검색된 정보의 출처(뉴스, 블로그 등)를 명확히 제공
* 여러 소스에서 데이터를 통합하여 보다 신뢰성 있는 응답 제공

> For more: <https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses>

```
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o",
    tools=[{"type": "web_search_preview"}],
    input="What was a positive news story from today?"
)

print(response.output_text)
```

(2) File Search
---------------

* 여러 개의 파일을 업로드하고, 필요한 정보를 빠르게 검색
* 메타데이터 필터링 및 벡터 스토어를 통한 고급 검색 기능 제공
* 문서 내용 요약 및 텍스트 분석 기능 포함

> For more: <https://platform.openai.com/docs/guides/tools-file-search>

```
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    input="What is deep research by OpenAI?",
    tools=[{
        "type": "file_search",
        "vector_store_ids": ["<vector_store_id>"]
    }]
)
print(response)
```

(3) Computer Use
----------------

* 운영체제(OS) 및 웹 인터페이스에서 자동화 작업 수행
* OpenAI의 **Operator 에이전트**를 기반으로 동작
* GUI 기반 시스템에서도 실행 가능하여 API가 없는 시스템에서도 활용 가능

![](https://velog.velcdn.com/images/euisuk-chung/post/6427b5d5-e106-43c6-8e74-0824bdc1e3ea/image.png)

> For more: <https://platform.openai.com/docs/guides/tools-computer-use>

**그림 단계 간략 설명**

1. 모델에 요청 보내기 (*Send a request to the model*)
2. 모델의 응답 확인 (*Receive a response from the model*)
3. 요청된 액션 실행 (*Execute the requested action*)
4. 업데이트된 상태 캡처 (*Capture the updated state*)
5. 반복 (*Repeat*)

---

```
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="computer-use-preview",
    tools=[{
        "type": "computer_use_preview",
        "display_width": 1024,
        "display_height": 768,
        "environment": "browser" # other possible values: "mac", "windows", "ubuntu"
    }],
    input=[
        {
            "role": "user",
            "content": "Check the latest OpenAI news on bing.com."
        }
        # Optional: include a screenshot of the initial state of the environment
        # {
        #     type: "input_image",
        #     image_url: f"data:image/png;base64,{screenshot_base64}"
        # }
    ],
    reasoning={
        "generate_summary": "concise",
    },
    truncation="auto"
)

print(response.output)
```

![](https://velog.velcdn.com/images/euisuk-chung/post/31b5a738-2e8e-4dc1-bc03-9d503b89e175/image.png)

> 참고 : Computer Use 성능표

---

3. Agents SDK: 멀티-에이전트 오케스트레이션
==============================

기존 **Swarm SDK**를 발전시킨 **Agents SDK**가 공개되었습니다.

* 이 SDK를 활용하면 여러 에이전트를 쉽게 조율하고, 역할을 분리하여 복잡한 애플리케이션을 개발할 수 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/72fe0a67-de3f-4f5e-97b8-67d75bb166f2/image.png)

> 🤔 (참고) **Swarm이란 무엇인가?**

* `링크`: <https://github.com/openai/swarm>
* Swarm은 OpenAI가 개발한 다중 에이전트 시스템을 탐구하기 위한 교육용 프레임워크로, 여러 개의 에이전트를 조율하고 협업을 관리하는 개념을 실험할 수 있도록 설계되었습니다.
* Swarm을 활용하면 에이전트 간의 역할 분담과 정보 공유를 손쉽게 구성할 수 있으며, 이를 통해 복잡한 AI 시스템을 보다 직관적으로 구축할 수 있습니다.
* 다만, Swarm은 주로 학습 및 실험을 위한 도구로 개발되었으며, 본격적인 프로덕션 환경보다는 연구 및 개발 단계에서 활용하는 것이 권장됩니다.

주요 기능
-----

* **개별 에이전트 정의 및 도구(Tools) 연결**: 각 에이전트는 고유한 역할을 가지며, 특정 도구(예: Web Search, File Search, Custom Functions)와 연계할 수 있음.
* **Tracing & Observability**: 에이전트 실행 흐름을 시각적으로 추적하고 디버깅 가능. 실행 내역을 확인하여 문제 발생 시 빠르게 해결할 수 있음.
* **Handoff 기능**: 대화 중 특정 요청이 발생하면, 관련된 에이전트로 자연스럽게 역할을 전달할 수 있음.
* **Guardrails**: 입력 및 출력 검증을 통한 보안 강화. 유해한 콘텐츠나 부적절한 요청을 자동으로 차단 가능.
* **멀티 모델 지원**: GPT-4, GPT-3.5뿐만 아니라, 다양한 AI 모델을 혼합하여 에이전트 구축 가능.

코드 예시
-----

```
from agents import Agent, Runner, WebSearchTool, function_tool

@function_tool
def submit_refund_request(item_id: str, reason: str):
    return "success"

support_agent = Agent(
    name="Support & Returns",
    instructions="고객 지원 및 환불 요청을 처리하는 에이전트입니다.",
    tools=[submit_refund_request],
)

shopping_agent = Agent(
    name="Shopping Assistant",
    instructions="패션 스타일을 추천하는 에이전트입니다.",
    tools=[WebSearchTool()],
)

triage_agent = Agent(
    name="Triage Agent",
    instructions="사용자를 적절한 에이전트로 연결하는 역할을 합니다.",
    handoffs=[shopping_agent, support_agent],
)

output = Runner.run_sync(
    starting_agent=triage_agent,
    input="내 옷 스타일에 맞는 신발을 추천해줘",
)
```

---

4. Responses API 데모
===================

4.1 기본적인 API 호출 (발표 시연)
-----------------------

* 기존 **Chat Completions API**와 유사한 형태
* 요청(Request) → 응답(Response) 방식으로 동작하며, 다양한 도구와 결합 가능

4.2 스타일 추천 에이전트(Personal Stylist Assistant) 구현
----------------------------------------------

1. **초기 설정**
   
   * 에이전트 역할 정의: "You are a personal stylist"
   * 최신 트렌드 분석 가능: "What are some of the latest trends?"

![](https://velog.velcdn.com/images/euisuk-chung/post/58af99a2-ffd7-4386-b7a6-94bb06995761/image.png)

2. **File Search Tool을 활용한 개인 스타일 분석**
   
   * 특정 사용자의 스타일 정보를 **벡터 저장소(Vector Store)에서 검색**이 가능

![](https://velog.velcdn.com/images/euisuk-chung/post/86b38fc0-1198-4efb-a6f8-d7110054756e/image.png)

* `예제`: "Can you briefly summarize what Ilan likes to wear?"
  + **응답**: "Ilan has a distinct ecosystem style characterized by Miami chic."

![](https://velog.velcdn.com/images/euisuk-chung/post/cda100a8-1a02-43be-a971-dd4ba0510321/image.png)

3. **Web Search Tool을 활용한 최신 정보 반영**
   
   * 사용자의 여행지를 입력하여 해당 지역에서 구매 가능한 제품 검색
   * `예제`: "Find me a jacket that I would like nearby."
   * **처리 과정**:
     
     1. File Search Tool → 사용자 스타일 분석
     2. Web Search Tool → 해당 지역에서 관련 제품 판매 매장 검색
     3. **결과**: "Kevin이 좋아할 만한 Patagonia 매장을 Tokyo에서 검색"

![](https://velog.velcdn.com/images/euisuk-chung/post/ed8e3f89-9e6b-412a-ad19-17891f746634/image.png)

4.3 Computer Use Tool 데모 (쇼핑 자동화 시연)
------------------------------------

* 사용자가 원하는 재킷을 직접 찾아 구매할 수 있도록 지원
* **Computer Use Tool 사용 절차**:
  
  1. Patagonia 쇼핑 웹사이트의 스크린샷을 캡처하여 모델에 전달
  2. 모델이 화면을 분석하고 적절한 `클릭`, `입력` 등의 동작을 수행  
     
     ![](https://velog.velcdn.com/images/euisuk-chung/post/4a9d4aa2-8395-420a-8c58-63bb8e1dac3f/image.png)
  3. 연속적인 스크린샷 분석을 통해 전체 구매 프로세스 자동화

4.4 Swarm(Agents API) 활용한 에이전트 오케스트레이션
--------------------------------------

* **Swarm 기반 멀티 에이전트 실행**: Swarm은 OpenAI에서 제공하는 다중 에이전트 프레임워크로, 여러 개의 에이전트가 역할을 나누어 협력하도록 설계됨.

아래 기능들을 겸비한 `Stylist Agent`가 있다고 해보겠습니다.

* **Web Search Tool**을 활용하여 실시간 정보를 검색
* **File Search Tool**을 통해 사용자의 선호도를 반영

![](https://velog.velcdn.com/images/euisuk-chung/post/5b516df0-b190-429a-9c3f-d862f94e7d38/image.png)

Swarm(Agents API)는 위 Agent가 다른 Agent에게 역할을 전달하기 쉽게합니다.

* **에이전트 간 역할 분담 및 자동화**:
  1. **Triage Agent**: 사용자의 요청을 분석하여 적절한 에이전트로 라우팅
  2. **Shopping Agent**: 사용자의 스타일을 분석하고 적절한 제품 추천
  3. **Support Agent**: 구매 후 고객 지원 및 환불 처리

![](https://velog.velcdn.com/images/euisuk-chung/post/dd749c9b-0ee2-42b8-b529-ad303149671a/image.png)

* **추가 기능 설명**:
  1. `Handoff`: 특정 작업을 수행하기 위해 요청을 다른 에이전트로 전달하여 협력적 문제 해결 가능
  2. `Runner.run_sync`:
     + 초기 입력을 처리하고, 필요 시 다른 에이전트로 전달하여 최적의 결과 도출
     + 특정 도구(WebSearchTool, FileSearchTool)를 활용하여 맞춤형 응답 생성

---

5. 향후 계획
========

OpenAI는 앞으로 **더 많은 도구 및 모델을 추가**할 계획이며, Responses API를 중심으로 AI 에이전트 구축을 지원할 예정입니다.

ℹ️ **향후 업데이트 예정 내용 정리**

1. **Responses API에 Assistance API의 기능을 추가하여 완전한 대체 API로 발전**
   
   * 2026년까지 Assistance API 단계적 종료 예정
   * Responses API로 자연스럽게 마이그레이션할 수 있도록 지원
2. **멀티모달 모델과 통합**하여 더욱 강력한 에이전트 구현 지원
   
   * 텍스트뿐만 아니라 이미지, 음성, 동영상 등 다양한 입력을 처리할 수 있도록 확장
3. **다양한 API를 단일 플랫폼에서 통합적으로 운영**
   
   * Responses API, Agents SDK, Web Search Tool, File Search Tool 등을 하나의 플랫폼에서 유기적으로 연결
4. **더욱 향상된 보안 및 개인정보 보호 기능 추가**
   
   * API 요청 및 응답에 대한 데이터 보호 강화
   * 민감한 정보 처리 시 추가적인 보호 메커니즘 적용

OpenAI는 2025년을 "The Year of the Agent"로 만들기 위해 지속적으로 기술을 개선하고, 개발자들이 더 강력한 AI 에이전트를 쉽게 구축할 수 있도록 지원할 것이라고 선언했습니다 🙌

앞으로는 또 어떤 방향으로 발전해 나갈지 궁금하군요🤔

