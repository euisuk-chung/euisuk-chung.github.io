---
title: "[NLP] 4. Natural Language Embeddings"
date: "2024-07-29"
tags:
  - "NLP"
  - "ê°•ì˜ë…¸íŠ¸"
year: "2024"
---

# [NLP] 4. Natural Language Embeddings

ì›ë³¸ ê²Œì‹œê¸€: https://velog.io/@euisuk-chung/NLP-4.-Natural-Language-Embeddings



Natural Language Embeddings
===========================

ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ìì—°ì–´ ì„ë² ë”© ê¸°ë²•(Natural Language Embedding)ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì„ ìì„¸íˆ ì„¤ëª…í•˜ê³ , ê° ê¸°ë²•ì˜ ì˜ˆì‹œë¥¼ í†µí•´ ì´í•´ë¥¼ ë•ê³ ì í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/8876c26b-aaad-4d2e-ae6c-cf3b3630b9f4/image.png)

> ğŸ” **Text Representation Learning & Natural Language Embeddingì´ë€?**
> 
> * `í…ìŠ¤íŠ¸ í‘œí˜„ í•™ìŠµ(Text Representation Learning)`ì€ ë¹„ì •í˜• í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
> * `ìì—°ì–´ ì„ë² ë”©(Natural Language Embedding)`ì€ ì´ëŸ¬í•œ ê³¼ì •ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.
> * ì´ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ê³¼ ë¬¸ë§¥ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

1. ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ê¸°
------------------------

* ë¹„ì •í˜• ë°ì´í„°ëŠ” í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë§í•˜ë©°, ì´ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤.
* ì´ ê³¼ì •ì€ ë°ì´í„°ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
* ë¹„ì •í˜• ë°ì´í„°ë¥¼ `ë²¡í„°` ë˜ëŠ” `ë§¤íŠ¸ë¦­ìŠ¤` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì •í˜• ë°ì´í„°ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **ì£¼ìš” ê³¼ì •:**
  
  + `ë°ì´í„° ìˆ˜ì§‘`: ì„œì , ì¸í„°ë„· ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
  + `ë°ì´í„° ì „ì²˜ë¦¬`: ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ NLP ê¸°ë²•ì„ ì´ìš©í•´ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - **ëŒ€ì†Œë¬¸ì í†µì¼**: "They"ì™€ "they"ë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ í†µì¼.
    - **ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ê¸°í˜¸ ì œê±°**: ì˜ˆë¥¼ ë“¤ì–´, "!"ë‚˜ "?" ê°™ì€ ê¸°í˜¸ë¥¼ ì œê±°.
    - **ìˆ«ì ì œê±°**: í•„ìš” ì—†ëŠ” ìˆ«ìëŠ” ì œê±°.
    - **ë¶ˆìš©ì–´ ì œê±°**: ë¬¸ë²• ìš”ì†Œ ë“± ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆì§€ ì•Šì€ ë‹¨ì–´ ì œê±°.
  + `ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜`: ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë²¡í„° ë˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

2. í…ìŠ¤íŠ¸ ë²¡í„°í™” ê¸°ë²•
-------------

í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### 2.1 ë°± ì˜¤ë¸Œ ì›Œì¦ˆ (Bag-of-Words) ëª¨ë¸

`Bag of Words`ëŠ” ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ, ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  `ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ë¹ˆë„`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/35b4ead2-0e0f-4470-b668-84748b0988fc/image.png)

ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì˜ BoWë“¤ì„ ê²°í•©í•˜ë©´ `ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬(Document-Term Matrix, DTM)` ë˜ëŠ” `ë‹¨ì–´ ë¬¸ì„œ í–‰ë ¬(Term-Document Matrix, TDM)`ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ê¹Œì§€ í™•ì¥í•´ì„œ ë¹„êµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

* **ê°œë…**: ë‹¨ì–´ì˜ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³ , ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì„¸ì–´ ë²¡í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
* **BoW ìƒì„± ë°©ì‹**:

```
    (1) ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤ ë¶€ì—¬.
    (2) ê° ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— ë‹¨ì–´ í† í°ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ê¸°ë¡í•œ ë²¡í„° ìƒì„±. 
```

* **íŠ¹ì§•**: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë¬´ì‹œí•˜ë©° ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
    ë¬¸ì„œ1: "I love machine learning."
    ë¬¸ì„œ2: "Machine learning is fun."
    
    BoW ë²¡í„°:
    ë¬¸ì„œ1: [1, 1, 1, 1, 0, 0]  # "I", "love", "machine", "learning", "is", "fun"ì˜ ìˆœì„œ
    ë¬¸ì„œ2: [0, 0, 1, 1, 1, 1]
```

* **ì˜ˆì‹œ ì½”ë“œ**:

```
from konlpy.tag import Okt

okt = Okt()

def build_bag_of_words(document):
  # ì˜¨ì  ì œê±° ë° í˜•íƒœì†Œ ë¶„ì„
  document = document.replace('.', '')
  tokenized_document = okt.morphs(document)

  word_to_index = {}
  bow = []

  for word in tokenized_document:  
    if word not in word_to_index.keys():
      word_to_index[word] = len(word_to_index)  
      # BoWì— ì „ë¶€ ê¸°ë³¸ê°’ 1ì„ ë„£ëŠ”ë‹¤.
      bow.insert(len(word_to_index) - 1, 1)
    else:
      # ì¬ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤
      index = word_to_index.get(word)
      # ì¬ë“±ì¥í•œ ë‹¨ì–´ëŠ” í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— 1ì„ ë”í•œë‹¤.
      bow[index] = bow[index] + 1

  return word_to_index, bow
```
### 2.2 ë‹¨ì–´ ê°€ì¤‘ì¹˜ (Word Weighting) - TF-IDF

íŠ¹ì • ë‹¨ì–´ê°€ ë¬¸ì„œì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ `ë‹¨ì–´ ë¹ˆë„(TF, Term-Frequency)`ì™€ `ë¬¸ì„œ ë¹ˆë„(DF, Document-Frequency)`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/be35c8f3-9abd-49e9-a70d-ff30182d3b46/image.png)

* **ê°œë…**: ìì£¼ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ê°€ íŠ¹ì • ë¬¸ì„œì—ì„œ ë§ì´ ë“±ì¥í•  ê²½ìš° í•´ë‹¹ ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì…ë‹ˆë‹¤.
* **TF-IDF ìƒì„± ë°©ì‹**:
  
  + `TF (Term Frequency)`: tf(d,t), íŠ¹ì • ë¬¸ì„œ dì—ì„œì˜ íŠ¹ì • ë‹¨ì–´ tì˜ ë“±ì¥ íšŸìˆ˜.
  + `DF (Document Frequency)`: df(t), íŠ¹ì • ë‹¨ì–´ tê°€ ë“±ì¥í•œ ë¬¸ì„œì˜ ìˆ˜.
  + `IDF (Inverse Document Frequency)`: idf(t), ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¹ˆë„ì˜ ì—­ìˆ˜  
    
    - ìˆ˜ì‹: log(n/(1+df(t)))log({n}/{(1+df(t))})log(n/(1+df(t)))
  + `TF-IDF`: TFì™€ IDFë¥¼ ê³±í•˜ì—¬ ê³„ì‚°.
* **íŠ¹ì§•**: ë‹¨ìˆœ ë¹ˆë„ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ì—¬ ë‹¨ì–´ì˜ ì¤‘ìš”ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤. (ê°€ì¤‘ì¹˜ ê°œë… ì¶”ê°€)
* **ì˜ˆì‹œ**:
  
  + `ë¬¸ì„œ1`: *"I love machine learning."*
  + `ë¬¸ì„œ2`: *"Machine learning is fun."*
  + ë‹¨ì–´ 'machine'ì— ëŒ€í•´:
    - n=2n = 2n=2 (ì „ì²´ ë¬¸ì„œ ìˆ˜)
    - df(t)=2\text{df}(t) = 2df(t)=2 ('machine'ì´ ë“±ì¥í•˜ëŠ” ë¬¸ì„œ ìˆ˜)
  + ë”°ë¼ì„œ IDFëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:
    - IDF(machine)=logâ¡(21+2)=logâ¡(23)=logâ¡(0.6667)â‰ˆâˆ’0.176\text{IDF}(machine) = \log\left(\frac{2}{1 + 2}\right) = \log\left(\frac{2}{3}\right) = \log(0.6667) \approx -0.176IDF(machine)=log(1+22â€‹)=log(32â€‹)=log(0.6667)â‰ˆâˆ’0.176
  + TF ê³„ì‚°
    - ë¬¸ì„œ1ì—ì„œ 'machine'ì˜ TF: 1
    - ë¬¸ì„œ2ì—ì„œ 'machine'ì˜ TF: 1
  + TF-IDF ê³„ì‚°
    - ë¬¸ì„œ1: TF-IDF(machine)=1Ã—âˆ’0.176=âˆ’0.176\text{TF-IDF}(machine) = 1 \times -0.176 = -0.176TF-IDF(machine)=1Ã—âˆ’0.176=âˆ’0.176
    - ë¬¸ì„œ2: TF-IDF(machine)=1Ã—âˆ’0.176=âˆ’0.176\text{TF-IDF}(machine) = 1 \times -0.176 = -0.176TF-IDF(machine)=1Ã—âˆ’0.176=âˆ’0.176
* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    documents = ["I love machine learning.", "Machine learning is fun."]
    
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
    
    print(tfidf_matrix.toarray())
    print(tfidf_vectorizer.get_feature_names_out())
    
```
### 2.3 N-Grams ëª¨ë¸

ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ N-Grams ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ì–´ì˜ ì—°ì†ëœ Nê°œì˜ ë‹¨ì–´ ì¡°í•©ì„ íŠ¹ì§•ìœ¼ë¡œ í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/15278f24-700f-4038-a428-79376db3b4f2/image.png)

* **ê°œë…**: Nê°œì˜ ì—°ì†ëœ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ë²¡í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
  + `ìœ ë‹ˆê·¸ë¨(Unigram)`: ë‹¨ì¼ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ N-ê·¸ë¨. (N=1)
  + `ë°”ì´ê·¸ë¨(Bigram)`: ë‘ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ N-ê·¸ë¨. (N=2)
  + `íŠ¸ë¼ì´ê·¸ë¨(Trigram)`: ì„¸ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ N-ê·¸ë¨. (N=3)
  + `4-ê·¸ë¨(4-gram)`: ë„¤ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ N-ê·¸ë¨. (N=4)
* **N-Gram ìƒì„± ë°©ì‹**:

```
    (1) í…ìŠ¤íŠ¸ë¥¼ Nê°œì˜ ë‹¨ì–´ ë¬¶ìŒìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    (2) ê° N-Gram ë¬¶ìŒì„ ë²¡í„°í™”í•©ë‹ˆë‹¤.
    
```

* **íŠ¹ì§•**: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ë°˜ì˜í•˜ì—¬ ë¬¸ë§¥ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
    ë¬¸ì¥: "I love machine learning"
    
    Unigram: ["I", "love", "machine", "learning"]
    Bigram: ["I love", "love machine", "machine learning"]
    Trigram: ["I love machine", "love machine learning"]
    4-gram: ["I love machine learning"]
```

* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from sklearn.feature_extraction.text import CountVectorizer
    
    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))
    trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))
    
    documents = ["I love machine learning."]
    
    bigram_matrix = bigram_vectorizer.fit_transform(documents)
    trigram_matrix = trigram_vectorizer.fit_transform(documents)
    
    print("Bigram:\\n", bigram_matrix.toarray())
    print("Bigram Features:\\n", bigram_vectorizer.get_feature_names_out())
    
    print("Trigram:\\n", trigram_matrix.toarray())
    print("Trigram Features:\\n", trigram_vectorizer.get_feature_names_out())
    
```

3. ë¶„ì‚° í‘œí˜„ (Distributed Representation)
-------------------------------------

ë¶„ì‚° í‘œí˜„ì´ë€, ë‹¨ì–´ë¥¼ ë¶„ì‚° ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ ìƒì—ì„œ ìœ ì‚¬í•˜ê²Œ ë°°ì¹˜í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/25e8fd3a-1304-496b-b1f2-9a7376307750/image.png)

### 3.1 NNLM (NeuralNet Language Model)

* **ê°œë…**: NNLMì€ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ì•„ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì…ë ¥ëœ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. (RNN/LSTM ê³„ì—´ ëª¨ë¸)

![](https://velog.velcdn.com/images/euisuk-chung/post/fc19df81-db85-4bfe-b37f-696a8946a182/image.png)

* **NNLM ìƒì„± ë°©ì‹**:

```
    (1) ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ì•„ ë‹¨ì–´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (2) ì€ë‹‰ì¸µì„ ê±°ì³ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    (3) ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ë‹¨ì–´ ì„ë² ë”© ë²¡í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
    
```

* **íŠ¹ì§•**: NNLMì€ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ë§¥ì„ ë°˜ì˜í•˜ë©°, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
    ë¬¸ì¥: "I love machine learning"
    >>> ì…ë ¥: ["I", "love", "machine"]
    >>> ì¶œë ¥: "learning"
```

* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, LSTM, Dense
    
    # ìƒ˜í”Œ ë°ì´í„°
    sentences = ["I love machine learning", "Machine learning is fun", "I love deep learning"]
    
    # í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sentences)
    total_words = len(tokenizer.word_index) + 1
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    input_sequences = []
    for line in sentences:
        token_list = tokenizer.texts_to_sequences([line])[0]
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)
    
    # íŒ¨ë”©
    max_sequence_len = max([len(x) for x in input_sequences])
    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')
    
    # íŠ¹ì§•ê³¼ ë ˆì´ë¸” ë¶„ë¦¬
    X, y = input_sequences[:,:-1], input_sequences[:,-1]
    
    # ë ˆì´ë¸” ì›-í•« ì¸ì½”ë”©
    y = tf.keras.utils.to_categorical(y, num_classes=total_words)
    
    # ëª¨ë¸ ìƒì„±
    model = Sequential()
    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
    model.add(LSTM(100))
    model.add(Dense(total_words, activation='softmax'))
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X, y, epochs=100, verbose=1)
    
    # ì˜ˆì¸¡
    seed_text = "I love"
    next_words = 3
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        output_word = tokenizer.index_word[np.argmax(predicted)]
        seed_text += " " + output_word
    print(seed_text)
    
```
### 3.2 Word2Vec

* **ê°œë…**: ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ëŠ” ì„ë² ë”© ê¸°ë²•ì…ë‹ˆë‹¤.  
  
  - ì£¼ìš” ëª¨ë¸ë¡œëŠ” `CBOW(Continuous Bag of Words)`ì™€ `Skip-gram`ì´ ìˆìŠµë‹ˆë‹¤.
  
  + Word2Vecì„ í•™ìŠµí•  ë•ŒëŠ” CBOWì™€ Skip-gram ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‘ ë°©ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ê°ê°ì˜ ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/0785db8a-da96-45cf-a285-d86c8060ee30/image.png)
  + CBOW (Continuous Bag of Words):
    
    - ì£¼ë³€ ë‹¨ì–´ë“¤ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ë” ì˜ ì‘ë™í•©ë‹ˆë‹¤.
    - ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ëŒ€í•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
  + Skip-gram:
    
    - ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
    - ë“œë¬¸ ë‹¨ì–´ì— ëŒ€í•´ ë” ë‚˜ì€ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

* (Method 1) **CBOW**: ì£¼ë³€ ë‹¨ì–´ë“¤ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡.  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/9047e348-257a-4ce6-86dc-8829ac00a6d1/image.png)
* (Method 2) **Skip-gram**: ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡.  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/2a255804-990d-44d5-b3c7-a728d5d7a22a/image.png)

* **íŠ¹ì§•**: ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì´ ê°€ê¹Œìš´ ë²¡í„°ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì‹ ê²½ë§ì„ í†µí•´ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
    ë¬¸ì¥: "The cat sat on the mat"
    ì¤‘ì‹¬ ë‹¨ì–´: "sat"
    ì£¼ë³€ ë‹¨ì–´: ["The", "cat", ___, "on", "the", "mat"]
    
    ## CBOW: ì£¼ë³€ ë‹¨ì–´ë“¤ â†’ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡
    >> ì¸í’‹: ["The", "cat", "on", "the", "mat"]
    >> ì•„ì›ƒí’‹: "sat"
    
    ## Skip-gram: ì¤‘ì‹¬ ë‹¨ì–´ â†’ ì£¼ë³€ ë‹¨ì–´ë“¤ ì˜ˆì¸¡
    >> ì¸í’‹: "sat"
    >> ì•„ì›ƒí’‹: ["The", "cat", "on", "the", "mat"]
```

* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from gensim.models import Word2Vec
    
    sentences = [["the", "cat", "sat", "on", "the", "mat"],
                 ["the", "dog", "sat", "on", "the", "couch"]]
    
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW
    
    vector = model.wv['cat']
    print(vector)
    
```

![](https://velog.velcdn.com/images/euisuk-chung/post/c66d7392-69b1-43d7-90f6-67d237a5ff53/image.png)

### 3.3 GloVe (Global Vectors for Word Representation)

* **ê°œë…**: ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ìœ¼ë¡œ, ë‹¨ì–´ì˜ ë™ì‹œ ë“±ì¥ í–‰ë ¬ì„ í™œìš©í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
  + `LSA ê¸°ë²•`ì€ DTM(TDM)ì´ë‚˜ TF-IDF í–‰ë ¬ê³¼ ê°™ì´ ê° ë¬¸ì„œì—ì„œì˜ ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì¹´ìš´íŠ¸ í•œ í–‰ë ¬ì´ë¼ëŠ” ì „ì²´ì ì¸ í†µê³„ ì •ë³´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì°¨ì›ì„ ì¶•ì†Œ(Truncated SVD)í•˜ì—¬ ì ì¬ëœ ì˜ë¯¸ë¥¼ ëŒì–´ë‚´ê³ ì í•©ë‹ˆë‹¤.
    - (ë‹¨ì ) LSAëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì½”í¼ìŠ¤ì˜ ì „ì²´ì ì¸ í†µê³„ ì •ë³´ë¥¼ ê³ ë ¤í•˜ê¸°ëŠ” í•˜ì§€ë§Œ, King : Man = Queen : ? (Woman) ë˜ëŠ” Korea : Seoul = Japan : ? (Tokyo)ì™€ ê°™ì€ ë‹¨ì–´ ì˜ë¯¸ì˜ ìœ ì¶” ì‘ì—…(Analogy task)ì—ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.
  + `Word2Vec ê¸°ë²•`ì€ CBOWì™€ Skip-gramì´ë¼ëŠ” ë”¥ëŸ¬ë‹ í•™ìŠµë°©ë²•ì„ í™œìš©í•˜ì—¬ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ì˜¤ì°¨ë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•´ ì¤„ì—¬ë‚˜ê°€ë©° í•™ìŠµí•˜ë©° ì ì¬ëœ ì˜ë¯¸ë¥¼ ëŒì–´ë‚´ê³ ì í•©ë‹ˆë‹¤.
    - (ë‹¨ì ) Word2VecëŠ” ì„ë² ë”© ë²¡í„°ê°€ ë‹¨ì–´ ê°„ì˜ ìƒê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ìœ ì¶” ì‘ì—…ì€ LSAë³´ë‹¤ ë›°ì–´ë‚˜ì§€ë§Œ, ìœˆë„ìš° í¬ê¸° ë‚´ì—ì„œë§Œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì— ì½”í¼ìŠ¤ì˜ ì „ì²´ì ì¸ í†µê³„ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
  + GloveëŠ” ì´ëŸ¬í•œ ê°ê°ì˜ ëª¨ë¸ì˜ ì¥ì ì„ í•©ì³ì„œ ê°ê°ì˜ ë‹¨ì ì„ í•´ê²°í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/4a82f544-e88d-45da-9044-bb27c89c214b/image.png)

* **GloVe ìƒì„± ë°©ì‹**:
  1. `ë™ì‹œ ë“±ì¥ í–‰ë ¬ (Co-occurrence Matrix)`ì„ ìƒì„±í•©ë‹ˆë‹¤.
     + **ì •ì˜**: ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ê° ë‹¨ì–´ ìŒì´ íŠ¹ì • ë¬¸ë§¥ ìœˆë„ìš° ë‚´ì—ì„œ í•¨ê»˜ ë“±ì¥í•˜ëŠ” ë¹ˆë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì…ë‹ˆë‹¤.
     + **êµ¬ì¡°**: í–‰ê³¼ ì—´ì´ ëª¨ë‘ ì–´íœ˜ì˜ ë‹¨ì–´ë“¤ë¡œ ì´ë£¨ì–´ì§„ ì •ì‚¬ê° í–‰ë ¬ì…ë‹ˆë‹¤.
     + **ê°’**: í–‰ë ¬ì˜ ê° ì…€ XijX\_{ij}Xijâ€‹ëŠ” ë‹¨ì–´ iì˜ ë¬¸ë§¥ì—ì„œ ë‹¨ì–´ jê°€ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  2. `ë™ì‹œ ë“±ì¥ ë¹ˆë„ (Co-occurrence Frequency)`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
     + **ì •ì˜**: íŠ¹ì • ë‹¨ì–´ ìŒì´ ì •ì˜ëœ ë¬¸ë§¥ ìœˆë„ìš° ë‚´ì—ì„œ í•¨ê»˜ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ì…ë‹ˆë‹¤.
     + **êµ¬ì¡°**: ë³´í†µ ì¤‘ì‹¬ ë‹¨ì–´ë¡œë¶€í„° ì¼ì • ê±°ë¦¬(ìœˆë„ìš° í¬ê¸°) ë‚´ì˜ ë‹¨ì–´ë“¤ì„ ë¬¸ë§¥ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
     + **ê°’**: ì´ ë¹ˆë„ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ í†µê³„ì  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/ce0e7819-2d2c-4f13-802c-25eff2f905c2/image.png)

* **íŠ¹ì§•**: ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ë‹¨ì–´ê°€ í•¨ê»˜ ë“±ì¥í•˜ëŠ” ë¹ˆë„ë¥¼ í™œìš©í•˜ì—¬ ì „ì—­ì ì¸ í†µê³„ ì •ë³´ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
ë¬¸ì¥ë“¤:
1. "The cat sits on the mat"
2. "The dog chases the cat"
3. "A cat and a dog play"

>> ë¬¸ë§¥ ìœˆë„ìš° í¬ê¸°: 2 (ì¤‘ì‹¬ ë‹¨ì–´ì˜ ì–‘ìª½ìœ¼ë¡œ 2ê°œì˜ ë‹¨ì–´ê¹Œì§€ ê³ ë ¤)

>> ë™ì‹œ ë“±ì¥ í–‰ë ¬:
       the  cat  sits  on   mat  dog  chases  a    and  play
the    0    3    1     1    1    1    1       0    0    0
cat    3    0    1     1    1    2    1       2    1    1
sits   1    1    0     1    1    0    0       0    0    0
on     1    1    1     0    1    0    0       0    0    0
mat    1    1    1     1    0    0    0       0    0    0
dog    1    2    0     0    0    0    1       2    1    1
chases 1    1    0     0    0    1    0       0    0    0
a      0    2    0     0    0    2    0       0    1    1
and    0    1    0     0    0    1    0       1    0    1
play   0    1    0     0    0    1    0       1    1    0
```

* **ì˜ˆì‹œ ì½”ë“œ**:
  + GloVe ì‚¬ìš©ì‹œ, Stanfordì˜ pre-trained ë²¡í„°ë¥¼ ë‹¤ìš´ë¡œë“œ í›„ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  + ì´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ ì§ì ‘ ë°›ìœ¼ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤: <https://nlp.stanford.edu/projects/glove/>  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/71a29108-fe12-4913-8b99-4452242a1bf7/image.png)

* ì•„ë˜ëŠ” ì‚¬ì „ í•™ìŠµëœ Glove ì„ë°°ë”©ì„ í˜¸ì¶œí•˜ê³ , `'cat'`ì´ë¼ëŠ” ë‹¨ì–´ì˜ ì„ë°°ë”© ê°’ì„ í™•ì¸í•´ë³´ëŠ” ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤.

```
from urllib.request import urlretrieve
import zipfile

# glove ì‚¬ì „í•™ìŠµ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
urlretrieve("http://nlp.stanford.edu/data/glove.6B.zip", filename="glove.6B.zip")

# glove ì‚¬ì „í•™ìŠµ íŒŒì¼ ì••ì¶• í•´ì œ
with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:
    zip_ref.extractall()

from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors

# GloVe íŒŒì¼ ê²½ë¡œ
glove_input_file = './glove.6B.100d.txt'

# ë³€í™˜ëœ Word2Vec íŒŒì¼ ê²½ë¡œ
word2vec_output_file = 'glove.6B.100d.word2vec.txt'

# GloVe í˜•ì‹ì„ Word2Vec í˜•ì‹ìœ¼ë¡œ ë³€í™˜
glove2word2vec(glove_input_file, word2vec_output_file)

# ë³€í™˜ëœ Word2Vec í˜•ì‹ì˜ íŒŒì¼ì„ ë¡œë“œ
model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)

# íŠ¹ì • ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ì¶œë ¥
vector = model['cat']
print(vector)
    
```
### 3.3 FastText

* **ê°œë…**: FastTextëŠ” Facebook(í˜„ Meta)ì—ì„œ ê°œë°œí•œ ì„ë°°ë”© ê¸°ë²•ìœ¼ë¡œ, ë‹¨ì–´ ë‚´ë¶€ì˜ ë¬¸ì n-gramì„ í™œìš©í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
  + Word2Vecì™€ FastTextì™€ì˜ ê°€ì¥ í° ì°¨ì´ì ì´ë¼ë©´ Word2VecëŠ” ë‹¨ì–´ë¥¼ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ìƒê°í•œë‹¤ë©´, FastTextëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ ì•ˆì—ë„ ë‹¨ìœ„ë“¤ì´ ìˆë‹¤ê³  ê°„ì£¼í•©ë‹ˆë‹¤. ì¦‰, ë‹¨ì–´ì˜ í˜•íƒœì†Œ ì •ë³´ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/2c0c75ff-a5f0-43ef-bdb9-a63b10084890/image.png)

* **FastText ìƒì„± ë°©ì‹**:

```
    ## Subword Model
    (1) ë‹¨ì–´ë¥¼ ë¬¸ì n-gramìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
    (2) ê° n-gram ë²¡í„°ë¥¼ í•©ì‚°í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
```

* **íŠ¹ì§•**: ë¬¸ì n-gramì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ë©°, ëª¨ë¥´ëŠ” ë‹¨ì–´(Out Of Vocabulary, OOV)ë„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**:

```
    ë‹¨ì–´: "apple"
    ë¬¸ì n-gram: ["a", "ap", "app", "p", "pp", "ple", "e"]
    ë²¡í„° í•™ìŠµ: ê° ë¬¸ì n-gramì˜ ë²¡í„°ë¥¼ í•©ì‚°í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
```

* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from gensim.models import FastText
    
    sentences = [["the", "cat", "sat", "on", "the", "mat"],
                 ["machine", "learning", "is", "fun"]]
    
    model = FastText(sentences, vector_size=100, window=5, min_count=1)
    vector = model.wv['cat']
    print(vector)
    
```

4. ë¬¸ì„œ ì„ë² ë”© (Document Embedding)
------------------------------

ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë²¡í„°í™”í•©ë‹ˆë‹¤.

### 4.1 ë‹¨ì–´ ì„ë² ë”©ì˜ í‰ê·  (Averaging Word Embeddings)

* **ê°œë…**: ê°€ì¥ ì›ì´ˆì ì¸ ë°©ë²•ìœ¼ë¡œ, ë¬¸ì„œ ë‚´ì˜ ëª¨ë“  ë‹¨ì–´ ì„ë² ë”© ë²¡í„°ì˜ í‰ê· ì„ êµ¬í•˜ì—¬ ë¬¸ì„œ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
* **ìƒì„± ë°©ì‹**:

```
    (1) ë¬¸ì„œ ë‚´ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (2) ë³€í™˜ëœ ëª¨ë“  ë‹¨ì–´ ì„ë² ë”© ë²¡í„°ì˜ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    
```

* **íŠ¹ì§•**: ê°„ë‹¨í•˜ê³  ê³„ì‚°ì´ ë¹ ë¥´ì§€ë§Œ, í‰ê· ì„ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì„œ ë‚´ ë‹¨ì–´ ìˆœì„œë‚˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ ì½”ë“œ**:

```
    import numpy as np
    from gensim.models import Word2Vec
    
    sentences = [["I", "love", "machine", "learning"], ["Machine", "learning", "is", "fun"]]
    word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
    
    def average_word_embeddings(document):
    
        vectors = [word2vec_model.wv[word] for word in document if word in word2vec_model.wv]
        
        return np.mean(vectors, axis=0)
    
    document = ["I", "love", "machine", "learning"]
    document_vector = average_word_embeddings(document)
    
    print(document_vector)
    
```
### 4.2 PV-DM (Distributed Memory Model of Paragraph Vectors)

* **ê°œë…**: ë¬¸ì„œì™€ ë‹¨ì–´ë¥¼ ë™ì‹œì— ì„ë² ë”©í•˜ì—¬ ë¬¸ì„œ ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë¬¸ë§¥ ë‹¨ì–´ì™€ ë¬¸ì„œ ë²¡í„°ë¥¼ ì¡°í•©í•˜ì—¬ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/e7b65c36-dbb0-437c-945a-dd0948d0d14e/image.png)

* **ìƒì„± ë°©ì‹**:

```
    (1) ê° ë¬¸ì„œì— ê³ ìœ í•œ IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    (2) ë¬¸ë§¥ ë‹¨ì–´ì™€ ë¬¸ì„œ IDë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
```

* **íŠ¹ì§•**: ë¬¸ì„œì˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë¬¸ì„œ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    
    documents = [TaggedDocument(words=["I", "love", "machine", "learning"], tags=['doc1']),
                 TaggedDocument(words=["Machine", "learning", "is", "fun"], tags=['doc2'])]
    
    model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4, dm=1)  # PV-DM
    vector = model.dv['doc1']
    
    print(vector)
    
```
### 4.3 PV-DBOW (Distributed Bag of Words Model of Paragraph Vectors)

* **ê°œë…**: ë‹¨ì–´ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ë¬¸ì„œ ë²¡í„°ë§Œìœ¼ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‹¨ì–´ì˜ ìˆœì„œë‚˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ì•Šìœ¼ë©°, Skip-gram ëª¨ë¸ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/577bd4d6-d7a0-43a5-8c45-de8470101e16/image.png)

* **ìƒì„± ë°©ì‹**:

```
    (1) ê° ë¬¸ì„œì— ê³ ìœ í•œ IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    (2) ë¬¸ì„œ IDë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
```

* **íŠ¹ì§•**: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©°, ë¬¸ì„œ ë²¡í„°ë§Œìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê³„ì‚°ì´ ê°„ë‹¨í•˜ê³  ë¹ ë¦…ë‹ˆë‹¤.
* **ì˜ˆì‹œ ì½”ë“œ**:

```
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    
    documents = [TaggedDocument(words=["I", "love", "machine", "learning"], tags=['doc1']),
                 TaggedDocument(words=["Machine", "learning", "is", "fun"], tags=['doc2'])]
    
    model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4, dm=0)  # PV-DBOW
    vector = model.dv['doc1']
    
    print(vector)
    
    
```

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ìì—°ì–´ ì„ë² ë”© í•™ìŠµì„ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²•ê³¼ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.

* ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ Bag-of-Words(BoW), TF-IDF, N-Grams ëª¨ë¸ì„ í†µí•´ ë‹¨ì–´ì˜ ë¹ˆë„ì™€ ìˆœì„œë¥¼ ë°˜ì˜í•œ ë²¡í„°í™”ë¥¼ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.
* Word2Vec, GloVe, FastTextë¥¼ í†µí•´ ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ìœ ì‚¬ì„±ì„ ë°˜ì˜í•œ ë¶„ì‚° í‘œí˜„ ê¸°ë²•ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.
* ë˜í•œ, ë¬¸ì„œ ì„ë² ë”© ê¸°ë²•ì„ í†µí•´ ë¬¸ì„œ ì „ì²´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ë„ ì„¤ëª…í•˜ì˜€ìŠµë‹ˆë‹¤.

ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ ğŸ’Œ

