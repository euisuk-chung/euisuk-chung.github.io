---
title: "[이탐] 밀도 기반 이상탐지 알고리즘"
date: "2024-05-14"
tags:
  - "머신러닝"
  - "이상탐지"
year: "2024"
---

# [이탐] 밀도 기반 이상탐지 알고리즘

원본 게시글: https://velog.io/@euisuk-chung/밀도-기반-이상탐지-알고리즘



밀도 기반 이상탐지
==========

`밀도 기반 알고리즘`은 데이터 포인트 간의 지역적 밀도를 기반으로 이상치를 탐지합니다. 이러한 알고리즘은 데이터 분포가 불균일한 경우에도 잘 작동할 수 있습니다.

DBSCAN
------

* **링크**: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html>
* **정의**: 데이터 포인트가 밀집한 영역을 기반으로 클러스터를 형성하고, 주어진 거리 내에서 충분한 수의 이웃을 갖지 못한 점들을 이상치로 간주합니다.
* **적합한 데이터**: 밀도 변화가 심하거나 불균일한 데이터 분포를 가진 데이터셋에 적합합니다.
* **sklearn 함수**: `sklearn.cluster.DBSCAN`
  
  + **함수 설명**:
    
    - DBSCAN은 "Density-Based Spatial Clustering of Applications with Noise"의 약자로, 높은 밀도를 가진 핵심 샘플을 찾고 이를 확장하여 클러스터를 형성합니다. 이 방법은 유사한 밀도를 가진 데이터 포인트의 클러스터를 식별하는 데 특히 유용합니다.
    - 이 알고리즘은 특히 데이터에 잡음이 많은 경우에도 강력하며, 클러스터의 형태와 크기에 구애받지 않고 효과적으로 작동합니다.
  + **매개변수**:
    
    - `eps` (float, default=0.5): 이 매개변수는 한 데이터 포인트를 핵심 포인트로 고려하기 위해 필요한 최대 이웃 거리를 정의합니다. 적절한 eps 값을 선택하는 것이 결과의 품질에 결정적인 영향을 미칩니다.
    - `min_samples` (int, default=5): 핵심 포인트가 되기 위해 필요한 이웃의 최소 수입니다. 이 값이 높을수록 더 조밀한 클러스터를 형성하게 됩니다.
    - `metric` (str or callable, default='euclidean'): 인스턴스 간의 거리를 계산할 때 사용되는 메트릭입니다.
    - `algorithm` (str, default='auto'): 이웃을 찾기 위해 사용되는 알고리즘으로 'auto', 'ball\_tree', 'kd\_tree', 'brute'가 있습니다.
  + **속성**:
    
    - `core_sample_indices_`: 핵심 샘플의 인덱스 배열입니다.
    - `components_`: 핵심 샘플의 실제 값 배열입니다.
    - `labels_`: 각 데이터 포인트에 할당된 클러스터 라벨 배열입니다. 잡음 포인트는 -1 라벨을 받습니다.
  + **메서드**:
    
    - `fit(X[, y, sample_weight])`: 특징 또는 거리 행렬로부터 DBSCAN 클러스터링을 수행합니다.
    - `fit_predict(X[, y, sample_weight])`: 데이터 또는 거리 행렬로부터 클러스터를 계산하고 레이블을 예측합니다.
    - `get_metadata_routing()`: 이 객체의 메타데이터 라우팅을 가져옵니다.
    - `get_params([deep])`: 이 추정기의 파라미터를 가져옵니다.
    - `set_fit_request(*[, sample_weight])`: fit 메서드에 전달된 메타데이터 요청을 설정합니다.
    - `set_params(**params)`: 이 추정기의 파라미터를 설정합니다.
  + **예시**:
    
    ```
    from sklearn.cluster import DBSCAN
    import numpy as np
    
    # 예제 데이터
    X = np.array([[1, 2], [2, 2], [2, 3],
                  [8, 7], [8, 8], [25, 80]])
    
    # DBSCAN 모델 생성 및 적합
    clustering = DBSCAN(eps=3, min_samples=2).fit(X)
    
    # 클러스터 라벨 출력
    print(clustering.labels_)
    ```

✍️ 이 예제는 DBSCAN을 사용하여 간단한 데이터셋에 대한 클러스터링을 수행하는 방법을 보여줍니다. DBSCAN은 파라미터 설정에 따라 매우 다양한 데이터 구조를 효과적으로 클러스터링할 수 있으며, 특히 비선형적이고 복잡한 구조에서 강력한 성능을 발휘합니다.

LOF (Local Outlier Factor)
--------------------------

* **링크**: <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html>
* **정의**: LOF(Local Outlier Factor)는 주어진 데이터 포인트의 밀도와 그 주변 이웃의 밀도를 비교하여 상대적으로 밀도가 낮은 포인트를 이상치로 간주하는 이상치 탐지 알고리즘입니다. 이 알고리즘은 각 데이터 포인트의 지역 밀도 편차를 측정하여 이상치 점수를 계산합니다.
* **적합한 데이터**: 밀도가 불균일한 데이터 분포를 가진 데이터셋에서, 특히 데이터의 지역적 밀도 차이를 활용하여 이상치를 탐지하는데 적합합니다.
* **sklearn 함수**: `sklearn.neighbors.LocalOutlierFactor`
  
  + **함수 설명**:
    
    - LOF는 "Local Outlier Factor"의 약자로, 데이터 포인트의 지역 밀도를 평가하여 이상치를 탐지합니다. 이는 주어진 포인트가 주변 이웃에 비해 얼마나 고립되어 있는지를 측정합니다.
    - 이 알고리즘은 데이터의 밀도 기반 이상치를 탐지하는데 강력하며, 특히 데이터 분포가 균일하지 않은 경우에 효과적입니다.
  + **매개변수**:
    
    - `n_neighbors` (int, default=20): k-이웃 알고리즘에서 사용할 이웃의 수를 정의합니다. 이 값은 지역 밀도 계산에 중요한 영향을 미칩니다.
    - `algorithm` (str, default='auto'): 이웃을 찾기 위해 사용되는 알고리즘으로 'auto', 'ball\_tree', 'kd\_tree', 'brute'가 있습니다.
    - `leaf_size` (int, default=30): BallTree나 KDTree에서 사용되는 리프의 크기입니다. 이는 트리 구축과 쿼리 속도에 영향을 줍니다.
    - `metric` (str or callable, default='minkowski'): 거리 계산에 사용되는 메트릭으로 기본적으로 Minkowski 거리를 사용합니다.
    - `p` (float, default=2): Minkowski 메트릭에서 사용할 파라미터입니다. p=2는 유클리드 거리, p=1은 맨해튼 거리를 의미합니다.
    - `metric_params` (dict, default=None): 메트릭 함수에 대한 추가 매개변수입니다.
    - `contamination` ('auto' or float, default='auto'): 데이터셋에서 이상치의 비율을 지정합니다. 'auto'는 원 논문에서 제안된 값을 사용합니다.
    - `novelty` (bool, default=False): True로 설정하면 새로운 데이터에 대해 이상치를 탐지하는데 사용할 수 있습니다.
    - `n_jobs` (int, default=None): 병렬 처리에 사용할 CPU 코어 수를 지정합니다. -1로 설정하면 모든 프로세서를 사용합니다.
  + **속성**:
    
    - `negative_outlier_factor_`: 훈련 샘플의 반대 LOF 값을 나타내는 배열입니다. 값이 높을수록 정상에 가깝습니다.
    - `n_neighbors_`: 실제로 사용된 이웃의 수입니다.
    - `offset_`: 이상치 탐지를 위한 기준 오프셋입니다. 기본적으로 -1.5로 설정됩니다.
    - `effective_metric_`: 거리 계산에 사용된 메트릭입니다.
    - `effective_metric_params_`: 메트릭 함수의 추가 매개변수입니다.
    - `n_features_in_`: 훈련 중에 본 특성의 수입니다.
    - `feature_names_in_`: 훈련 중에 본 특성 이름 배열입니다.
    - `n_samples_fit_`: 훈련 데이터의 샘플 수입니다.
  + **메서드**:
    
    - `decision_function(X)`: 주어진 데이터 X의 Local Outlier Factor(LOF)의 반대값을 변환하여 반환합니다.
    - `fit(X[, y])`: 훈련 데이터셋에서 LOF(Local Outlier Factor) 탐지기를 학습시킵니다.
    - `fit_predict(X[, y])`: 훈련 데이터셋 X에 모델을 적합시키고 레이블을 반환합니다.
    - `get_metadata_routing()`: 이 객체의 메타데이터 라우팅을 가져옵니다.
    - `get_params([deep])`: 이 추정기의 파라미터를 가져옵니다.
    - `kneighbors([X, n_neighbors, return_distance])`: 특정 포인트의 K-이웃을 찾습니다.
    - `kneighbors_graph([X, n_neighbors, mode])`: 데이터 포인트에 대한 K-이웃 그래프(가중치 포함)를 계산합니다.
    - `predict([X])`: LOF를 기준으로 주어진 데이터 X의 레이블(정상: 1, 이상치: -1)을 예측합니다.
    - `score_samples(X)`: 주어진 데이터 X의 Local Outlier Factor(LOF)의 반대값을 반환합니다.
    - `set_params(**params)`: 이 추정기의 파라미터를 설정합니다.
  + **예시**:
    
    ```
    from sklearn.neighbors import LocalOutlierFactor
    import numpy as np
    
    # 예제 데이터
    X = [[-1.1], [0.2], [101.1], [0.3]]
    
    # LOF 모델 생성 및 적합
    clf = LocalOutlierFactor(n_neighbors=2)
    y_pred = clf.fit_predict(X)
    
    # 이상치 점수 출력
    print(clf.negative_outlier_factor_)
    ```

✍️ 이 예제는 LocalOutlierFactor를 사용하여 간단한 데이터셋에 대한 이상치 탐지를 수행하는 방법을 보여줍니다. LOF는 주변 이웃과의 밀도 비교를 통해 이상치를 효과적으로 식별할 수 있으며, 특히 밀도 기반 클러스터링에서 강력한 성능을 발휘합니다.

