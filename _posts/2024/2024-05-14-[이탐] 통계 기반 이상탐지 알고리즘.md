---
title: "[이탐] 통계 기반 이상탐지 알고리즘"
date: "2024-05-14"
tags:
  - "머신러닝"
  - "이상탐지"
year: "2024"
---

# [이탐] 통계 기반 이상탐지 알고리즘

원본 게시글: https://velog.io/@euisuk-chung/통계-기반-이상탐지-알고리즘-8rjzifg0



통계 기반 이상탐지
==========

`통계적 접근 방식`을 사용하여 모델이 데이터의 '정상적인' 행동을 학습하고 통계적으로 이례적인 행동을 보이는 데이터 포인트를 이상치로 식별합니다.

GMM (Gaussian Mixture Models)
-----------------------------

* **링크**: <https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html>
* **정의**: Gaussian Mixture Models(GMM)은 데이터를 여러 가우시안 분포의 혼합으로 모델링하여 각 데이터 포인트가 속할 확률을 계산합니다. 이를 통해 데이터를 클러스터링하거나 이상치를 탐지할 수 있습니다.
* **적합한 데이터**: 데이터가 하나 이상의 정규 분포를 따르는 것으로 가정할 때 효과적이며, 데이터의 잠재적 클러스터를 식별하는 데 유용합니다.
* **sklearn 함수**: `sklearn.mixture.GaussianMixture`
  
  + **함수 설명**:
    
    - GaussianMixture는 가우시안 혼합 모델을 사용하여 데이터의 확률 분포를 추정하는 클래스입니다. EM(Expectation-Maximization) 알고리즘을 사용하여 각 가우시안 성분의 파라미터를 최적화합니다.
    - 이 알고리즘은 데이터가 여러 개의 가우시안 분포로 구성되어 있다고 가정하고, 각 데이터 포인트가 특정 가우시안 분포에 속할 확률을 계산합니다.
  + **매개변수**:
    
    - `n_components` (int, default=1): 혼합 성분의 수를 정의합니다. 클러스터의 수와 대응합니다.
    - `covariance_type` (str, default='full'): 공분산 매트릭스의 형태를 정의합니다. 'full', 'tied', 'diag', 'spherical' 중 선택할 수 있습니다.
    - `tol` (float, default=1e-3): EM 알고리즘의 수렴 임계값입니다. 이 값 이하로 로그 가능도 변화가 작아지면 알고리즘이 수렴합니다.
    - `reg_covar` (float, default=1e-6): 공분산 매트릭스에 추가되는 정규화 항입니다. 공분산 매트릭스가 양의 정부호성을 갖도록 보장합니다.
    - `max_iter` (int, default=100): EM 알고리즘의 최대 반복 횟수입니다.
    - `n_init` (int, default=1): 초기화를 수행할 횟수입니다. 가장 좋은 결과를 선택합니다.
    - `init_params` (str, default='kmeans'): 초기화 방법을 정의합니다. 'kmeans', 'k-means++', 'random', 'random\_from\_data' 중 선택할 수 있습니다.
    - `weights_init` (array-like, default=None): 초기 가중치 배열입니다. 제공되지 않으면 `init_params` 방법을 사용하여 초기화됩니다.
    - `means_init` (array-like, default=None): 초기 평균 배열입니다. 제공되지 않으면 `init_params` 방법을 사용하여 초기화됩니다.
    - `precisions_init` (array-like, default=None): 초기 정밀도(공분산의 역행렬) 배열입니다. 제공되지 않으면 `init_params` 방법을 사용하여 초기화됩니다.
    - `random_state` (int, RandomState instance or None, default=None): 난수 생성 시드를 설정하여 결과를 재현 가능하게 합니다.
    - `warm_start` (bool, default=False): True로 설정하면 이전에 학습된 결과를 초기화에 사용합니다.
    - `verbose` (int, default=0): 학습 과정의 진행 상황을 출력합니다.
    - `verbose_interval` (int, default=10): 몇 번째 반복마다 출력을 할지 설정합니다.
  + **속성**:
    
    - `weights_`: 각 혼합 성분의 가중치 배열입니다.
    - `means_`: 각 혼합 성분의 평균 배열입니다.
    - `covariances_`: 각 혼합 성분의 공분산 행렬 배열입니다.
    - `precisions_`: 각 혼합 성분의 정밀도 행렬 배열입니다.
    - `precisions_cholesky_`: 각 혼합 성분의 정밀도 행렬의 Cholesky 분해입니다.
    - `converged_`: 모델이 수렴했는지 여부를 나타냅니다.
    - `n_iter_`: 수렴하는 데 사용된 반복 횟수입니다.
    - `lower_bound_`: EM 알고리즘에서 계산된 로그 가능도의 하한 값입니다.
    - `n_features_in_`: 학습 시 사용된 특성의 수입니다.
    - `feature_names_in_`: 학습 시 사용된 특성 이름 배열입니다.
  + **메서드**:
    
    - `aic(X)`: 주어진 데이터 X에 대한 현재 모델의 Akaike 정보 기준(AIC)을 계산합니다.
    - `bic(X)`: 주어진 데이터 X에 대한 현재 모델의 베이지안 정보 기준(BIC)을 계산합니다.
    - `fit(X[, y])`: EM 알고리즘을 사용하여 모델 파라미터를 추정합니다.
    - `fit_predict(X[, y])`: 주어진 데이터 X를 사용하여 모델 파라미터를 추정하고 레이블을 예측합니다.
    - `get_metadata_routing()`: 이 객체의 메타데이터 라우팅을 가져옵니다.
    - `get_params([deep])`: 이 추정기의 파라미터를 가져옵니다.
    - `predict(X)`: 학습된 모델을 사용하여 주어진 데이터 샘플 X의 레이블을 예측합니다.
    - `predict_proba(X)`: 각 샘플에 대한 구성 요소의 밀도를 평가합니다.
    - `sample([n_samples])`: 학습된 가우시안 분포에서 랜덤 샘플을 생성합니다.
    - `score(X[, y])`: 주어진 데이터 X에 대한 샘플 당 평균 로그 가능도를 계산합니다.
    - `score_samples(X)`: 각 샘플에 대한 로그 가능도를 계산합니다.
    - `set_params(**params)`: 이 추정기의 파라미터를 설정합니다.
  + **예시**:
    
    ```
    from sklearn.mixture import GaussianMixture
    import numpy as np
    
    # 예제 데이터
    X = np.array([[1, 2], [1, 4], [1, 0],
                  [10, 2], [10, 4], [10, 0]])
    
    # GMM 모델 생성 및 적합
    gm = GaussianMixture(n_components=2, random_state=0).fit(X)
    
    # 각 혼합 성분의 평균 출력
    print(gm.means_)
    
    # 예측 라벨 출력
    print(gm.predict([[0, 0], [12, 3]]))
    ```

✍️ 이 예제는 GaussianMixture를 사용하여 데이터를 두 개의 가우시안 성분으로 모델링하고, 새로운 데이터 포인트의 클러스터를 예측하는 방법을 보여줍니다. GMM은 데이터의 잠재적인 분포를 학습하고 이를 통해 클러스터링이나 이상치 탐지를 수행하는 데 매우 유용합니다.

PCA (Principal Component Analysis)
----------------------------------

* **링크**: <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>
* **정의**: PCA(Principal Component Analysis)는 데이터의 주성분을 분석하여 차원을 축소하는 기법입니다. 이를 통해 데이터의 주요 변동성을 캡처하고, 원 데이터와 재구성된 데이터 간의 차이를 통해 이상치를 탐지할 수 있습니다.
* **적합한 데이터**: 고차원 데이터셋에서 주요 변동성을 파악하고 차원의 저주를 줄이고자 할 때 유용합니다. 주로 데이터 시각화, 노이즈 제거, 특징 추출 등에 사용됩니다.
* **sklearn 함수**: `sklearn.decomposition.PCA`
  
  + **함수 설명**:
    
    - PCA는 특이값 분해(Singular Value Decomposition, SVD)를 사용하여 데이터를 낮은 차원으로 투영하는 선형 차원 축소 기법입니다. 입력 데이터는 각 특성에 대해 중심화되며, SVD를 통해 주성분을 추출합니다.
    - LAPACK 구현의 전체 SVD 또는 데이터의 형태와 추출할 성분의 수에 따라 확률적 SVD를 사용합니다.
  + **매개변수**:
    
    - `n_components` (int, float or 'mle', default=None): 유지할 주성분의 수입니다. 기본적으로 모든 성분을 유지합니다.
    - `copy` (bool, default=True): True로 설정하면 fit 시 입력 데이터를 복사하고, False로 설정하면 데이터를 덮어씁니다.
    - `whiten` (bool, default=False): True로 설정하면 주성분 벡터를 단위 분산을 가지도록 스케일링합니다. 이는 예측 정확도를 높일 수 있습니다.
    - `svd_solver` (str, default='auto'): SVD를 계산할 방법입니다. 'auto', 'full', 'arpack', 'randomized' 중 선택할 수 있습니다.
    - `tol` (float, default=0.0): 'arpack' SVD 솔버의 특이값에 대한 허용 오차입니다.
    - `iterated_power` (int or 'auto', default='auto'): 'randomized' SVD 솔버에서 파워 방법의 반복 횟수입니다.
    - `n_oversamples` (int, default=10): 'randomized' SVD 솔버에서 추가로 샘플링할 무작위 벡터의 수입니다.
    - `power_iteration_normalizer` (str, default='auto'): 'randomized' SVD 솔버의 파워 반복 정규화 방법입니다.
    - `random_state` (int, RandomState instance or None, default=None): 난수 생성을 제어하여 결과를 재현 가능하게 합니다.
  + **속성**:
    
    - `components_`: 주성분 벡터 배열입니다.
    - `explained_variance_`: 선택된 각 성분에 의해 설명된 분산의 양입니다.
    - `explained_variance_ratio_`: 선택된 각 성분에 의해 설명된 분산의 비율입니다.
    - `singular_values_`: 선택된 각 성분에 해당하는 특이값입니다.
    - `mean_`: 각 특성의 경험적 평균입니다.
    - `n_components_`: 추정된 주성분의 수입니다.
    - `n_samples_`: 훈련 데이터의 샘플 수입니다.
    - `noise_variance_`: 추정된 노이즈 공분산입니다.
    - `n_features_in_`: 학습 시 본 특성의 수입니다.
    - `feature_names_in_`: 학습 시 본 특성 이름 배열입니다.
  + **메서드**:
    
    - `fit(X[, y])`: 주어진 데이터 X로 모델을 학습시킵니다.
    - `fit_transform(X[, y])`: 주어진 데이터 X로 모델을 학습시키고, 차원 축소를 적용합니다.
    - `get_covariance()`: 생성 모델을 사용하여 데이터의 공분산을 계산합니다.
    - `get_feature_names_out([input_features])`: 변환된 출력 특성 이름을 가져옵니다.
    - `get_metadata_routing()`: 이 객체의 메타데이터 라우팅을 가져옵니다.
    - `get_params([deep])`: 이 추정기의 파라미터를 가져옵니다.
    - `get_precision()`: 생성 모델을 사용하여 데이터의 정밀도 행렬을 계산합니다.
    - `inverse_transform(X)`: 데이터를 원래의 공간으로 변환합니다.
    - `score(X[, y])`: 모든 샘플의 평균 로그 가능도를 반환합니다.
    - `score_samples(X)`: 각 샘플의 로그 가능도를 반환합니다.
    - `set_output(*[, transform])`: 출력 컨테이너를 설정합니다.
    - `set_params(**params)`: 이 추정기의 파라미터를 설정합니다.
    - `transform(X)`: 주어진 데이터 X에 차원 축소를 적용합니다.
  + **예시**:
    
    ```
    from sklearn.decomposition import PCA
    import numpy as np
    
    # 예제 데이터
    X = np.array([[-1, -1], [-2, -1], [-3, -2],
                  [1, 1], [2, 1], [3, 2]])
    
    # PCA 모델 생성 및 적합
    pca = PCA(n_components=2)
    pca.fit(X)
    
    # 설명된 분산 비율 출력
    print(pca.explained_variance_ratio_)
    
    # 특이값 출력
    print(pca.singular_values_)
    
    # 새로운 데이터에 대한 주성분 투영
    X_new = pca.transform([[0, 0], [12, 3]])
    print(X_new)
    ```

✍️ 이 예제는 PCA를 사용하여 데이터의 주성분을 추출하고, 새로운 데이터 포인트를 주성분 공간에 투영하는 방법을 보여줍니다. PCA는 데이터의 주요 변동성을 파악하고 차원을 축소하여 데이터 분석 및 시각화에서 유용하게 활용될 수 있습니다.

