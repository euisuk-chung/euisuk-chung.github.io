---
title: "[Paper Review] 합성 데이터 = 모델 붕괴?"
date: "2024-08-26"
tags:
  - "paper-review"
year: "2024"
---

# [Paper Review] 합성 데이터 = 모델 붕괴?

원본 게시글: https://velog.io/@euisuk-chung/리뷰-인공-데이터로-모델을-학습하면-모델-붕괴가-일어난다고



들어가며
====

최근 인공지능(AI) 분야에서 매우 흥미로운 두 논문이 Nature에 게재되어 큰 화제를 모으고 있습니다. 아래 두 논문은 공통적으로 AI 모델의 발전과 관련된 중대한 문제를 다루고 있으며, AI 기술의 장기적인 위험성을 경고하고 있습니다.

* **AI models collapse when trained on recursively generated data**  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/9cfabae6-bfb3-47cd-96ae-5826d08d9616/image.png)
* **AI produces gibberish when trained on too much AI-generated data**  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/b93a3609-8077-4595-a1f2-6b580c581899/image.png)

저는 이번 논문을 계기로 **Nature 학술지의 간략한 소개**와 함께, 위 **두 논문이 다루고 있는 중요한 내용**을 자세히 정리해보겠습니다.

Nature 학술지 소개
=============

먼저, Nature에 대해 간단히 소개하겠습니다. Nature는 1869년에 창간된 이후로 과학계에서 가장 중요한 학술지 중 하나로 자리 잡았습니다. 이 저널은 다양한 과학 분야의 최신 연구 결과를 발표하는데, 논문들이 게재되기 위해서는 엄격한 피어 리뷰 과정을 거쳐야 합니다. 따라서 Nature에 실리는 논문들은 그 자체로 과학적 신뢰성이 높으며, 학문적 및 사회적으로 큰 영향을 미칩니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/df826fb4-9afe-45ad-932d-2ea4b431cae9/image.png)

또한 Nature에는 다양한 유형의 글들이 게재되는데, 그 중 몇 가지를 소개하자면:

* **Research Article**: 원본 연구 결과를 상세히 보고하는 정식 학술 논문입니다.
* **News & Views**: 최근 발표된 중요 연구에 대한 전문가의 해설과 논평입니다.
* **Letter**: 짧고 중요한 연구 결과를 신속하게 보고하는 형식입니다.
* **Review**: 특정 주제에 대한 포괄적인 개요와 분석을 제공합니다.
* **Perspective**: 특정 과학 분야의 현재 상태와 미래 방향에 대한 개인적 견해를 제시합니다.
* **Commentary**: 과학 정책, 윤리, 사회적 이슈 등에 대한 의견을 제시합니다.
* **Editorial**: 저널 편집진의 의견이나 중요한 과학적 이슈에 대한 입장을 표명합니다.
* **Correspondence**: 이전에 출판된 논문에 대한 짧은 의견이나 추가 정보를 제공합니다.

이렇듯 Nature는 다양한 형식의 글을 통해 과학적 논의를 촉진하고, 독자들이 최신 과학 연구를 다각도로 이해할 수 있도록 돕고 있습니다.

### AI models collapse when trained on recursively generated data

(번역) **AI 모델 붕괴: AI 모델이 자기 자신을 망칠 수 있다?**

* 링크 : <https://www.nature.com/articles/s41586-024-07566-y>

`첫 번째 논문`, **"AI models collapse when trained on recursively generated data"**에서는 대형 언어 모델(LLM)과 같은 생성형 인공지능이 반복적으로 자체 생성 데이터를 학습할 때 발생하는 "모델 붕괴(model collapse)" 현상에 대해 연구하고 있습니다. 논문의 핵심 내용을 자세히 설명하겠습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/361d7a85-f364-4228-a575-802006aecbe0/image.png)

#### 1. **모델 붕괴란 무엇인가?**

`모델 붕괴`는 학습된 생성 모델들이 이전 세대에서 생성한 데이터를 다시 학습할 때 발생하는 퇴행적 과정입니다.

* **정의**: 모델 붕괴는 학습된 생성 모델의 세대에 영향을 미치는 퇴행적 과정으로, 모델이 생성한 데이터가 다음 세대의 훈련 세트를 오염시키는 현상입니다. 이로 인해 후속 모델들이 현실을 잘못 인식하게 됩니다.
* **두 가지 특수 경우**:
  
  + `조기 모델 붕괴`: 모델이 분포의 꼬리 정보(낮은 확률 이벤트)를 잃기 시작합니다.
  + `후기 모델 붕괴`: 모델이 원래 분포와 거의 유사하지 않은 분포에 수렴하며, 분산이 상당히 줄어듭니다.
* **원인**:
  
  + `통계적 근사 오류`: 통계적 근사 오류는 샘플 수가 유한할 때 발생하는 오류입니다. 이는 모집단에서 샘플을 추출할 때, 샘플의 크기가 충분하지 않거나 샘플의 분포가 원 데이터의 실제 분포를 잘 반영하지 못할 때 나타납니다.
    
    - **샘플의 크기**: 샘플 크기가 작을 경우, 특히 저확률 이벤트(즉, 발생 빈도가 매우 낮은 사건들)의 사례는 통계적인 식별이 어렵습니다. 예를 들어, 수십 개의 샘플만 사용하는 경우, 이 샘플이 대표할 수 있는 전체 모집단의 특성을 충분히 반영하지 못할 수 있습니다.
    - **꼬리 사라짐**: 사건의 꼬리 부분은 다른 사건에 비해 발생할 가능성이 매우 낮기 때문에, 샘플링 과정에서 이런 사건들이 대표적으로 나타나지 않을 수 있습니다. 시간이 지나면서 반복적으로 모델이 학습할 때, 이 저확률 사건들이 점점 더 소외되며 결국 잊혀지고, 모델의 분포는 더욱 중앙집중적으로 변하게 됩니다.
  + `함수 표현 오류`: 함수 표현 오류는 신경망이 특정 함수 공간 내에서 데이터를 표현할 수 있는 능력의 한계를 반영합니다. 이는 네트워크 구조의 크기나 형태에 따라 달라집니다.
    
    - **유한한 표현력**: 신경망은 이론적으로 무한한 표현력을 가지고 있지만, 실제로 사용되는 네트워크의 크기나 구조는 제한적입니다. 예를 들어, 단일 은닉층의 신경망은 복잡한 비선형 함수로 사람들을 표현할 수 없습니다.
    - **떨어진 가능성**: 이 오류는 특정 구간에서 중요한 이벤트(예: 극단적 사건)에 대해 잘못된 확률 예측을 하게 만듭니다. 즉, 모델이 잘 적합되지 않는 구간에서 잘못된 가정이나 추정에 의해 중요한 정보를 놓칠 수 있습니다. 예를 들어, 두 개의 서로 다른 가우시안 분포를 단일 가우시안 분포로 모델링할 경우, 이 정보 손실이 발생할 수 있습니다.
  + `함수 근사 오류`: 함수 근사 오류는 모델이 주어진 데이터에 대해 학습하는 과정에서 발생하는 오류입니다. 이는 무한한 데이터와 완벽한 표현 가능성에도 불구하고 나타날 수 있습니다.
    
    - **학습 절차의 한계**: 학습 알고리즘(예: 경량화된 경량 경량 네트워크)을 사용하는 과정에서 발생할 수 있는 구조적 편향과 오류가 포함됩니다. 예를 들어, 경량화된 경량 경량 경량 경량 모델은 무한한 데이터에 대해 훈련하더라도, 본질적으로 학습하지 못하는 구조적 한계가 존재합니다.
    - **결과적인 저하**: 근사 시점에서의 오류는, 시간이 지남에 따라 학습된 모델이 원래의 데이터 분포로부터 멀어질 수 있게 만듭니다. 이는 각 세대가 앞서 언급한 통계적 및 표현력 오류와 합쳐져, 모델의 성능 저하로 이어질 수 있습니다. 특히, 이러한 오류가 누적되면 모델이 예측하는 분포와 실제 분포 간의 차이가 점점 더 커지게 됩니다.
* **결과**: 시간이 지남에 따라 모델은 원래 데이터 분포에 대한 정보 손실이 있으며, 이는 분포의 축소를 초래합니다. 모델은 실제 현실을 왜곡하여, '확률이 높은 사건'은 과대평가되고, '확률이 낮은 사건'은 과소평가됩니다.

> 💡 **혼란도(perplexity)**는 모델이 얼마나 "혼란스러워"하는지를 나타내는 지표입니다. 즉, 주어진 데이터에 대해 모델이 얼마나 불확실한지를 측정하는 지표로, 값이 낮을수록 모델이 데이터를 더 잘 예측하고 있다는 의미입니다.
> 
> * **낮은 perplexity**는 모델이 잘 학습한 것이고, **높은 perplexity**는 모델이 예측에 어려움을 겪고 있다는 뜻입니다.
> * 특히, 언어 모델에서 텍스트의 자연스러움을 평가할 때 매우 유용하며, 낮은 혼란도를 달성할수록 모델이 더 좋은 성능을 나타낸다고 볼 수 있습니다.
>   + 혼란도(Perplexity)는 기본적으로 언어 모델이 특정 단어 시퀀스(문장 등)를 예측하는 데 얼마나 효율적인지를 나타내는 지표입니다. 참고로, 언어 모델에서 혼란도는 다음과 같이 정의됩니다.  
>     
>     PPL(W)=2H(W)PPL(W) = 2^{H(W)}PPL(W)=2H(W)
>   + 여기서 H(W)H(W)H(W)는 시퀀스 WWW에 대한 모델의 **평균 교차 엔트로피**입니다.
>   + 언어 모델에서 교차 엔트로피는 모델이 예측한 확률과 실제 데이터 간의 차이를 나타내는 지표입니다.
>   + 간단하게 말하면, **혼란도는 모델이 예측한 확률의 역수**에 해당하며, 값이 작을수록 모델이 예측을 잘하고 있는 것입니다.

> 📊 `Figure 1`은 "모델 붕괴(model collapse)"의 피드백 메커니즘과 관련된 학습 과정 및 실험 결과를 시각적으로 보여줍니다. 각 하위 그림인 a, b, c는 다른 측면을 다루고 있으며, 다음과 같은 의미를 지니고 있습니다.

**(a) 모델 붕괴 설정**  

![](https://velog.velcdn.com/images/euisuk-chung/post/84d98518-4ff0-4386-b58b-92399d877d21/image.png)

* **Real Data (실제 데이터)**: 처음에는 사람이 생성한 깨끗한 데이터로 학습을 시작합니다.
* **Model 0, Model 1, Model n**: Model 0이 먼저 실제 데이터를 학습하고, 그 이후에는 Model 0이 생성한 데이터를 다시 Model 1이 학습하게 됩니다. 이 과정은 n세대까지 반복됩니다.
* **모델 붕괴**: 시간이 지나면서, 모델이 생성한 데이터는 모델 자체의 현실을 왜곡하기 시작합니다. 특히 저확률 사건(드문 사건)을 점차 잊어버리게 됩니다. 그림 오른쪽에서 붉은 화살표와 노란 텍스트는 모델이 과대평가되는 사건(Probable events)과 과소평가되는 사건(Improbable events)이 어떻게 현실을 오염시키는지를 보여줍니다.
  + **Finite sampling(유한 샘플링)** 및 **Approximate fitting(근사 피팅)**: 샘플링과 피팅 과정에서 발생하는 오류로 인해 모델 붕괴가 가속화됩니다. 시간이 지남에 따라 모델은 현실과 점점 더 멀어지게 됩니다.

**(b) 원본 데이터 없이 5번의 학습 에포크**  

![](https://velog.velcdn.com/images/euisuk-chung/post/2de89ac7-147f-4422-ae0e-3c9867a40550/image.png)

두 번째 그림은 **원본 데이터를 보존하지 않고** 5번의 에포크 동안 학습한 모델들의 성능을 나타냅니다.

* **왼쪽 그래프**는 각 세대의 모델이 생성한 데이터 포인트들의 perplexity(혼란도) 분포를 보여줍니다. Generation 0에서 Generation 9까지의 데이터를 보여주며, 세대가 진행될수록 모델이 생성한 데이터의 분포가 점점 더 실제 데이터와 차이가 나기 시작합니다.
  + 예를 들어, Generation 0은 실제 데이터에 가깝지만, Generation 9으로 갈수록 분포가 더 넓고 꼬리가 길어지면서, 모델이 잘못된 데이터를 생성하는 경향이 증가합니다.
* **오른쪽 그래프**는 각 세대에서 학습된 모델이 원본 wikitext2 데이터셋에서 얼마나 잘 학습했는지를 보여줍니다. 세대가 지날수록 perplexity가 증가하는데, 이는 모델이 데이터를 제대로 이해하지 못하고 있다는 것을 의미합니다.

**(c) 원본 데이터 10%를 보존하고 10번의 학습 에포크**  

![](https://velog.velcdn.com/images/euisuk-chung/post/fbaffb14-c21d-4b5e-8d0d-1aca3f0d5b90/image.png)

세 번째 그림은 **원본 데이터의 10%를 보존**하면서 10번의 에포크 동안 학습한 모델들의 성능을 나타냅니다.

* **왼쪽 그래프**는 b 그림과 유사하게 각 세대에서 생성된 데이터의 perplexity 분포를 보여줍니다. 여기서도 세대가 지남에 따라 혼란도가 증가하지만, 일부 원본 데이터를 보존했기 때문에 그 변화가 조금 더 완만합니다.
* **오른쪽 그래프**는 원본 데이터를 일부 보존함으로써, 모델의 성능 저하가 덜 극단적이라는 점을 보여줍니다. 원본 데이터를 전혀 사용하지 않은 b 그림에 비해 perplexity가 덜 증가했습니다.

> 🤔 **서쿠 의견** : 그렇지만 제 개인적인 생각으로는 전체 데이터를 전부 다 synthetic 데이터로 학습하게 될 경우는 드물거 같은데, 특히나 augmentation과 같은 경우로 쓰이지 않을까 생각이 들긴합니다만... 실제로 뭐 개인정보 보안이나 그런 케이스에서는 새롭게 개인정보가 없는 이미지를 만들고 학습을 하기도 하지만 신기하군요.

#### 2. **이론적 직관**

연구진은 모델 붕괴 현상이 모든 생성형 모델에서 발생할 수 있음을 수학적으로 증명합니다. 특히, 통계적 근사 오류와 함수적 표현력의 한계가 붕괴를 가속화시킵니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/63dc0058-e86f-4082-977b-365e532c9f0b/image.png)  

이미지 출처: 길벗

논문에서는 이 현상을 설명하기 위해 두 가지 수학적 모델을 사용했습니다:

* **이산 확률 분포**: 이산 분포에서 모델 붕괴는 주로 통계적 오류로 인해 발생합니다. 여기서는 다음과 같은 내용을 통해 모델 붕괴를 설명합니다.
  
  + `통계적 근사`: 이 경우, 함수 근사나 표현력 오류가 없는 상황에서 모델 붕괴는 오로지 통계적 오류에 의해 발생합니다. 데이터의 샘플링 과정에서 저확률 사건은 통계적으로 잘 대표되지 못하게 되며, 이로 인해 점차적으로 그 사건들에 대한 정보가 잃어버려져 가는 과정을 설명합니다.
  + `확률 손실`: 각 세대의 훈련에서 특정 사건의 확률이 낮은 경우, 해당 사건을 샘플링하지 못할 확률이 증가합니다. 이는 시간이 지나면서 모델이 낮은 확률로 발생하는 사건을 잊게 만들고, 결과적으로 분포의 꼬리 부분이 점차 사라지게 됩니다. 추정된 데이터 분포는 결국 델타 함수 형태로 수렴하게 되며, 이는 모델이 원래 데이터의 특성을 더 이상 반영하지 못하게 됨을 시사합니다.
  + `마르코프 과정`: 이산 분포에서의 모델 붕괴를 마르코프 체인(Markov Chain)으로 설명하며, 이 과정에서는 다음 세대의 분포가 이전 세대에만 의존한다고 가정합니다. 모든 세대가 같은 값을 가질 경우, 후속 세대의 분포는 델타 함수와 같아지며, 따라서 원래 분포와의 정보가 완전히 상실됩니다.
* **다차원 가우시안 분포**: 다변량 가우시안 모델에서의 모델 붕괴는 여러 가지 특성으로 인해 더욱 복잡합니다. 여기서는 다음과 같은 점으로 설명됩니다.
  
  + `재귀적 적합`: 각 세대는 이전 세대의 평균 및 분산을 바탕으로 다변량 가우시안 분포로 맞춰지며, 이는 샘플 크기가 고정되어 있을 때 사용되는 무편향 추정량에 따라 수행됩니다. 이러한 재귀적 적합 과정에서 원래 데이터 분포에서 발생한 통계적 오류가 누적될 수 있습니다.
  + `다변량 함수 근사`: 이 과정에서 발생하는 디지털 통계적 근사와 표현력 오류는 각 세대가 이전 세대의 분포를 점차적으로 부정확하게 만들 수 있으며, 그 결과 원래 분포에서 점점 더 멀어지게 됩니다. 이는 결국 모델이 원래의 데이터 분포로부터 동떨어진 분포로 수렴하게 만듭니다.
  + `수렴과 분산 감소`: 논문에서는 원래 데이터가 분산이 있는 비가우시안 분포에서 샘플링된다고 가정하며, 이 각 세대의 추정된 분포가 원래 분포로부터 무한히 멀어지고, 동일한 시간 내에 분산이 제로로 수렴하게 되는 과정을 보여줍니다. 이로 인해, 점차적으로 모델이 동일한 상태로 수렴하게 되고, 초기 데이터를 학습하는 데 필요한 정보를 전부 잃게 됩니다.

#### 3. **언어 모델에서의 붕괴**

이 챕터는 주로 LLM에서 **모델 붕괴**가 어떻게 진행되는지, 그 과정에서 발생하는 성능 저하를 설명하며, 실험을 통해 이 현상을 입증하려고 합니다.

**3.1. 배경**  

이전 실험들은 비교적 단순한 모델들(예: 가우시안 혼합 모델(GMM)이나 변분 오토인코더(VAE))에서 모델 붕괴가 발생하는 것을 확인했습니다. (앞장 2)

하지만 **대형 언어 모델**(LLM)은 그 규모와 복잡성 때문에 약간 다른 방식으로 붕괴 현상이 나타납니다. LLM은 매우 큰 데이터셋을 사용해 훈련되며, 주로 GPT-2, RoBERTa, BERT와 같은 대형 모델들을 처음부터 다시 훈련하지 않고, **사전 학습된 모델을 미세 조정(fine-tuning)**하는 방식으로 사용합니다.

이 챕터는 LLM에서 모델 붕괴 현상이 실제로 발생하는지, 그리고 그 정도가 어떻게 다른지에 대해 설명합니다.

**3.2. 모델 붕괴 실험**

LLM에서 모델 붕괴를 재현하기 위해, 연구진은 **OPT-125m**이라는 언어 모델을 사용했습니다. 이 모델은 Meta(메타)에서 제공한 모델로, **wikitext2** 데이터셋을 사용해 학습되었습니다.

> `OPT-125m`은 Meta에서 개발한 오픈AI 언어 모델 시리즈 중 하나로, 훈련된 매개변수 수가 약 125백만 개(125 million)인 모델입니다.
> 
> * "OPT"는 "Open Pre-trained Transformer"의 약어로, 이 모델은 Transformer 아키텍처를 기반으로 하여 설계되었습니다.  
>   
>   ![](https://velog.velcdn.com/images/euisuk-chung/post/bebf31b7-58dd-45c9-b866-3cc71aebe1d3/image.png)

**OPT-125m** 모델은 다양한 자연어 처리(NLP) 작업에서 사용될 수 있으며, 특히 텍스트 생성, 질문 응답, 대화형 AI 및 기타 언어 관련 작업에서 활용됩니다. OPT 모델은 공공 데이터와 인공지능 생태계의 발전에 기여하는 방향으로 개발되었고, 사용자들이 쉽게 접근할 수 있도록 하여 연구 및 상용 응용 프로그램에서 활용될 수 있도록 되어 있습니다.

논문에서는 OPT-125m은 큰 모델에 비해 상대적으로 적은 수의 매개변수를 가지므로, 훈련 및 실행 비용이 낮아 다양한 환경에서 사용할 수 있는 이점이 있다고 얘기합니다.

> 🤔 **서쿠 의견** : 그냥 작은 모델이라서 synthetic 데이터에 취약한 거일 수도 있지 않을까?
> 
> * OPT-125m은 매개변수(parameter)가 약 1억 2천 5백만 개로, GPT-3나 GPT-4 같은 초대형 언어 모델과 비교하면 상대적으로 작은 모델입니다. 일반적으로 모델 크기가 클수록 더 복잡한 패턴을 학습할 수 있으며, 다양한 데이터에서 더 좋은 성능을 보이는 경향이 있습니다.
> * 작은 모델은 큰 모델에 비해 학습 데이터의 패턴을 충분히 포착하지 못할 가능성이 높습니다. 이 때문에 합성 데이터나 노이즈가 많은 데이터에 대해 더 쉽게 과적합하거나, 왜곡된 패턴을 학습할 수 있습니다. 이 현상은 작은 모델이 더 적은 매개변수를 가지고 있기 때문에, 데이터에서 잘못된 패턴이 포함될 경우 이를 극복할 능력이 부족하기 때문입니다.

1) **실험 설정**

* 연구진은 **미세 조정(fine-tuning)** 환경에서 실험을 진행했습니다. 미세 조정은 이미 사전 학습된 모델을 사용하고, 이를 새로운 데이터로 추가 학습시키는 과정입니다.
* 이 실험에서는 각 세대의 모델이 생성한 데이터를 사용해 다음 세대의 모델을 학습시킵니다. 즉, **모델 n 세대**는 **모델 n-1 세대**가 생성한 데이터를 학습하는 방식으로 진행됩니다.
* 학습은 두 가지 조건에서 수행되었습니다:
  
  + **원본 데이터를 사용하지 않고 5번의 에포크(epoch) 동안 학습**.
  + **원본 데이터의 10%를 보존하면서 10번의 에포크(epoch) 동안 학습**.

2) **실험 과정**

실험은 다음과 같이 진행되었습니다:

* 원본 **wikitext2** 데이터셋에서 모델을 훈련시킵니다. 훈련된 모델은 64개의 토큰 단위로 데이터를 예측합니다.
* 각 세대에서 모델이 생성한 데이터를 다시 학습에 사용하여 새로운 세대를 훈련합니다.
* 이러한 실험을 5번 반복했으며, 각 실험은 다른 무작위 초기값(seed)을 사용하여 진행되었습니다. 그 결과, 모델이 생성한 데이터의 **perplexity(혼란도)**가 어떻게 변화하는지를 측정했습니다.

**3.3. 결과 분석**

모델 붕괴 실험 결과는 모델 붕괴가 실제로 LLM에서 발생하며, 그 과정에서 성능이 저하되는 것을 보여줍니다. 이 실험은 두 가지 주요 설정에서 진행되었으며, 각 설정에서의 결과는 다음과 같습니다.

(1) **원본 데이터 없이 5번의 에포크**

이 실험에서는 학습 초기에만 원본 데이터를 사용하고, 이후에는 이전 세대 모델이 생성한 데이터로만 학습을 진행했습니다.

* 실험 결과, **혼란도(perplexity)**가 급격히 증가했습니다. 이는 모델이 데이터의 특성을 잃고, 점차 잘못된 데이터를 학습했기 때문입니다.
* 그래프에서 보듯이, 세대가 반복될수록 혼란도가 상승하며, 이는 모델이 점점 더 실제 데이터를 제대로 이해하지 못하게 됨을 나타냅니다.
* **세대가 지날수록**: 초기 세대에서 성능이 급격히 떨어지며, 세대가 거듭될수록 더 이상 개선되지 않고 일정한 수준에서 성능이 정체되는 경향이 있습니다.

(2) **원본 데이터 10%를 보존한 경우**

이 실험에서는 각 세대에서 원본 데이터의 10%를 유지하면서 학습을 진행했습니다.

* 이 설정에서는 혼란도가 덜 급격히 증가했으며, 모델 성능이 조금 더 유지되었습니다.
* 원본 데이터를 일부라도 보존함으로써, 모델 붕괴를 어느 정도 늦출 수 있음을 보여줍니다.
* 그래프에서도 혼란도가 점진적으로 증가하지만, 원본 데이터를 전혀 사용하지 않은 경우에 비해 덜 극단적입니다.

**3.4. 텍스트 생성 예시**

논문에서는 **텍스트 생성 예시**도 포함되어 있는데, 이는 각 세대에서 모델이 생성한 텍스트의 질이 어떻게 악화되는지를 보여줍니다.

* **Generation 0**: 원본 wikitext2 데이터로 학습한 초기 모델은 비교적 자연스럽고 의미 있는 문장을 생성합니다.
* **Generation 1**: 첫 번째 세대에서는 문장 구조는 여전히 남아 있지만, 내용의 일관성이 떨어지고 있습니다.
* **Generation 5**: 다섯 번째 세대에서는 문장이 난해하고, 일관성이 없는 내용들이 포함되기 시작합니다.
* **Generation 9**: 아홉 번째 세대에서는 모델이 거의 의미 없는 무작위적인 단어들을 생성하며, 완전히 붕괴된 상태를 보여줍니다.

**3.5. 반복 현상(Ablation: Repetitions)**

모델이 붕괴되면, 생성된 텍스트에서 **반복적인 구문**이 자주 등장하는 문제가 발생합니다. 논문에서는 이 문제를 해결하기 위해 **반복 페널티(repetition penalty)**를 적용한 실험도 진행했으나, 이는 오히려 모델의 성능을 더욱 저하시켰습니다.

#### 4. Discussion

이 섹션에서는 **모델 붕괴의 장기적 영향**과 **언어 모델에서의 함의**에 대해 논의합니다.

**모델 붕괴의 장기적 영향**  

연구팀은 LLM의 학습에서 발생하는 **장기적 데이터 오염** 문제를 심도 있게 다룹니다. 특히, 대규모로 생성된 모델 데이터가 장기적으로 어떻게 **데이터 분포를 오염시키고** 결과적으로 모델 붕괴를 촉진하는지 설명합니다.

1. **데이터 오염의 예**
   
   * LLM과 같은 자동화된 생성형 AI는 인간의 개입 없이 대규모 데이터를 생성할 수 있습니다. 이와 유사한 인간 주도 데이터 오염의 예로, 클릭 농장, 콘텐츠 농장, 트롤 농장 등을 들 수 있습니다. 이러한 데이터 오염은 소셜 미디어나 검색 엔진 알고리즘을 교란시키는 결과를 낳았습니다.
   * Google은 이를 방지하기 위해 교육기관 도메인과 같은 신뢰할 수 있는 출처의 콘텐츠를 우선적으로 배치하는 방식으로 알고리즘을 변경했고, DuckDuckGo는 이러한 콘텐츠를 완전히 제거하는 방식으로 대응했습니다.
2. **LLM에서의 문제 확대**
   
   * 그러나 LLM은 인간의 개입 없이 **대규모로 자동화된 방식으로 데이터를 생성할 수 있기 때문에**, 데이터 오염 문제가 훨씬 더 심각해질 수 있습니다. 시간이 지나면, **LLM이 생성한 데이터가 인터넷 상에서 점점 더 큰 비율을 차지하게 되어, 모델 붕괴가 더욱 심각해질 수 있습니다**.
   * 모델 붕괴는 **저확률 사건**을 예측하는 능력을 저하시켜, 특히 **소수 집단이나 중요한 예외적인 사건**을 처리하는 데 심각한 영향을 미칠 수 있습니다. 이러한 사건을 무시하는 모델은 불공정하거나 잘못된 예측을 하게 될 수 있습니다.

**‘선발자 우위(First Mover Advantage)’**

* 연구는 **‘선발자 우위(first mover advantage)’** 개념을 도입하여, 초기 세대의 모델이 중요하다는 점을 강조합니다.
  + **초기 세대의 모델이 생성한 데이터**는 후속 모델들이 학습하는 데이터 분포를 크게 좌우할 수 있습니다. 만약 초기 모델이 잘못된 데이터 패턴을 학습하고 이를 후속 모델들이 반복해서 학습하면, 점차적으로 더 심각한 모델 붕괴가 발생하게 됩니다.
  + 따라서, 장기적인 학습에서 모델이 붕괴하지 않기 위해서는 **원본 데이터**에 지속적으로 접근할 수 있어야 하며, LLM이 생성한 데이터를 다른 데이터와 구별하는 메커니즘이 필요합니다.

**데이터 원본 문제**

* 인터넷에서 수집한 데이터 중 **LLM이 생성한 데이터와 실제 인간이 생성한 데이터를 구분하는 문제**는 매우 어려운 과제입니다. 대규모 웹 데이터를 학습에 사용하기 위해서는 각 데이터의 **출처**를 명확히 구분할 수 있어야 하며, 그렇지 않으면 모델 붕괴가 가속화될 수 있습니다.
* 연구팀은 **데이터 출처 관리의 중요성**을 강조하며, LLM이 대규모로 생성된 데이터를 학습하지 않도록 하는 방법을 제안합니다. 이를 위해서는 **커뮤니티 차원의 협력이 필요**하며, 인터넷 상에서 크롤링된 데이터의 출처와 신뢰성을 관리할 수 있는 시스템이 필요합니다.

> 🤔 **서쿠 의견** : 공감되는 부분이 많은 논문이긴 하나, 실제로 거대한 언어모델(Ex. Gemini나, GPTs)에도 동일하게 작동할지 궁금하네요.
> 
> * 하지만, 저런 big tech company들이 하듯이 어느 정도의 데이터 정제가 필수적이라는 부분은 매우 공감하는 부분입니다.
> * 분석가 또는 사이언티스트들이 늘 하는 말, "Trash in Trash out"처럼 결국 학습에 사용되는 데이터 퀄리티가 좋아야 학습되는 모델 자체도 의미가 있는 것이니까요.

---

### AI produces gibberish when trained on too much AI-generated data

(번역) **AI가 AI 데이터를 학습할 때 발생하는 문제: 말이 되지 않는 결과물**

* 링크 : <https://universe-review.ca/garbage.pdf>

![](https://velog.velcdn.com/images/euisuk-chung/post/7646db2c-8c1a-47cc-9b9d-97b1dcb5b75d/image.png)

`두 번째 논문`, **"AI produces gibberish when trained on too much AI-generated data"**는 AI가 생성한 콘텐츠가 다시 AI 모델의 학습 데이터로 사용될 때, 이로 인해 발생하는 문제점과 그 영향에 대해 다룹니다.

* 특히 AI 생성 콘텐츠가 인터넷에 확산됨에 따라, 이러한 콘텐츠가 AI 모델의 학습 데이터로 포함될 경우 모델의 성능이 어떻게 저하될 수 있는지를 탐구합니다.

#### 1. AI 생성 콘텐츠의 증가와 그 문제점

* AI 생성 콘텐츠(예: 블로그, 이미지 등)가 인터넷 상에서 급격히 증가하고 있으며, 이는 점점 더 흔해지고 있습니다.
* 그러나 이 AI 생성 콘텐츠가 다시 AI 모델의 학습 데이터로 사용될 때, 그 모델에 심각한 영향을 미칠 수 있다는 연구 결과가 나왔습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/1b63961b-044f-46ba-b1fa-e008a17ca95b/image.png)

#### 2. AI 모델의 '모델 붕괴' 문제

AI 모델은 인터넷에 있는 방대한 양의 데이터를 학습하여 패턴을 추출하고, 이를 기반으로 현실과 유사한 콘텐츠를 생성합니다.

* 그러나 문제는 **이 데이터가 점차 AI가 생성한 데이터로 채워질 때 발생**합니다.
* 연구에 따르면 AI 모델이 자기 자신의 생성물로 학습할 경우, 그 모델이 점차 **붕괴**한다는 결과를 보였습니다. (아래 그림 참고)
  
  + 예를 들어, AI가 개 사진을 생성하는 모델을 생각해보면, 이 모델은 실제 이미지 데이터에서 자주 나타나는 개 품종(예: 골든 리트리버)을 더 많이 학습하게 됩니다.
  + 시간이 지나면 AI는 흔하지 않은 품종(예: 프렌치 불독, 달마시안 등)을 잊어버리고, 결국은 흔한 품종만 생성하게 됩니다. 이를 반복하면 모델은 더 이상 유효한 콘텐츠를 생성하지 못하게 됩니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/5b333ad2-5ee1-4359-9c5d-c1df2fb7df5d/image.png)

위 Figure1 그림은 **AI 모델이 스스로 생성한 데이터로 다시 학습할 때 발생하는 문제**, 즉 **모델 붕괴**에 대한 과정을 설명하고 있습니다. 다음은 각 패널의 설명입니다.

**a) 실제 이미지에 있는 개 품종들 기반 학습**

* 첫 번째 패널(a)에서는 실제 데이터에서 AI 모델이 학습하는 개 품종들이 나열되어 있습니다. 여기에는 **웰시코기**, **프티 바세 그리퐁 방댕**, **달마시안**, **프렌치 불독**, **골든 리트리버**가 포함되어 있습니다.
* 이 모델은 실제로 존재하는 이미지를 사용하여 각 품종의 특성을 학습합니다.

**b) AI가 실제 이미지로 학습했을 때 생성된 이미지**

* 두 번째 패널(b)에서는 AI 모델이 실제 이미지로 학습한 후 개 이미지를 생성한 결과를 보여줍니다.
* 이때 AI는 골든 리트리버와 같은 흔한 품종을 더 자주 생성하고, 덜 흔한 품종(예: 프렌치 불독, 프티 바세 그리퐁 방댕)은 상대적으로 적게 생성하게 됩니다.

**c) AI 생성 데이터를 다시 학습했을 때의 결과**

* 세 번째 패널(c)에서는 AI 모델이 자신이 생성한 이미지로 다시 학습한 결과를 보여줍니다.
* 이 경우, AI는 원래 학습한 데이터에서 자주 등장한 품종(예: 골든 리트리버)만을 더욱 많이 생성하고, 덜 흔한 품종들은 점점 잊혀집니다.
* 결과적으로 골든 리트리버와 같은 흔한 품종의 비율이 더욱 높아집니다.

**d) 모델 붕괴**

* 네 번째 패널(d)에서는 여러 번의 학습 주기를 거친 후 AI 모델이 **모델 붕괴**를 겪는 과정을 설명합니다.
* 반복적인 자기 학습을 통해 AI 모델은 결국 의미 없는 출력, 즉 실제로 존재하지 않는 왜곡된 이미지나 데이터만 생성하게 됩니다.

#### 3. 모델 붕괴의 공정성 문제

모델 붕괴는 단순히 AI 모델의 성능 저하 문제를 넘어서 **공정성**에도 심각한 영향을 미칠 수 있습니다. 구체적으로 설명하자면, AI 모델이 반복적으로 자기 자신의 데이터를 학습하게 될 경우, **덜 일반적인 데이터**나 **소수 그룹의 데이터**가 제대로 학습되지 않을 가능성이 커집니다.

* 예를 들어, 대다수의 사람들이 자주 사용하는 언어나 표현만을 반복적으로 학습하게 된다면, 소수 집단이 사용하는 특유의 언어나 문화적인 표현은 무시될 수 있습니다. 이로 인해 AI 모델은 소수 그룹의 언어나 관점을 잘 반영하지 못하게 되고, 이는 **사회적 공정성**에 문제를 일으킬 수 있습니다.
* 특히, AI 모델이 소수 그룹을 대변하는 중요한 개념이나 표현을 이해하거나 생성하지 못하게 될 수 있으며, 이는 AI가 생성하는 콘텐츠에서 **편향**이나 **차별**을 발생시킬 수 있습니다.
* 따라서, AI 모델이 데이터를 학습할 때 다양한 사회적 배경과 문화적 표현을 적절히 반영할 수 있도록 해야 한다는 문제가 제기됩니다.

#### 4. 문제 해결 방안

이러한 문제를 해결하기 위한 방법 중 하나로 **워터마크**의 사용이 제안됩니다.

* **워터마크**는 AI가 생성한 콘텐츠에 추가되는 **보이지 않는 신호**로, AI 모델이 이러한 신호를 탐지하여 AI 생성 콘텐츠를 구분할 수 있습니다. 이렇게 함으로써, AI 생성 콘텐츠가 학습 데이터에서 자동으로 제거되거나 최소한 표시될 수 있습니다.
* 예를 들어, 이미지나 텍스트 데이터에 워터마크를 삽입하여 AI가 이 데이터를 학습할 때 "이 데이터는 이미 AI가 생성한 것이다"라는 사실을 알 수 있도록 하는 방식입니다.

그러나, 워터마크 방식에도 한계가 존재합니다:

* 워터마크는 **쉽게 제거**될 수 있습니다. 악의적인 목적을 가진 사람들이 워터마크를 삭제하거나 변형함으로써 AI 생성 데이터를 다시 학습에 사용할 수 있게 만들 수 있습니다.
* 또한, **AI 기업 간의 협력**이 필요합니다. AI 모델을 개발하는 모든 기업이 동일한 워터마크 시스템을 사용하고 정보를 공유해야 하지만, 이는 상업적인 이익이나 경쟁 구도 때문에 현실적으로 어려울 수 있습니다. 한 기업이 자신의 워터마크 정보를 공유하지 않으면, 다른 기업의 AI 모델이 그 데이터를 학습하게 될 위험이 존재합니다.

#### 5. 모델 붕괴의 확장 가능성

이 연구는 주로 **텍스트 생성 모델**을 대상으로 했지만, 이 현상은 이미지 생성 모델, 음성 생성 모델, 멀티모달 모델(여러 형태의 데이터를 동시에 처리하는 모델) 등 **다양한 유형의 AI 모델**에서 동일하게 발생할 수 있다는 점이 중요합니다.

* 예를 들어, 이미지 생성 모델에서도 AI가 자신이 생성한 이미지를 반복해서 학습할 경우, 모델이 더 이상 새로운 이미지를 생성하지 못하고 왜곡된 이미지나 오류가 포함된 이미지를 생성할 수 있습니다.
* AI가 생성한 텍스트와 이미지를 동시에 학습하는 멀티모달 모델 역시 이러한 문제에 직면할 가능성이 큽니다.

따라서 텍스트 외의 **다른 종류의 AI 모델**에서 모델 붕괴 현상이 일어나는지, 그 원인은 무엇인지에 대한 추가 연구가 필요합니다.

#### 6. 최초 진입자의 이점

AI 모델의 붕괴 문제는 또한 **최초 진입자 이점**(first-mover advantage)을 시사합니다. 이는 초기 단계에서 인간이 만든 데이터를 기반으로 AI 모델을 구축한 기업들이 나중에 등장한 기업들에 비해 더 유리한 위치에 있을 수 있다는 것을 의미합니다.

* **초기 AI 모델**들은 인터넷에 존재하는 인간이 생성한 데이터를 기반으로 학습했기 때문에, 더 현실적이고 복잡한 세계를 반영할 수 있는 모델을 구축할 가능성이 큽니다.
* 반면, 시간이 지나면서 인터넷에 AI가 생성한 콘텐츠의 비중이 점점 더 커지면, 나중에 등장한 AI 모델들은 이러한 AI 생성 데이터를 기반으로 학습하게 되고, 이는 모델 붕괴의 위험을 높일 수 있습니다.

따라서, **초기 모델을 구축한 기업**들은 더 나은 품질의 AI 모델을 갖출 수 있으며, 이는 AI 산업에서 중요한 경쟁력이 될 수 있습니다. 이 현상은 AI 기술이 발전함에 따라 더 명확하게 드러날 가능성이 큽니다.

여러분들의 생각은?
----------

이 두 논문을 통해, AI 기술의 발전이 가져올 수 있는 위험에 대해 다시 한번 생각하게 됩니다. AI가 생성한 데이터를 반복적으로 학습하게 될 때 발생하는 문제는 단순한 성능 저하를 넘어서, AI 모델 자체가 현실을 왜곡하고, 다양한 목소리를 반영하지 못하게 만드는 심각한 결과를 초래할 수 있습니다.

저는 개인적으로 원본 데이터를 필터링하여 학습시키면 이러한 문제를 어느 정도 완화할 수 있다고 생각합니다. 예를 들어, 학습 데이터 중 일부는 항상 사람에 의해 생성된 데이터를 포함시키고, AI가 생성한 데이터를 적절히 관리한다면, 모델 붕괴를 피할 수 있을 것입니다. 실제로 연구에서도 일부 원본 데이터를 유지하면 모델 붕괴가 완화된다고 언급하고 있습니다.

그렇다면 여러분의 생각은 어떠신가요? AI 생성 콘텐츠가 AI 모델에 미치는 영향에 대해 어떻게 생각하시나요? 댓글로 여러분의 의견을 공유해 주세요! AI와 함께하는 미래를 어떻게 만들어 나가야 할지, 여러분의 소중한 의견을 기다립니다.

