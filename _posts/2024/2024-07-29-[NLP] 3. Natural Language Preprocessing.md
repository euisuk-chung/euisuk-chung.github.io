---
title: "[NLP] 3. Natural Language Preprocessing"
date: "2024-07-29"
tags:
  - "NLP"
  - "ê°•ì˜ë…¸íŠ¸"
year: "2024"
---

# [NLP] 3. Natural Language Preprocessing

ì›ë³¸ ê²Œì‹œê¸€: https://velog.io/@euisuk-chung/NLP-3.-ìì—°ì–´-ì „ì²˜ë¦¬-ê¸°



1. ìì—°ì–´ ì²˜ë¦¬(NLP) ê°œìš”
=================

**ìì—°ì–´ ì²˜ë¦¬ì˜ ì¼ë°˜ì ì¸ ìˆœì„œ**

* ìì—°ì–´ ì²˜ë¦¬ëŠ” ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ , í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ë° ì˜ë¯¸ë¥¼ ì¶”ì¶œí•œ ë’¤, ì´ë¥¼ ë‹¤ì‹œ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤. (*ì•„ë˜ ê·¸ë¦¼ ì°¸ê³ *)
* ì´ ê³¼ì •ì€ í¬ê²Œ STT(Speech to Text)ì™€ TTS(Text to Speech)ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/faa02188-ecb4-4af7-b464-b6dbe8b2a686/image.png)

1.1 ìì—°ì–´ ì²˜ë¦¬ì˜ ì£¼ìš” ë¶„ì•¼
-----------------

* ìŒìš´ë¡ , í˜•íƒœë¡ , êµ¬ë¬¸ë¡ , ì˜ë¯¸ë¡ , í™”ìš©ë¡ , ë‹´ë¡ ê³¼ ê°™ì€ Classical Categorizationì€ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì—°êµ¬í•˜ê³  ì‘ìš©í•˜ëŠ” ë° ìˆì–´ì„œ ì–¸ì–´ì˜ ê° ì¸¡ë©´ì„ ë¶„ì„í•˜ê³  ì´í•´í•˜ëŠ” ë‹¤ì–‘í•œ ìˆ˜ì¤€ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•œ ê²ƒì…ë‹ˆë‹¤.
* ì´ëŸ¬í•œ ë¶„ë¥˜ëŠ” ê°ê°ì˜ ì–¸ì–´ì  í˜„ìƒì„ ê°œë³„ì ìœ¼ë¡œ ë‹¤ë£¨ê³ , ê° í˜„ìƒì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ëŠ”ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³µì¡í•œ ìì—°ì–´ë¥¼ ë‹¤ì–‘í•œ ì¸µìœ„ì—ì„œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/ac9ee907-7324-45e6-a7b0-7119079c5443/image.png)

1. **ìŒìš´ë¡ (Phonology)**:
   
   * **ê¸°ì¤€**: ì†Œë¦¬ì™€ ê´€ë ¨ëœ ì–¸ì–´ì  í˜„ìƒ
   * **ì„¤ëª…**: ìŒìš´ë¡ ì€ ì–¸ì–´ì˜ ì†Œë¦¬ ì²´ê³„ì™€ ìŒì†Œ(phoneme)ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê±°ë‚˜, ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œì„ ê°œë°œí•  ë•Œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
2. **í˜•íƒœë¡ (Morphology)**:
   
   * **ê¸°ì¤€**: ë‹¨ì–´ì˜ êµ¬ì¡°ì™€ í˜•ì„±
   * **ì„¤ëª…**: í˜•íƒœë¡ ì€ ë‹¨ì–´ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì—°êµ¬í•˜ë©°, ë‹¨ì–´ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(morpheme)ë¡œ ë¶„í•´í•˜ëŠ” ì‘ì—…ì„ í¬í•¨í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„(morphological analysis) ë° í˜•íƒœì†Œ ê¸°ë°˜ í† í°í™”(tokenization)ì— í™œìš©ë©ë‹ˆë‹¤.
3. **êµ¬ë¬¸ë¡ (Syntax)**:
   
   * **ê¸°ì¤€**: ë¬¸ì¥ êµ¬ì¡°ì™€ ë¬¸ë²• ê·œì¹™
   * **ì„¤ëª…**: êµ¬ë¬¸ë¡ ì€ ë‹¨ì–´ë“¤ì´ ê²°í•©ë˜ì–´ ë¬¸ì¥ì„ ì´ë£¨ëŠ” ê·œì¹™ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ë¬¸ì¥ êµ¬ë¬¸ ë¶„ì„(parsing)ì„ í†µí•´ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
4. **ì˜ë¯¸ë¡ (Semantics)**:
   
   * **ê¸°ì¤€**: ì˜ë¯¸ì™€ í•´ì„
   * **ì„¤ëª…**: ì˜ë¯¸ë¡ ì€ ë‹¨ì–´, êµ¬, ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ê³ , ë¬¸ë§¥ì„ ì´í•´í•˜ë©°, ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì— í™œìš©ë©ë‹ˆë‹¤.
5. **í™”ìš©ë¡ (Pragmatics)**:
   
   * **ê¸°ì¤€**: ì–¸ì–´ ì‚¬ìš©ì˜ ë§¥ë½ê³¼ ëª©ì 
   * **ì„¤ëª…**: í™”ìš©ë¡ ì€ ì–¸ì–´ê°€ ì‹¤ì œ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ëŒ€í™” ì‹œìŠ¤í…œ, ì±—ë´‡ ë“±ì—ì„œ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.
6. **ë‹´ë¡ (Discourse)**:
   
   * **ê¸°ì¤€**: ë¬¸ì¥ ê°„ì˜ ê´€ê³„ì™€ ë§¥ë½
   * **ì„¤ëª…**: ë‹´ë¡ ì€ ë¬¸ì¥ë“¤ì´ ê²°í•©ë˜ì–´ ë” í° í…ìŠ¤íŠ¸ë‚˜ ëŒ€í™”ë¥¼ í˜•ì„±í•˜ëŠ” ë°©ì‹ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” í…ìŠ¤íŠ¸ ìš”ì•½, ë¬¸ì„œ ë¶„ë¥˜, ëŒ€í™”ì˜ ì¼ê´€ì„± ìœ ì§€ ë“±ì— ì ìš©ë©ë‹ˆë‹¤.

1.2 ìš°ë¦¬ëŠ” ì§€ê¸ˆ ì–´ëŠ ë‹¨ê³„ì¸ê°€?
-------------------

* í˜„ì¬ ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ë°œì „ ìˆ˜ì¤€ì„ í‰ê°€í•  ë•Œ, ìŒìš´ë¡ , í˜•íƒœë¡ , êµ¬ë¬¸ë¡ , ì˜ë¯¸ë¡ , í™”ìš©ë¡ , ë‹´ë¡ ì˜ 6ê°€ì§€ ì˜ì—­ì—ì„œ ê°ê°ì˜ ì„±ê³¼ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ê°ê°ì˜ ì˜ì—­ì—ì„œ í˜„ì¬ ê¸°ìˆ ì´ ì–´ëŠ ì •ë„ ì„±ìˆ™ë˜ì—ˆëŠ”ì§€ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
  
  **1. ìŒìš´ë¡  (Phonology)**
  
  + **ìŒì„± ì¸ì‹(Speech Recognition)**: ë§¤ìš° ë†’ì€ ì •í™•ë„ë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.
  + **í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜(Text-to-Speech)**: ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±ì„ ìƒì„±í•˜ëŠ” TTS ê¸°ìˆ ë„ ìƒë‹¹íˆ ë°œì „í–ˆìŠµë‹ˆë‹¤. DeepMindì˜ WaveNetê³¼ ê°™ì€ ëª¨ë¸ì´ ê·¸ ì˜ˆì…ë‹ˆë‹¤.
  + **ìˆ˜ì¤€**: ë§¤ìš° ë°œì „ë˜ì–´ ì‹¤ìš©í™” ë‹¨ê³„.
  + **ìƒìš©í™” ì˜ˆì‹œ**: Siri, Google Assistant, Amazon Alexa ë“±ì˜ ìŒì„± ë¹„ì„œ ê¸°ëŠ¥

![](https://velog.velcdn.com/images/euisuk-chung/post/529b5473-ebd8-4b3c-87f8-e7d904706217/image.png)

**2. í˜•íƒœë¡  (Morphology)**

* **í˜•íƒœì†Œ ë¶„ì„(Morphological Analysis)**: í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ í† í¬ë‚˜ì´ì €(tokenizer)ê°€ ë‹¤ìˆ˜ ê°œë°œë˜ì—ˆìœ¼ë©°, í•œêµ­ì–´, ì¼ë³¸ì–´, í•€ë€ë“œì–´ ë“± í˜•íƒœê°€ ë³µì¡í•œ ì–¸ì–´ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
* **ìˆ˜ì¤€**: ì„±ìˆ™ ë‹¨ê³„ë¡œ, ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì„.
* **ìƒìš©í™” ì˜ˆì‹œ**: Grammarly, Microsoft, í•œì»´ ë“±ì˜ ë§ì¶¤ë²• ê²€ì‚¬ ê¸°ëŠ¥

![](https://velog.velcdn.com/images/euisuk-chung/post/61e83dc8-b226-4ed0-85da-9ae580cf46a0/image.png)

**3. êµ¬ë¬¸ë¡  (Syntax)**

* **êµ¬ë¬¸ ë¶„ì„(Parsing)**: êµ¬ë¬¸ ë¶„ì„ê¸°ëŠ” CFG, PCFG, Dependency Parsing ë“± ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë°œì „ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” BERT, GPT ë“± Transformer ê¸°ë°˜ ëª¨ë¸ë“¤ì´ êµ¬ë¬¸ ë¶„ì„ì—ì„œë„ ì¢‹ì€ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
* **ìˆ˜ì¤€**: ì‹¤ìš©í™” ë‹¨ê³„ë¡œ, ë§ì€ NLP ì‘ìš©ì—ì„œ í™œìš© ê°€ëŠ¥.
* **ì‚¬ìš©í™” ì˜ˆì‹œ**: Stanford Parser, spaCy, NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬

> ğŸ¤”**Stanford Parser, spaCy, NLTK?**
> 
> * `Stanford Parser`:
>   1. Stanford ëŒ€í•™ì—ì„œ ê°œë°œí•œ ìì—°ì–´ êµ¬ë¬¸ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
>   2. ë‹¤ì–‘í•œ ì–¸ì–´(ì˜ì–´, ì¤‘êµ­ì–´, ì•„ëì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´, ìŠ¤í˜ì¸ì–´ ë“±)ì— ëŒ€í•œ êµ¬ë¬¸ ë¶„ì„ì„ ì§€ì›í•©ë‹ˆë‹¤.
>   3. êµ¬ì„±ì„±ë¶„ ë¶„ì„(constituency parsing)ê³¼ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„(dependency parsing)ì„ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤.
>   4. PCFG(Probabilistic Context-Free Grammar), Shift-Reduce, Neural Network ê¸°ë°˜ì˜ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ê¸°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
>   5. ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë˜ë©°, ë¹„ìƒì—…ì  ìš©ë„ë¡œëŠ” ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> * `spaCy`:
>   1. ì‚°ì—…ìš© ê°•ë„ì˜ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
>   2. í† í°í™”, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹, ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ ë“± ë‹¤ì–‘í•œ NLP ì‘ì—…ì„ ì§€ì›í•©ë‹ˆë‹¤.
>   3. ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì™€ ë†’ì€ ì •í™•ë„ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.
>   4. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì œê³µí•˜ì—¬ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>   5. ë‹¤êµ­ì–´ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤.
> * `NLTK (Natural Language Toolkit)`:  
>   
>   NLTKëŠ” êµìœ¡ê³¼ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ NLP ì‘ì—…ì„ ì§€ì›í•©ë‹ˆë‹¤.
>   1. í† í°í™”: ë¬¸ì¥ê³¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
>   2. í’ˆì‚¬ íƒœê¹…: ê° ë‹¨ì–´ì— ëŒ€í•´ í’ˆì‚¬ ì •ë³´ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
>   3. êµ¬ë¬¸ ë¶„ì„: êµ¬ì„±ì„±ë¶„ ë¶„ì„ê³¼ ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ì„ ì§€ì›í•©ë‹ˆë‹¤.
>   4. ì–´íœ˜ ìì›: WordNetê³¼ ê°™ì€ ì–´íœ˜ ìì›ì„ í¬í•¨í•˜ì—¬ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
>   5. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ë¶ˆìš©ì–´ ì œê±°, ì–´ê°„ ì¶”ì¶œ, í‘œì œì–´ ì¶”ì¶œ ë“± ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì‘ì—…ì„ ì§€ì›í•©ë‹ˆë‹¤.

**4. ì˜ë¯¸ë¡  (Semantics)**

* **ì˜ë¯¸ ë¶„ì„(Semantic Analysis)**: ë‹¨ì–´ ì„ë² ë”©(word embedding), ë¬¸ì¥ ì„ë² ë”© ë“±ì˜ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. BERT, GPT-3 ê°™ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë“¤ì´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
* **ìˆ˜ì¤€**: ë§¤ìš° ë°œì „ëœ ë‹¨ê³„ë¡œ, ì‹¤ìƒí™œ ì‘ìš©ì—ì„œ í™œìš© ì¤‘.
* **ì‚¬ìš©í™” ì˜ˆì‹œ**: Word2Vec, BERT, GPT ë“±ì˜ ì–¸ì–´ ëª¨ë¸

![](https://velog.velcdn.com/images/euisuk-chung/post/f4ff5030-8da4-464f-88f4-a4caf5e9e3e1/image.png)

**5. í™”ìš©ë¡  (Pragmatics)**

* **ëŒ€í™” ì‹œìŠ¤í…œ(Dialogue Systems)**: ì‚¬ìš©ì ì˜ë„ë¥¼ ì´í•´í•˜ê³  ì ì ˆíˆ ì‘ë‹µí•˜ëŠ” ì±—ë´‡ê³¼ ëŒ€í™” ì—ì´ì „íŠ¸ê°€ í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ìƒìš© ì œí’ˆë“¤ë„ ë“±ì¥í–ˆì§€ë§Œ, ì•„ì§ ì™„ë²½í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
* **ë§¥ë½ ì´í•´(Context Understanding)**: ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ì–¸ì–´ ì‚¬ìš©ì„ ì´í•´í•˜ëŠ” ë° ìˆì–´ ì œí•œì ì¸ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
* **ìˆ˜ì¤€**: ë°œì „ ì¤‘, ìƒë‹¹í•œ ê°œì„  ì—¬ì§€ ìˆìŒ
* **ì‚¬ìš©í™” ì˜ˆì‹œ**: ì±—ë´‡, ê³ ê° ì„œë¹„ìŠ¤ ìë™í™” ì‹œìŠ¤í…œ

![](https://velog.velcdn.com/images/euisuk-chung/post/4431958e-a8e9-43a9-a1ab-26d6a363162a/image.png)

**6. ë‹´ë¡  (Discourse)**

* **í…ìŠ¤íŠ¸ ì¼ê´€ì„±(Text Coherence)**: ê¸´ ë¬¸ë§¥ì„ ìœ ì§€í•˜ê³  ì¼ê´€ì„± ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. GPT-3ì™€ ê°™ì€ ëª¨ë¸ë“¤ì´ ê¸´ ë¬¸ì„œ ìƒì„±ì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
* **ë‹´ë¡  ë¶„ì„(Discourse Analysis)**: ë¬¸ì¥ ê°„ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ê³ , ì¼ê´€ëœ ëŒ€í™”ë¥¼ ìœ ì§€í•˜ëŠ” ë°ì—ëŠ” ì•„ì§ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
* **ìˆ˜ì¤€**: ì´ˆê¸° ë‹¨ê³„, ë§ì€ ê°œì„  í•„ìš”
* **ì‚¬ìš©í™” ì˜ˆì‹œ**: ìë™ ìš”ì•½ ë„êµ¬, ëŒ€í™”í˜• AI ì‹œìŠ¤í…œ

![](https://velog.velcdn.com/images/euisuk-chung/post/c24f5930-2fe0-4b54-bb7c-bc35c01d1fe1/image.png)

> ğŸ” **ì±—ë´‡ vs ëŒ€í™”í˜• ì±—ë´‡, ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€?**  
> 
> ì±—ë´‡ê³¼ ëŒ€í™”í˜• AIì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë‘ ê¸°ìˆ ì„ ë¹„êµí•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‘ ê¸°ìˆ ì˜ ì£¼ìš” ì°¨ì´ì ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

> **ì±—ë´‡ (Chatbots) ğŸ—£ï¸**
> 
> * **ê¸°ë°˜ ê¸°ìˆ **: ê·œì¹™ ê¸°ë°˜(rule-based) ë˜ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ìŠ¤í¬ë¦½íŠ¸(predefined scripts)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì§ˆì˜ë‚˜ ëª…ë ¹ì— ì‘ë‹µí•©ë‹ˆë‹¤.
> * **ì´í•´ ëŠ¥ë ¥**: ìì—°ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì´ ì œí•œì ì´ë©°, ë³µì¡í•œ ëŒ€í™”ë¥¼ ì˜ ì²˜ë¦¬í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
> * **ì‘ë‹µ ë°©ì‹**: íŠ¹ì • ì…ë ¥ì— ëŒ€í•´ ë¯¸ë¦¬ ì •ì˜ëœ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.
> * **ì ìš© ë¶„ì•¼**: ì£¼ë¡œ ê³ ê° ì§€ì›, FAQ, ê°„ë‹¨í•œ ì •ë³´ ì œê³µ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

> **ëŒ€í™”í˜• AI (Conversational AI) ğŸ™ï¸**
> 
> * **ê¸°ë°˜ ê¸°ìˆ **: ìì—°ì–´ ì²˜ë¦¬(NLP), ë¨¸ì‹  ëŸ¬ë‹(ML), ì¸ê³µì§€ëŠ¥(AI) ë“±ì˜ ë‹¤ì–‘í•œ ê¸°ìˆ ì„ í¬í•¨í•˜ëŠ” ë„“ì€ ë²”ìœ„ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ê³„ê°€ ìì—°ì–´ë¡œ ì¸ê°„ê³¼ ê°™ì€ ì‘ë‹µì„ ì´í•´í•˜ê³ , ì²˜ë¦¬í•˜ë©°, ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> * **ì´í•´ ëŠ¥ë ¥**: ë” ë°œì „ëœ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥, ê°ì •, ì–¸ì–´ì˜ ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> * **ì‘ë‹µ ë°©ì‹**: ìƒí˜¸ì‘ìš©ì—ì„œ í•™ìŠµí•˜ê³ , ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ ì ì‘í•˜ë©°, ë” ë³µì¡í•˜ê³  ë™ì ì¸ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> * **ì ìš© ë¶„ì•¼**: ê³ ê° ì§€ì›, ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ìë™í™”, ê°œì¸ ë¹„ì„œ, ë³µì¡í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/2d727c7b-a608-4ff4-ac93-4b8527058204/image.png)

* í˜„ì¬ NLP ê¸°ìˆ ì˜ ë°œì „ ë‹¨ê³„ëŠ” `4~5 ì˜ì—­`ì— ê±¸ì³ ìˆìœ¼ë©°, ê° ì˜ì—­ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ë³´ì…ë‹ˆë‹¤:
  
  1. `ì˜ë¯¸ë¡  (Semantics)` ì˜ì—­ì—ì„œëŠ” ìƒë‹¹í•œ ì§„ì „ì„ ì´ë£¨ì–´ ì‹¤ìš©ì ì¸ ìˆ˜ì¤€ì— ë„ë‹¬í–ˆì§€ë§Œ, ì—¬ì „íˆ ì¸ê°„ ìˆ˜ì¤€ì˜ ì´í•´ì™€ í•´ì„ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•©ë‹ˆë‹¤.
     
     + **ê³¼ì œ**: ë³µì¡í•œ ì¶”ë¡ , ìƒì‹ì  ì´í•´, ì€ìœ  í•´ì„ ë“±ì—ì„œ ì—¬ì „íˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.
  2. `í™”ìš©ë¡  (Pragmatics)` ì˜ì—­ì€ ì˜ë¯¸ë¡ ë³´ë‹¤ëŠ” ëœ ë°œì „í–ˆì§€ë§Œ, ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì ì°¨ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëŒ€í™” ì‹œìŠ¤í…œê³¼ ê°ì • ë¶„ì„ ë“±ì—ì„œ ìœ ìš©ì„±ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
     
     + **ê³¼ì œ**: í™”ìì˜ ì˜ë„, ì•”ì‹œì  ì˜ë¯¸, ì‚¬íšŒë¬¸í™”ì  ë§¥ë½ ì´í•´ ë“±ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì´í•´ë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
  3. `ë‹´ë¡  (Discourse)` ì˜ì—­ì€ ê°€ì¥ ë³µì¡í•˜ê³  ë„ì „ì ì¸ ì˜ì—­ìœ¼ë¡œ, í˜„ì¬ ì´ˆê¸° ì—°êµ¬ ë‹¨ê³„ì— ìˆìŠµë‹ˆë‹¤. ì´ ì˜ì—­ì˜ ë°œì „ì€ í–¥í›„ NLP ê¸°ìˆ ì˜ í° ë„ì•½ì„ ê°€ì ¸ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
     
     + **ê³¼ì œ**: ê¸´ ë¬¸ë§¥ì„ ìœ ì§€í•˜ë©´ì„œ ì¼ê´€ì„± ìˆëŠ” ëŒ€í™”ë¥¼ ì´ì–´ê°€ëŠ” ëŠ¥ë ¥, ë³µì¡í•œ ë…¼ë¦¬ êµ¬ì¡° ì´í•´ ë“±ì´ ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤.

ì „ë°˜ì ìœ¼ë¡œ, NLP ê¸°ìˆ ì€ ì´ ì„¸ ì˜ì—­ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆì§€ë§Œ, ì¸ê°„ ìˆ˜ì¤€ì˜ ì–¸ì–´ ì´í•´ì™€ ìƒì„±ì„ ìœ„í•´ì„œëŠ” ì•„ì§ ë§ì€ ì—°êµ¬ì™€ í˜ì‹ ì´ í•„ìš”í•œ ìƒíƒœì…ë‹ˆë‹¤. (=> ì§€ê¸ˆë„ í™œë°œí•˜ê²Œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤)

2. NLPì˜ ì–´ë ¤ì›€ê³¼ ì—­ì‚¬
===============

* **ì™œ NLPê°€ ì–´ë ¤ìš´ê°€?** : ìì—°ì–´ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì™€ ë‹¬ë¦¬ ë°©ëŒ€í•œ ë‹¨ì–´ì˜ ì–‘, ë³µì¡í•œ êµ¬ë¬¸, ëª¨í˜¸ì„± ë“±ìœ¼ë¡œ ì¸í•´ ë‹¤ë£¨ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‚¬ìš©ë˜ëŠ” ë°©ëŒ€í•œ ë‹¨ì–´ì˜ ì–‘ ë¿ë§Œ ì•„ë‹ˆë¼, êµ¬ë¬¸ì˜ ë³µì¡ì„± ë° ëª¨í˜¸ì„±, ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì§„í™”í•˜ê¸° ë•Œë¬¸ì— ì´ˆê¸° NLPì—°êµ¬ì— ì–´ë ¤ì›€ì´ ì¡´ì¬í–ˆìŠµë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/f1814bab-1847-4007-ba7a-bd5b99281902/image.png)

* **NLP ì—°êµ¬ì˜ ì—­ì‚¬** : ì´ˆê¸°ì—ëŠ” ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ì£¼ë¡œ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ, ìì—°ì–´ì˜ ë™ì  íŠ¹ì„±ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ëŠ” ë‹¨ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” í†µê³„ì  ë°©ë²•ê³¼ ê·œì¹™ ê¸°ë°˜ ë°©ë²•ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ë©°, ë¨¸ì‹  ëŸ¬ë‹ ë° ë”¥ ëŸ¬ë‹ ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ì£¼ë¥˜ë¥¼ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/36529fb4-7c92-4e73-95ae-eeaa3bace814/image.png)

3. Lexical Analysis
===================

3.1. í† í°í™” (Tokenization)
-----------------------

í† í°í™”ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ìœ„ëŠ” ë‹¨ì–´, êµ¬, ì‹¬ì§€ì–´ëŠ” ê°œë³„ ë¬¸ìì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "Hello, world!"ë¼ëŠ” ë¬¸ì¥ì€ ["Hello", ",", "world", "!"]ë¡œ ë¶„í• ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í°í™”ëŠ” í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ê° ë‹¨ì–´ê°€ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/b3fd625a-113e-4481-8368-94dfebcca96e/image.png)

í† í°í™”ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:

* **ê³µë°± ê¸°ë°˜ í† í°í™”**: ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
* **êµ¬ë‘ì  ê¸°ë°˜ í† í°í™”**: êµ¬ë‘ì ë„ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
* **ì–´ì ˆ ê¸°ë°˜ í† í°í™”**: ì–¸ì–´ì  ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. (í•œêµ­ì–´ì™€ ê°™ì€ ì–¸ì–´ì—ì„œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤)

**ì½”ë“œ ì˜ˆì‹œ**

```
from nltk.tokenize import word_tokenize

sentence = "Hello, world! Welcome to NLP."
tokens = word_tokenize(sentence)
print(tokens)
```

ì¶œë ¥: `['Hello', ',', 'world', '!', 'Welcome', 'to', 'NLP', '.']`

3.2. í˜•íƒœì†Œ ë¶„ì„ (Morphological Analysis)
------------------------------------

í˜•íƒœì†Œ ë¶„ì„ì€ ë‹¨ì–´ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•˜ëŠ” í† í°í™” ë‹¨ê³„ë³´ë‹¤ ë” ê¹Šì€ ë¶„ì„ì„ í¬í•¨í•©ë‹ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ì€ ì£¼ë¡œ ë‘ ê°€ì§€ ì£¼ìš” ì‘ì—…ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:

![](https://velog.velcdn.com/images/euisuk-chung/post/d82e5945-65b3-4847-9433-55a466761982/image.png)

* **í˜•íƒœì†Œ(Morpheme) ì‹ë³„**: ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ìµœì†Œ ì˜ë¯¸ ë‹¨ìœ„ì¸ í˜•íƒœì†Œë¥¼ ì‹ë³„í•©ë‹ˆë‹¤. í˜•íƒœì†ŒëŠ” ìë¦½ í˜•íƒœì†Œ(ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ë‹¨ì–´)ì™€ ì˜ì¡´ í˜•íƒœì†Œ(ë‹¤ë¥¸ í˜•íƒœì†Œì™€ ê²°í•©ë˜ì–´ì•¼ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë‹¨ì–´)ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.
  
  + ì˜ˆì‹œ: "unhappiness"ëŠ” "un-", "happy", "-ness"ì˜ ì„¸ ê°€ì§€ í˜•íƒœì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
* **í˜•íƒœì†Œ ë³€í˜•(Morphological Inflection)**: ë‹¨ì–´ê°€ ë¬¸ë²•ì  í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ë³€í˜•ë˜ëŠ” ê³¼ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤:
  
  + **Stemming**: ë‹¨ì–´ì˜ ì–´ê°„(Stem)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ì–´ì˜ ë³€í˜•ëœ í˜•íƒœë“¤ì„ ë™ì¼í•˜ê²Œ ì¸ì‹í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "running", "runner", "ran"ì€ ëª¨ë‘ "run"ìœ¼ë¡œ ë³€í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Stemmingì€ ê·œì¹™ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ë©°, ë‹¨ì–´ì˜ ì¼ë¶€ë¶„ì„ ì œê±°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.
    - **ì˜ˆì‹œ**: Porter Stemmer, Snowball Stemmer
  + **Lemmatization**: ë‹¨ì–´ì˜ ê¸°ë³¸ í˜•íƒœ(Lemma)ë¥¼ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ì–´íœ˜ í˜•íƒœì†Œ ë¶„ì„ì„ í¬í•¨í•˜ì—¬ ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "running"ì€ "run"ìœ¼ë¡œ, "better"ëŠ” "good"ìœ¼ë¡œ ë³€í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Lemmatizationì€ ì–¸ì–´ì  ì§€ì‹(ì‚¬ì „)ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.
    - **ì˜ˆì‹œ**: WordNet Lemmatizer

**ì½”ë“œ ì˜ˆì‹œ**

```
from nltk.stem import PorterStemmer, WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

word1 = "running"
word2 = "better"

print(stemmer.stem(word1))   # ì¶œë ¥: run
print(lemmatizer.lemmatize(word1, pos='v'))  # ì¶œë ¥: run
print(lemmatizer.lemmatize(word2, pos='a'))  # ì¶œë ¥: good
```

ì¶œë ¥:

```
run
run
good
```

3.3. ë¬¸ì¥ ë¶„í•  (Sentence Splitting)
-------------------------------

ë¬¸ì¥ ë¶„í• ì€ í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” êµ¬ë‘ì ì´ë‚˜ ë¬¸ì¥ì˜ êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ë¬¸ì¥ ë¶„í• ì€ í…ìŠ¤íŠ¸ ë¶„ì„ì˜ ë‹¨ìœ„ë¥¼ ëª…í™•íˆ í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/570d290e-0299-4794-a0ca-e5087d3a35a5/image.png)

**ì½”ë“œ ì˜ˆì‹œ**

```
from nltk.tokenize import sent_tokenize

text = "Hello world! How are you doing? NLP is interesting."
sentences = sent_tokenize(text)
print(sentences)
```

ì¶œë ¥: `['Hello world!', 'How are you doing?', 'NLP is interesting.']`

3.4. í’ˆì‚¬ íƒœê¹… (Part-of-Speech Tagging)
-----------------------------------

í’ˆì‚¬ íƒœê¹…ì€ ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” í’ˆì‚¬ë¥¼ í• ë‹¹í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì¥ì˜ êµ¬ë¬¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/366b0c4b-8238-4a86-95b4-34fd1a9ff81b/image.png)

í’ˆì‚¬ íƒœê¹…ì€ ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

![](https://velog.velcdn.com/images/euisuk-chung/post/48d6f99d-94d7-44ef-a277-f10ce5b3d109/image.png)

* **ê·œì¹™ ê¸°ë°˜ íƒœê¹…(Rule-Based Tagging)**: ì •í•´ì§„ ë¬¸ë²• ê·œì¹™ì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
* **í†µê³„ì  íƒœê¹…(Statistical Tagging)**: ì½”í¼ìŠ¤ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í’ˆì‚¬ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤. HMM(Hidden Markov Model)ì´ë‚˜ Maximum Entropy ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.
* **ê¸°ê³„ í•™ìŠµ ê¸°ë°˜ íƒœê¹…(Machine Learning-Based Tagging)**: ì§€ë„ í•™ìŠµì„ í†µí•´ í’ˆì‚¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. SVM, Decision Tree, ê·¸ë¦¬ê³  ìµœê·¼ì—ëŠ” ì‹ ê²½ë§ ëª¨ë¸(Neural Networks)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  + **ì˜ˆì‹œ**: CRF(Conditional Random Fields), Bi-LSTM

**ì½”ë“œ ì˜ˆì‹œ**

```
import nltk
nltk.download('averaged_perceptron_tagger')

sentence = "I love eating chicken."
tokens = word_tokenize(sentence)
pos_tags = nltk.pos_tag(tokens)
print(pos_tags)
```

ì¶œë ¥: `[('I', 'PRP'), ('love', 'VBP'), ('eating', 'VBG'), ('chicken', 'NN')]`

4. Named Entity Recognition (NER)
=================================

* **ê°ì²´ëª… ì¸ì‹ (Named Entity Recognition, NER)** : ê°ì²´ëª… ì¸ì‹ì€ ë¬¸ì¥ì—ì„œ íŠ¹ì • ìš”ì†Œë¥¼ ë¯¸ë¦¬ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬(ì˜ˆ: ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§, ë‚ ì§œ ë“±)ë¡œ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/753fcf77-d269-478f-ab65-77191931dd45/image.png)

**ì ‘ê·¼ ë°©ë²•**  

1. **ì‚¬ì „ ê¸°ë°˜ ì ‘ê·¼ë²• (Dictionary / Rule-based Approach)**

* **List Lookup**: ì‚¬ì „ì— ì •ì˜ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
  + **ì¥ì **: ê°„ë‹¨í•˜ê³  ë¹ ë¥´ë©°, íŠ¹ì • ì–¸ì–´ì— íŠ¹í™”ëœ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  + **ë‹¨ì **: ë¦¬ìŠ¤íŠ¸ì˜ í¬ê¸°ì™€ ìœ ì§€ ê´€ë¦¬ê°€ ì–´ë µê³ , ìƒˆë¡œìš´ ë‹¨ì–´ë‚˜ ë³€í™”ì— ë¯¼ê°í•©ë‹ˆë‹¤.
* **Shallow Parsing Approach**: ê·¼ê±° ìˆëŠ” ì¦ê±°(evidence)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
  + **ì˜ˆì‹œ**: "Wall Street"ì—ì„œ "Street"ê³¼ ê°™ì€ ë‹¨ì–´ë¥¼ í†µí•´ ì§€ëª…ì„ ì¸ì‹í•©ë‹ˆë‹¤.

2. **ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ë²• (Model-based Approach)**
   * **CRF (Conditional Random Fields)**: ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ì–´ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
   * **ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ (Neural Network-based Models)**: RNN, CNN ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
     + **ì¥ì **: ë¬¸ë§¥ì„ ì˜ ë°˜ì˜í•˜ì—¬ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.
     + **ë‹¨ì **: ë§ì€ í•™ìŠµ ë°ì´í„°ì™€ ê³„ì‚° ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ì½”ë“œ ì˜ˆì‹œ**

```
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Barack Obama was born in Hawaii.")

for ent in doc.ents:
    print(ent.text, ent.label_)
```

ì¶œë ¥: `Barack Obama PERSON`, `Hawaii GPE`

5. Syntax Analysis (êµ¬ë¬¸ ë¶„ì„)
==========================

**êµ¬ë¬¸ ë¶„ì„ (Syntax Analysis)** : êµ¬ë¬¸ ë¶„ì„ì€ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ì´ ì–´ë–»ê²Œ êµ¬ì¡°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì¥ì˜ ë¬¸ë²•ì  êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ê° ë‹¨ì–´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.

5.1. Parsing
------------

![](https://velog.velcdn.com/images/euisuk-chung/post/b34bfbf7-a6b6-4659-8a4b-f599bfe4aa03/image.png)

* **Top-down Parsing**: ë¬¸ì¥ì˜ ì‹œì‘ë¶€í„° ë¬¸ë²• ê·œì¹™ì„ ì ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
* **Bottom-up Parsing**: ë‹¨ì–´ë¶€í„° ì‹œì‘í•˜ì—¬ ë¬¸ë²• ê·œì¹™ì„ ì—­ìœ¼ë¡œ ì ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

5.2. ëª¨í˜¸ì„± (Ambiguity)
--------------------

![](https://velog.velcdn.com/images/euisuk-chung/post/6c9870fc-15ff-4038-882a-77971d084144/image.png)

êµ¬ë¬¸ ë¶„ì„ì—ì„œ ì¤‘ìš”í•œ ë¬¸ì œëŠ” ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

* **êµ¬ì¡°ì˜ ëª¨í˜¸ì„± (Structural Ambiguity)**: ê°™ì€ ë¬¸ì¥ì´ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  + ì˜ˆì‹œ: "John saw Mary in the park." (ì¡´ì´ ê³µì›ì—ì„œ ë©”ë¦¬ë¥¼ ë´¤ë‹¤ / ì¡´ì´ ë©”ë¦¬ë¥¼ ë´¤ëŠ”ë°, ë©”ë¦¬ëŠ” ê³µì›ì— ìˆë‹¤)
* **ì–´íœ˜ì˜ ëª¨í˜¸ì„± (Lexical Ambiguity)**: ê°™ì€ ë‹¨ì–´ê°€ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ í˜•íƒœì†Œë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  + ì˜ˆì‹œ: "Time flies like an arrow." (ì‹œê°„ì€ í™”ì‚´ì²˜ëŸ¼ ë‚ ì•„ê°„ë‹¤ / íŒŒë¦¬ëŠ” í™”ì‚´ì„ ì¢‹ì•„í•œë‹¤)

**ì½”ë“œ ì˜ˆì‹œ**

```
import nltk

sentence = "John saw the man with a telescope."
grammar = nltk.CFG.fromstring("""
  S -> NP VP
  VP -> V NP | VP PP
  PP -> P NP
  V -> "saw"
  NP -> "John" | "man" | "telescope" | Det N | NP PP
  Det -> "a" | "the"
  N -> "man" | "telescope"
  P -> "with"
""")
parser = nltk.ChartParser(grammar)
for tree in parser.parse(sentence.split()):
    print(tree)
```

6. ì–¸ì–´ ëª¨ë¸ë§ (Language Modeling)
=============================

**í™•ë¥ ì  ì–¸ì–´ ëª¨ë¸ (Probabilistic Language Modeling)** : ì–¸ì–´ ëª¨ë¸ë§ì€ ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì‹¤ì œ ë¬¸ë²•ì ìœ¼ë¡œ íƒ€ë‹¹í•œì§€ í‰ê°€í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì¥ì˜ ë¬¸ë²•ì  êµ¬ì¡°ì™€ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/euisuk-chung/post/b7dc68e4-87f5-478e-b2eb-5585adf187f6/image.png)

**ì ‘ê·¼ ë°©ë²•**  

1. **N-ê·¸ë¨ ëª¨ë¸ (N-gram Models)**

* ë‹¨ì–´ì˜ ì—°ì†ì ì¸ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
* **Unigram**: ë‹¨ì–´ í•˜ë‚˜ì˜ í™•ë¥ 
* **Bigram**: ë‘ ë‹¨ì–´ì˜ ì—°ì† í™•ë¥ 
* **Trigram**: ì„¸ ë‹¨ì–´ì˜ ì—°ì† í™•ë¥ 

2. **ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ (Neural Network-based Models)**
   * RNN, LSTM, Transformerì™€ ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
   * **BERT, GPT-2**: ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, ë¬¸ë§¥ì„ ì˜ ë°˜ì˜í•˜ì—¬ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

**ì‘ìš© ë¶„ì•¼**

* **ê¸°ê³„ ë²ˆì—­ (Machine Translation)**
* **ì² ì êµì • (Spell Correction)**
* **ìŒì„± ì¸ì‹ (Speech Recognition)**
* **ìš”ì•½ (Summarization)**
* **ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œ (Question Answering)**

**ì½”ë“œ ì˜ˆì‹œ**

```
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

input_text = "Natural language processing is"
input_ids = tokenizer.encode(input_text, return_tensors="pt")
output = model.generate(input_ids, max_length=50, num_return_sequences=1)

print(tokenizer.decode(output[0], skip_special_tokens=True))
```
