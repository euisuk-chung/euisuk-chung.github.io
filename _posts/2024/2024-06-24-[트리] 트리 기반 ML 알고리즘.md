---
title: "[íŠ¸ë¦¬] íŠ¸ë¦¬ ê¸°ë°˜ ML ì•Œê³ ë¦¬ì¦˜"
date: "2024-06-24"
tags:
  - "ê°œë…ì •ë¦¬"
  - "ë¨¸ì‹ ëŸ¬ë‹"
year: "2024"
---

# [íŠ¸ë¦¬] íŠ¸ë¦¬ ê¸°ë°˜ ML ì•Œê³ ë¦¬ì¦˜

ì›ë³¸ ê²Œì‹œê¸€: https://velog.io/@euisuk-chung/íŠ¸ë¦¬-íŠ¸ë¦¬-ê¸°ë°˜-ML-ì•Œê³ ë¦¬ì¦˜



íŠ¸ë¦¬ ê¸°ë°˜ ML ì•Œê³ ë¦¬ì¦˜
=============

`íŠ¸ë¦¬ ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹(ML) ì•Œê³ ë¦¬ì¦˜`ì€ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ì˜ˆì¸¡(Regression) ë˜ëŠ” ë¶„ë¥˜(Classification)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ë‚´ë¶€ ë…¸ë“œì—ì„œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ê³ , ë¦¬í”„ ë…¸ë“œì—ì„œ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì‚°ì¶œí•©ë‹ˆë‹¤. íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì–´ í•´ì„ì´ ìš©ì´í•˜ë©°, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

ì•Œê³ ë¦¬ì¦˜ì˜ ì¥ë‹¨ì 
---------

### ì¥ì 

1. **í•´ì„ ìš©ì´ì„± (Interpretability)**: íŠ¸ë¦¬ êµ¬ì¡°ëŠ” ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê³ , ëª¨ë¸ì˜ ê²°ì • ê³¼ì •ì„ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì–´ë–»ê²Œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ”ì§€ ì‰½ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, íŠ¹íˆ ì˜ì‚¬ ê²°ì •ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.
2. **ë¹„ì„ í˜• ê´€ê³„ ëª¨ë¸ë§ (Non-linear Relationships)**: íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ë¹„ì„ í˜•ì ì¸ ë°ì´í„° ê´€ê³„ë¥¼ ì˜ í¬ì°©í•  ìˆ˜ ìˆì–´ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **íŠ¹ì§• ì„ íƒ (Feature Selection)**: ì•Œê³ ë¦¬ì¦˜ì´ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ê³  ë¶ˆí•„ìš”í•œ íŠ¹ì§•ì„ ë¬´ì‹œí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
4. **ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ì²˜ë¦¬ (Handling Various Data Types)**: ì—°ì†í˜• ë° ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°ì´í„°ì…‹ì— ìœ ì—°í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. **ë¡œë²„ìŠ¤íŠ¸í•¨ (Robustness)**: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì€ ì´ìƒì¹˜(outliers)ë‚˜ ì¡ìŒ(noise)ì— ê°•í•œ íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë°ì´í„° ì¡°ê±´ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ê²Œ í•©ë‹ˆë‹¤.

### ë‹¨ì 

1. **ê³¼ì í•© ìœ„í—˜ (Risk of Overfitting)**: ë‹¨ì¼ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëŠ” ê³¼ì í•©ì— ë¯¼ê°í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ ë§ì¶°ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ì§€ì¹˜ê¸°(pruning) ë“±ì˜ ê¸°ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.
2. **ë†’ì€ ê³„ì‚° ë¹„ìš© (High Computational Cost)**: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ íŠ¸ë¦¬ë¥¼ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ë° ì‹œê°„ì´ ë§ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¶€ìŠ¤íŒ… ê³„ì—´ ëª¨ë¸(ì˜ˆ: GBM, AdaBoost ë“±)ì€ ë§ì€ íŠ¸ë¦¬ë¥¼ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ ê³„ì‚° ë¹„ìš©ì´ ë” ë†’ìŠµë‹ˆë‹¤.
3. **ë°ì´í„° ë¯¼ê°ì„± (Data Sensitivity)**: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì€ ë°ì´í„°ì˜ ì‘ì€ ë³€í™”ì—ë„ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© ë³µì¡ì„± (Complexity of Interactions)**: íŠ¸ë¦¬ êµ¬ì¡°ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ í•´ì„ì„ ì–´ë µê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. **ê²°ì • ê²½ê³„ ë¶ˆì—°ì†ì„± (Discontinuity of Decision Boundaries)**: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì€ ê²°ì • ê²½ê³„ê°€ ë¶ˆì—°ì†ì ì´ì–´ì„œ, ì‹¤ì œ ë°ì´í„°ì˜ ì—°ì†ì„±ì„ ì˜ ë°˜ì˜í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ê²½ê³„ê°€ ì¤‘ìš”í•œ ì—°ì†í˜• ë³€ìˆ˜ì— ëŒ€í•´ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¸ë¦¬ ê¸°ë°˜ ML ì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „
-----------------

íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€/ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì—­ì‚¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ìœ„ì¹˜ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤. ê° ì•Œê³ ë¦¬ì¦˜ì€ ê·¸ ì´ì „ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

ì•„ë˜ì— ì£¼ìš” íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „ ìˆœì„œì™€ íŠ¹ì§•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê¸°ì…í•œ ì—­ì‚¬ëŠ” ì°¾ì€ ë…¼ë¬¸ ë‚ ì§œë¥¼ í† ëŒ€ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. (ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ ë‚ ì§œì™€ ìƒì´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!!)

**1. Decision Tree (ì˜ì‚¬ê²°ì •ë‚˜ë¬´)**

* **ì—­ì‚¬**: 1960ë…„ëŒ€ ([ë…¼ë¬¸ ë§í¬](https://link.springer.com/article/10.1007/BF00116251))
* **íŠ¹ì§•**: ë°ì´í„°ì˜ íŠ¹ì§•ì— ë”°ë¼ ë¶„í• ì„ ë°˜ë³µí•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ID3, C4.5, CART ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì´ í¬í•¨ë©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ì—”íŠ¸ë¡œí”¼, ì§€ë‹ˆ ë¶ˆìˆœë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„í•  ê¸°ì¤€ì„ ì •í•¨.

**2. Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)**

* **ì—­ì‚¬**: 2001ë…„ (Leo Breiman) ([ë…¼ë¬¸ ë§í¬](https://link.springer.com/article/10.1023/A:1010933404324))
* **íŠ¹ì§•**: ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ì•™ìƒë¸”í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê° íŠ¸ë¦¬ëŠ” ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ê³¼ ë¬´ì‘ìœ„ íŠ¹ì§• ì„ íƒì„ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ë°°ê¹…(bagging) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ í‰ê·  ë‚´ê±°ë‚˜ ë‹¤ìˆ˜ê²°ë¡œ ê²°í•©.

**3. Gradient Boosting (ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)**

* **ì—­ì‚¬**: 2001ë…„ (Jerome Friedman) ([ë…¼ë¬¸ ë§í¬](https://www.jstor.org/stable/2699986))
* **íŠ¹ì§•**: ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì£¼ë¡œ íšŒê·€ ë¶„ì„ì— ì‚¬ìš©ë˜ì§€ë§Œ, ë¶„ë¥˜ì—ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚´.

**4. AdaBoost (Adaptive Boosting)**

* **ì—­ì‚¬**: 1996ë…„ (Yoav Freund, Robert Schapire) ([ë…¼ë¬¸ ë§í¬](https://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf))
* **íŠ¹ì§•**: ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì— ê°€ì¤‘ì¹˜ë¥¼ ë” ë¶€ì—¬í•˜ì—¬ ë‹¤ìŒ íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ê°•ë ¥í•œ í•™ìŠµê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ê° í•™ìŠµê¸°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì„±.

**5. Extra Trees (Extremely Randomized Trees)**

* **ì—­ì‚¬**: 2006ë…„ (Pierre Geurts, Damien Ernst, Louis Wehenkel) ([ë…¼ë¬¸ ë§í¬](https://link.springer.com/article/10.1007/s10994-006-6226-1))
* **íŠ¹ì§•**: ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ìœ ì‚¬í•˜ì§€ë§Œ, íŠ¸ë¦¬ì˜ ë¶„í•  ê¸°ì¤€ì„ ì™„ì „íˆ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë” ë§ì€ íŠ¸ë¦¬ë¥¼ í•„ìš”ë¡œ í•˜ì§€ë§Œ, ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ë¶„í•  ì‹œ ë¬´ì‘ìœ„ë¡œ íŠ¹ì§•ê³¼ ë¶„í• ì ì„ ì„ íƒ.

**6. XGBoost (Extreme Gradient Boosting)**

* **ì—­ì‚¬**: 2016ë…„ (Tianqi Chen) ([ë…¼ë¬¸ ë§í¬](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf))
* **íŠ¹ì§•**: ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ê°œì„ í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì •ê·œí™”ì™€ ë³‘ë ¬ ì²˜ë¦¬ ë“±ì˜ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ì •ê·œí™” í•­ì„ ì¶”ê°€í•˜ì—¬ ê³¼ì í•© ë°©ì§€.

**7. LightGBM (Light Gradient Boosting Machine)**

* **ì—­ì‚¬**: 2017ë…„ (Microsoft) ([ë…¼ë¬¸ ë§í¬](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree))
* **íŠ¹ì§•**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ê³ ì°¨ì› ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: Gradient-based One-Side Sampling (GOSS)ì™€ Exclusive Feature Bundling (EFB) ê¸°ë²• ì‚¬ìš©.

**8. CatBoost (Categorical Boosting)**

* **ì—­ì‚¬**: 2018ë…„ (Yandex) ([ë…¼ë¬¸ ë§í¬](https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf))
* **íŠ¹ì§•**: ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆœì°¨ ë¶€ìŠ¤íŒ…ê³¼ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ì— ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ìˆœì°¨ì ì¸ ìˆœì„œì— ë”°ë¼ ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ì˜ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì²˜ë¦¬.

**9. Histogram-based Gradient Boosting (HGBT)**

* **ì—­ì‚¬**: íŠ¹ì •í•˜ê¸° ì–´ë ¤ì›€. (LightGBM ê°œë°œ ë‹¹ì‹œ, íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ í™•ì¸í•¨ => ì´ë¥¼ ë°œì „ì‹œí‚¨ í˜•íƒœê°€ ë°”ë¡œ HGBT)
* **íŠ¹ì§•**: íˆìŠ¤í† ê·¸ë¨ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì—°ì† ë³€ìˆ˜ë¥¼ ë²„í‚·ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
* **ì˜ˆì‹œ**: ê° íŠ¹ì§•ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥´ê²Œ ìµœì  ë¶„í• ì„ ì°¾ìŒ.

ì´ ì™¸ì—ë„ íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€í˜•ê³¼ ê°œì„ ì´ ê³„ì†í•´ì„œ ì´ë£¨ì–´ì§€ê³  ìˆìœ¼ë©°, ìµœì‹  ê¸°ìˆ  ë™í–¥ì— ë”°ë¼ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì´ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì— ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì£¼ìš” íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì—­ì‚¬ì  ìˆœì„œì™€ íŠ¹ì§•ì„ ì„¤ëª…í•˜ê³ , ê° ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë… ë° ì›ë¦¬, íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

### 1. Decision Tree (ì˜ì‚¬ê²°ì •ë‚˜ë¬´)

![](https://velog.velcdn.com/images/euisuk-chung/post/2c1e4d0a-f871-4ac7-a49a-59461168de9f/image.png)  

ì¶œì²˜: ì§ì¥ì¸ ê³ ë‹ˆì˜ ë°ì´í„° ë¶„ì„

#### ê°œë… ë° ì›ë¦¬

**ì˜ì‚¬ê²°ì •ë‚˜ë¬´**ëŠ” ë°ì´í„°ì˜ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. íŠ¸ë¦¬ëŠ” ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ë‚´ë¶€ ë…¸ë“œì—ì„œ ë°ì´í„°ì˜ íŠ¹ì§•ì— ë”°ë¼ ë¶„í• ë˜ê³ , ë¦¬í”„ ë…¸ë“œì—ì„œ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì‚°ì¶œí•©ë‹ˆë‹¤. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ë¶„í•  ê¸°ì¤€ (Split Criterion)**: ë°ì´í„° ë¶„í• ì˜ ê¸°ì¤€ì„ ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì—”íŠ¸ë¡œí”¼(entropy)ì™€ ì§€ë‹ˆ ë¶ˆìˆœë„(gini impurity)ê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤.
2. **ì •ë³´ íšë“ (Information Gain)**: íŠ¹ì • ë¶„í•  ê¸°ì¤€ì— ì˜í•´ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ì˜ ë¶„í• ë˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ì •ë³´ íšë“ì´ ìµœëŒ€í™”ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
3. **ê°€ì§€ì¹˜ê¸° (Pruning)**: ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ íŠ¸ë¦¬ì˜ ì„±ì¥ì„ ì œí•œí•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ê°€ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# íŠ¸ë¦¬ ì‹œê°í™”
plt.figure(figsize=(20, 10))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `DecisionTreeClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì§€ë‹ˆ ë¶ˆìˆœë„ë¥¼ ë¶„í•  ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ìµœëŒ€ ê¹Šì´ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **íŠ¸ë¦¬ ì‹œê°í™”**: `plot_tree()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ íŠ¸ë¦¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. íŠ¸ë¦¬ì˜ ê° ë…¸ë“œëŠ” íŠ¹ì§•ê³¼ í´ë˜ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### 2. Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)

![](https://velog.velcdn.com/images/euisuk-chung/post/0c54b624-a64f-4906-b334-e259238db9c5/image.png)  

ì¶œì²˜ : GeeksforGeeks

#### ê°œë… ë° ì›ë¦¬

**ëœë¤ í¬ë ˆìŠ¤íŠ¸**ëŠ” ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì•™ìƒë¸”í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê° ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëŠ” ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ë©°, ìµœì¢… ì˜ˆì¸¡ì€ ê° ë‚˜ë¬´ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ë°°ê¹… (Bagging)**: ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ì„ í†µí•´ ì—¬ëŸ¬ íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ê° ì„¸íŠ¸ì— ëŒ€í•´ ë…ë¦½ì ì¸ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
2. **ëœë¤ íŠ¹ì§• ì„ íƒ**: ê° ë…¸ë“œì—ì„œ ë¶„í• í•  ë•Œ ì „ì²´ íŠ¹ì§• ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì¼ë¶€ íŠ¹ì§•ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„í• í•©ë‹ˆë‹¤.
3. **ì•™ìƒë¸”**: ê° ë‚˜ë¬´ì˜ ì˜ˆì¸¡ì„ í‰ê·  ë‚´ê±°ë‚˜ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importances_
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in Random Forest')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `RandomForestClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ê°œì˜ íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³ , ê° íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 3. Gradient Boosting (ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)

![](https://velog.velcdn.com/images/euisuk-chung/post/d14c2f78-1d7e-4de9-bfca-000d2cdd2bf8/image.png)  

ì¶œì²˜ : GeeksforGeeks

#### ê°œë… ë° ì›ë¦¬

**ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…**ì€ ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì•™ìƒë¸” í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤. ê° íŠ¸ë¦¬ëŠ” ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•™ìŠµë˜ë©°, ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ìˆœì°¨ì  í•™ìŠµ**: ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©°, ê° ë‹¨ê³„ì—ì„œ ì´ì „ ëª¨ë¸ì˜ ì”ì—¬ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
2. **ì†ì‹¤ í•¨ìˆ˜ (Loss Function)**: ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¡œ, ì£¼ë¡œ íšŒê·€ì—ì„œëŠ” í‰ê·  ì œê³± ì˜¤ì°¨(MSE), ë¶„ë¥˜ì—ì„œëŠ” ë¡œê·¸ ì†ì‹¤(Log Loss)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. **ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°**: ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê° ë‹¨ê³„ì—ì„œ ì”ì—¬ ì˜¤ì°¨ì˜ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì„ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = GradientBoostingClassifier(n

_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importances_
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in Gradient Boosting')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `GradientBoostingClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ê°œì˜ íŠ¸ë¦¬, í•™ìŠµë¥  0.1, ìµœëŒ€ ê¹Šì´ 3ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 4. AdaBoost (Adaptive Boosting)

![](https://velog.velcdn.com/images/euisuk-chung/post/27425899-627f-4186-8b4f-b234944654b0/image.png)  

ì¶œì²˜ : Towards AI

#### ê°œë… ë° ì›ë¦¬

**AdaBoost (Adaptive Boosting)**ëŠ” ì•½í•œ í•™ìŠµê¸°(Weak Learner)ë¥¼ ê²°í•©í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°(Strong Learner)ë¥¼ ë§Œë“œëŠ” ì•™ìƒë¸” í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤. AdaBoostëŠ” ì´ˆê¸° í•™ìŠµê¸°ì˜ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ í›„ì† í•™ìŠµê¸°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì ì°¨ì ìœ¼ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: ì´ˆê¸° í•™ìŠµê¸°ì˜ ì˜¤ë¥˜ê°€ í° ë°ì´í„° í¬ì¸íŠ¸ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë‹¤ìŒ í•™ìŠµê¸°ê°€ ì´ ì˜¤ë¥˜ë¥¼ ì¤„ì´ë„ë¡ í•©ë‹ˆë‹¤.
2. **ìˆœì°¨ì  í•™ìŠµ**: ê° í•™ìŠµê¸°ëŠ” ì´ì „ í•™ìŠµê¸°ì˜ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.
3. **ì•™ìƒë¸”**: ìµœì¢… ì˜ˆì¸¡ì€ ëª¨ë“  í•™ìŠµê¸°ì˜ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ëœ íˆ¬í‘œë¥¼ í†µí•´ ê²°ì •ë©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ AdaBoostë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# AdaBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)
clf = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importances_
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in AdaBoost')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `DecisionTreeClassifier`ë¥¼ ì•½í•œ í•™ìŠµê¸°ë¡œ ì‚¬ìš©í•˜ì—¬ `AdaBoostClassifier`ë¥¼ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìµœëŒ€ ê¹Šì´ 1ì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì•½í•œ í•™ìŠµê¸°ë¡œ ì‚¬ìš©í•˜ë©°, 50ê°œì˜ í•™ìŠµê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 5. Extra Trees (Extremely Randomized Trees)

![](https://velog.velcdn.com/images/euisuk-chung/post/f737e96c-7451-41cc-902c-daae20a04fe2/image.png)  

ì¶œì²˜ : stackexchange, difference-between-random-forest-and-extremely-randomized-trees

#### ê°œë… ë° ì›ë¦¬

**Extra Trees (Extremely Randomized Trees)**ëŠ” ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ìœ ì‚¬í•˜ì§€ë§Œ, ë” ë§ì€ ë¬´ì‘ìœ„ì„±ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì•™ìƒë¸” ë°©ë²•ì…ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ê°€ ê° ë…¸ë“œì—ì„œ ìµœì ì˜ ë¶„í• ì„ ì°¾ëŠ” ë°˜ë©´, Extra TreesëŠ” ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ë¶„í•  ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ íŠ¸ë¦¬ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ëª¨ë¸ì˜ í¸í–¥ì€ ì•½ê°„ ì¦ê°€í•  ìˆ˜ ìˆì§€ë§Œ, ë¶„ì‚°ì€ ê°ì†Œí•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ì™„ì „ ë¬´ì‘ìœ„ ë¶„í• **: ê° ë…¸ë“œì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ë¶„í•  ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ ë¶„í• í•©ë‹ˆë‹¤.
2. **ë°°ê¹… (Bagging)**: ì—¬ëŸ¬ ê°œì˜ íŠ¸ë¦¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•©ë‹ˆë‹¤.
3. **íŠ¹ì§•ì˜ ë¬´ì‘ìœ„ ì„ íƒ**: ê° ë…¸ë“œì—ì„œ ë¶„í• í•  ë•Œ ì‚¬ìš©í•  íŠ¹ì§•ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

> ğŸ’¡ **ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ì°¨ì´**  
> 
> ì €ëŠ” ê°œì¸ì ìœ¼ë¡œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ì˜ ê°œë…ì´ ë§ì´ í˜¼ë™ìŠ¤ëŸ½ë”ë¼ê³ ìš”... ê·¸ë˜ì„œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤!
> 
> * ë‘ ì•Œê³ ë¦¬ì¦˜ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì•™ìƒë¸”(ensemble) ë°©ì‹ì˜ ì˜ì‚¬ê²°ì • ë‚˜ë¬´(decision tree) ëª¨ë¸ì´ë¼ëŠ” ì ì—ì„œ ìœ ì‚¬í•˜ì§€ë§Œ, ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.
> * `ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)`ì™€ `ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬(Extra Trees)`ì˜ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ **ë…¸ë“œ ë¶„í•  ë°©ì‹**ê³¼ **ìƒ˜í”Œë§ ë°©ì‹**ì˜ ì°¨ì´ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë´ì•¼ í•©ë‹ˆë‹¤.

> ğŸŒ³ğŸŒ³  **1. ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)** ğŸŒ³ğŸŒ³
> 
> * **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ (Bootstrap Sampling, ìƒì´)**:
>   + ê° íŠ¸ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì—ì„œ **ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œ**ì„ ì„ íƒí•˜ì—¬ í•™ìŠµí•¨.
>   + ìƒ˜í”Œë§ì€ **ë³µì› ì¶”ì¶œ ë°©ì‹**ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ì¼ë¶€ ë°ì´í„° í¬ì¸íŠ¸ê°€ ì—¬ëŸ¬ ë²ˆ ì„ íƒë  ìˆ˜ ìˆìŒ.
> * **í”¼ì²˜ ë¬´ì‘ìœ„ì„± (Feature Sampling, ë™ì¼)**:
>   + ê° ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ, ì „ì²´ í”¼ì²˜ ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ í”¼ì²˜ì˜ ë¶€ë¶„ì§‘í•©ì„ ì‚¬ìš©í•¨.
> * **ë…¸ë“œ ìƒ˜í”Œ ë¶„í•  (Node Sample Splits, ìƒì´)**:
>   + ê° ë…¸ë“œì—ì„œ ìµœì ì˜ ë¶„í• ì„ ì°¾ê¸° ìœ„í•´ **ì„ íƒëœ í”¼ì²˜ì˜ ë¶€ë¶„ì§‘í•©**ì„ ì‚¬ìš©í•¨.
>   + ì„ íƒëœ í”¼ì²˜ë“¤ ì¤‘ì—ì„œ ë¶„í•  ê¸°ì¤€ì´ ë˜ëŠ” ì„ê³„ê°’ì„ ì°¾ìŒ.

> ğŸŒ²ğŸŒ² **2. ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ (Extra Trees)** ğŸŒ²ğŸŒ²
> 
> * **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ ì—†ìŒ (No Bootstrap Sampling, ìƒì´)**:
>   + ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
>   + **ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©**í•˜ì—¬ ê° íŠ¸ë¦¬ë¥¼ í•™ìŠµí•¨.
> * **í”¼ì²˜ ë¬´ì‘ìœ„ì„± (Feature Sampling, ë™ì¼)**:
>   + ê° ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ, ì „ì²´ í”¼ì²˜ ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ í”¼ì²˜ì˜ ë¶€ë¶„ì§‘í•©ì„ ì‚¬ìš©í•¨.
> * **ë…¸ë“œ ëœë¤ ë¶„í•  (Node Random Splits, ìƒì´)**:
>   + ì„ íƒëœ í”¼ì²˜ì— ëŒ€í•´ **ë¶„í•  ì„ê³„ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ**í•¨.
>   + ì—¬ëŸ¬ ë¬´ì‘ìœ„ ë¶„í•  ì¤‘ ê°€ì¥ ì¢‹ì€ ë¶„í• ì„ ì„ íƒí•¨.
>   + ì´ ê³¼ì •ì—ì„œ ë¬´ì‘ìœ„ì„±ì´ ë” ë§ì´ ë„ì…ë˜ì–´, ê³„ì‚° ì†ë„ê°€ ë¹¨ë¼ì§€ê³  íŠ¸ë¦¬ë“¤ì´ ì„œë¡œ ë” ë…ë¦½ì ì„.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Extra Treesë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Extra Trees ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = ExtraTreesClassifier(n_estimators=100, max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importances_
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in Extra Trees')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `ExtraTreesClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ Extra Trees ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ê°œì˜ íŠ¸ë¦¬ì™€ ìµœëŒ€ ê¹Šì´ 3ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 6. XGBoost (Extreme Gradient Boosting)

![](https://velog.velcdn.com/images/euisuk-chung/post/c56b3bf8-90fd-4399-b6fd-b475bef4f4e7/image.png)  

ì¶œì²˜: ResearchGate, Flow chart of XGBoost

#### ê°œë… ë° ì›ë¦¬

**XGBoost (Extreme Gradient Boosting)**ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì„ í™•ì¥í•˜ì—¬ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. XGBoostëŠ” ì •ê·œí™”, ë³‘ë ¬ ì²˜ë¦¬, ì¡°ê¸° ì¢…ë£Œ ë“± ì—¬ëŸ¬ ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ì •ê·œí™” (Regularization)**: L1, L2 ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ê³  ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
2. **ë³‘ë ¬ ì²˜ë¦¬ (Parallel Processing)**: ë‹¤ì¤‘ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
3. **ì¡°ê¸° ì¢…ë£Œ (Early Stopping)**: ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ í–¥ìƒì´ ë©ˆì¶”ë©´ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
4. **ë¶„í•  ê²€ìƒ‰ ìµœì í™”**: ë¶„í• ì ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `xgboost` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ XGBoostë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
import xgboost as xgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# XGBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False)
clf.fit(X_train, y_train, eval_metric='logloss')

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importance()
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in XGBoost')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `XGBClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ XGBoost ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ê°œì˜ íŠ¸ë¦¬, í•™ìŠµë¥  0.1, ìµœëŒ€ ê¹Šì´ 3ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. `use_label_encoder=False`ëŠ” ìµœì‹  ë²„ì „ì˜ XGBoostì—ì„œ ê²½ê³ ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importance()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 7. LightGBM (Light Gradient Boosting Machine)

![](https://velog.velcdn.com/images/euisuk-chung/post/5027f422-a2ec-41a4-afa2-d50772c9a2a5/image.png)  

ì¶œì²˜ : <https://www.linkedin.com/pulse/xgboost-vs-lightgbm-ashik-kumar/>

#### ê°œë… ë° ì›ë¦¬

**LightGBM (Light Gradient Boosting Machine)**ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ê³ ì°¨ì› ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. LightGBMì€ ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ê³¼ ì—¬ëŸ¬ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  (Leaf-wise Tree Growth)**: íŠ¸ë¦¬ì˜ ë¦¬í”„ë¥¼ í™•ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì†ì‹¤ì„ ê°€ì¥ ë§ì´ ì¤„ì´ëŠ” ë¦¬í”„ë¥¼ ì„ íƒí•˜ì—¬ ë¶„í• í•©ë‹ˆë‹¤. ì´ëŠ” ê¹Šì´ ì¤‘ì‹¬ ë¶„í• ë³´ë‹¤ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.
2. **Gradient-based One-Side Sampling (GOSS)**: ì¤‘ìš”í•œ ìƒ˜í”Œì„ ë” ë§ì´ ì‚¬ìš©í•˜ê³  ëœ ì¤‘ìš”í•œ ìƒ˜í”Œì„ ì¤„ì—¬ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
3. **Exclusive Feature Bundling (EFB)**: ìƒí˜¸ ë°°íƒ€ì ì¸ íŠ¹ì§•ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ íŠ¹ì§•ì˜ ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.
4. **Histogram-based Decision Tree**: ì—°ì†í˜• íŠ¹ì§•ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ë¶„í• ì ì„ ì°¾ìŠµë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `lightgbm` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LightGBMì„ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
import lightgbm as lgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# LightGBM ë°ì´í„°ì…‹ ìƒì„±
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# LightGBM ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'boosting_type': 'gbdt',
    'objective': 'multiclass',
    'num_class': 3,
    'metric': 'multi_logloss',
    'learning_rate': 0.1,
    'max_depth': 3,
    'num_leaves': 31,
    'random_state': 42
}

# ëª¨ë¸ í•™ìŠµ
clf = lgb.train(params, train_data, num_boost_round=100, valid_sets=[test_data], early_stopping_rounds=10)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test, num_iteration=clf.best_iteration)
y_pred = [np.argmax(line) for line in y_pred]
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importance()
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in LightGBM')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **LightGBM ë°ì´í„°ì…‹ ìƒì„±**: `lgb.Dataset`ë¥¼ ì‚¬ìš©í•˜ì—¬ LightGBMì˜ ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
3. **ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •**: `params` ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
4. **ëª¨ë¸ í•™ìŠµ**: `lgb.train()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. `early_stopping_rounds`ë¥¼ ì„¤ì •í•˜ì—¬ ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ í–¥ìƒì´ ë©ˆì¶”ë©´ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.
5. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
6. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importance()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 8. CatBoost (Categorical Boosting)

![](https://velog.velcdn.com/images/euisuk-chung/post/829a8010-f27b-4064-9735-3734ad102d47/image.png)  

ì¶œì²˜ : <https://www.mdpi.com/sensors/sensors-23-01811/article_deploy/html/images/sensors-23-01811-g003.png>

#### ê°œë… ë° ì›ë¦¬

**CatBoost (Categorical Boosting)**ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. CatBoostëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ê³ ìœ í•œ ì²˜ë¦¬ ë°©ë²•ê³¼ ìˆœì°¨ ë¶€ìŠ¤íŒ… ê¸°ë²•ì„ í†µí•´ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ìˆœì°¨ ë¶€ìŠ¤íŒ… (Ordered Boosting)**: ë°ì´í„° ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ë¶€ìŠ¤íŒ… ë‹¨ê³„ë§ˆë‹¤ ìƒˆë¡œìš´ ìˆœì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
2. **ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬**: ê° ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•´ ê³ ìœ í•œ í†µê³„ëŸ‰(í‰ê·  ëª©í‘œê°’ ë“±)ì„ ì‚¬ìš©í•˜ì—¬ ë³€í™˜í•©ë‹ˆë‹¤.
3. **ëŒ€ì¹­ íŠ¸ë¦¬ (Symmetric Trees)**: ê· í˜• ì¡íŒ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ì†ë„ë¥¼ ë†’ì´ê³ , ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.
4. **ê¸°íƒ€ ìµœì í™”**: GPU ì§€ì›, ì¡°ê¸° ì¢…ë£Œ, ì •ê·œí™” ë“±ì˜ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `catboost` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ CatBoostë¥¼ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
import catboost as cb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# CatBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, random_seed=42, verbose=0)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.get_feature_importance()
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title

('Feature Importance in CatBoost')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `CatBoostClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ CatBoost ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ë²ˆì˜ ë°˜ë³µ(iterations), í•™ìŠµë¥  0.1, ìµœëŒ€ ê¹Šì´ 3ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `get_feature_importance()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 9. Histogram-based Gradient Boosting (HGBT)

![](https://velog.velcdn.com/images/euisuk-chung/post/74d58d84-359a-4084-8101-eaacbd74bf40/image.png)  

ì¶œì²˜ : <https://ars.els-cdn.com/content/image/1-s2.0-S0926580523000274-gr3.jpg>

#### ê°œë… ë° ì›ë¦¬

**íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…**ì€ íˆìŠ¤í† ê·¸ë¨ì„ ì‚¬ìš©í•˜ì—¬ ì—°ì†í˜• íŠ¹ì§•ì„ ë²„í‚·ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¶„í• ì ì„ ì°¾ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ê³„ì‚° ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©°, ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ëŠ” ë° ìœ ë¦¬í•©ë‹ˆë‹¤. ì£¼ìš” ê°œë…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **íˆìŠ¤í† ê·¸ë¨ ë³€í™˜**: ì—°ì†í˜• íŠ¹ì§•ì„ ì¼ì •í•œ ê°„ê²© ë˜ëŠ” ë°ì´í„° ë¶„í¬ì— ë”°ë¼ ë²„í‚·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2. **ë¹ ë¥¸ ë¶„í• ì  ì°¾ê¸°**: ê° ë²„í‚· ë‚´ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¶„í• ì ì„ ë¹ ë¥´ê²Œ ì°¾ìŠµë‹ˆë‹¤.
3. **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬**: íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì—ì„œë„ ë†’ì€ íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

#### íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ

ì•„ë˜ëŠ” `scikit-learn`ì˜ `HistGradientBoostingClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì„ êµ¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”
feature_importances = clf.feature_importances_
features = iris.feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance in Histogram-based Gradient Boosting')
plt.show()
```
#### ì½”ë“œ ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: `load_iris()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split()`ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ìƒì„± ë° í•™ìŠµ**: `HistGradientBoostingClassifier`ë¥¼ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ ìƒì„±í•˜ê³ , `fit()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100ë²ˆì˜ ë°˜ë³µ(iterations), í•™ìŠµë¥  0.1, ìµœëŒ€ ê¹Šì´ 3ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
3. **ì˜ˆì¸¡ ë° í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. **ì¤‘ìš” íŠ¹ì§• ì‹œê°í™”**: `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , `seaborn`ì„ ì‚¬ìš©í•˜ì—¬ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 10. ì •ë¦¬

ë‹¤ìŒì€ `scikit-learn`ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµí•˜ê³  ê²€ì¦ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import pandas as pd

# ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ì•Œê³ ë¦¬ì¦˜ ë¦¬ìŠ¤íŠ¸
classifiers = {
    "Decision Tree": DecisionTreeClassifier(max_depth=3, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    "AdaBoost": AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1, random_state=42), n_estimators=50, learning_rate=1.0, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, max_depth=3, random_state=42),
    "XGBoost": xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    "CatBoost": cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, random_seed=42, verbose=0),
    "HistGradientBoosting": HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=3, random_state=42)
}

# ì„±ëŠ¥ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
results = []

# ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ í‰ê°€
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    train_accuracy = clf.score(X_train, y_train)
    test_accuracy = clf.score(X_test, y_test)
    cross_val_scores = cross_val_score(clf, X, y, cv=5)
    cross_val_mean = cross_val_scores.mean()
    cross_val_std = cross_val_scores.std()

    results.append({
        "Algorithm": name,
        "Train Accuracy": train_accuracy,
        "Test Accuracy": test_accuracy,
        "Cross-Validation Mean": cross_val_mean,
        "Cross-Validation Std": cross_val_std
    })

# ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
results_df = pd.DataFrame(results)

# ê²°ê³¼ ì¶œë ¥
print(results_df)
```

![](https://velog.velcdn.com/images/euisuk-chung/post/d306f98b-aa11-47af-a7e7-c0f56e6b5433/image.png)

ì˜¤ëŠ˜ì€ íŠ¸ë¦¬ ê¸°ë°˜ ML ì•Œê³ ë¦¬ì¦˜ ì „ë°˜ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ê³ , ê¸°ë³¸ì ì¸ ì½”ë“œê¹Œì§€ ì ì–´ë³´ëŠ” ì‹œê°„ì„ ê°€ì ¸ë´¤ëŠ”ë°ìš” ğŸ“Š ì €ë„ ê°œì¸ì ìœ¼ë¡œ í•œë²ˆì¯¤ ì •ë¦¬í•˜ê³  ê°€ê³  ì‹¶ì—ˆë˜ ê°œë…ì´ë¼ ìœ ìµí–ˆë˜ ì‹œê°„ì´ì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤!!

ê°ì‚¬í•©ë‹ˆë‹¤ ğŸ™

