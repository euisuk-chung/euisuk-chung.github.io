---
title: "[Day 2] Reinforcement Fine-Tuning (RFT) 소개"
date: "2024-12-07"
tags:
  - "OpenAI"
  - "chatGPT"
year: "2024"
---

# [Day 2] Reinforcement Fine-Tuning (RFT) 소개

원본 게시글: https://velog.io/@euisuk-chung/Day-2-OpenAI-Reinforcement-Fine-Tuning



안녕하세요!! 어제(**12 Days of OpenAI: Day 1**)에서 `O1 모델`을 공식 출시하며, ChatGPT의 추론 능력과 신뢰성을 강화한 **Pro 플랜을 소개**했는데요!!

오늘은 그 다음 단계(**12 Days of OpenAI: Day 2**)로, **"강화 학습 기반 파인튜닝(Reinforcement Fine-Tuning, 이하 RFT)"**을 활용한 최첨단 모델 커스터마이제이션 솔루션을 미리 선보입니다.

이 기술은 고급 사용자, 연구자, 기업 고객 모두가 자신만의 **전문 분야 데이터**로 O1 모델을 단련시켜, 마치 고급 과외 선생님처럼 `특정 도메인`에 대한 “**전문가급 추론 모델**”을 만들어 낼 수 있습니다.

---

Reinforcement Fine-Tuning(RFT)란 무엇인가?
-------------------------------------

기존의 파인튜닝(Fine-Tuning)은 주로 지도학습 방식을 사용합니다. 즉, 모델에게 특정 스타일, 어조, 포맷을 모방하도록 학습시키는 방식입니다. 이는 모델이 특정 예제를 따라하는 "모방 학습" 수준으로 볼 수 있습니다. 그러나 **RFT**는 한 단계 더 나아갑니다. RFT는 단순히 정답 예제를 따라하는 것이 아니라, **강화 학습(Reinforcement Learning, RL)** 기법을 통해 모델이 새로운 추론 전략을 습득하도록 만듭니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/05a25392-d019-485d-9b75-eb6f6d520e5f/image.png)

> **이미지 출처** : A short note on Reinforced Fine-Tuning or ReFT | by Nilesh Barla (Medium)

이 방법의 핵심은 모델이 문제를 해결하는 과정을 평가하는 **"채점기(Grader)"**를 사용하는 것입니다. 모델이 특정 문제(입력 텍스트, 질문, 증상 데이터, 과학적 문헌 등)에 답변을 내놓으면, 채점기는 그 답변의 정확도를 점수화합니다. 그 후, 강화 학습 알고리즘은 올바른 추론 경로에 높은 보상을, 잘못된 추론 경로에 낮은 보상을 할당하여 모델이 스스로 더 나은 추론 전략을 개발하도록 합니다.

### RFT의 특징

1. **단 소량의 데이터로도 새로운 추론 전략 습득**:  
   
   기존 대규모 언어 모델이 수만~수십만 개 이상의 예제가 필요한 것과 달리, RFT는 수십 개~수백 개 단위의 “고품질 골든 데이터셋”으로도 높은 전문성을 가진 모델을 만들 수 있습니다. 작은 규모의 전문 데이터셋으로도 새로운 추론 패턴을 습득하여, **연구나 전문분야에서 맞춤형 전문가 모델**을 빠르게 개발할 수 있습니다.
2. **정답을 ‘맞추는 것’ 그 이상을 학습**:  
   
   단순히 정답을 따라하는 것이 아니라, 답변을 도출하는 **추론 과정** 자체를 개선합니다. 이로써 모델은 새로운 문제나 변형된 상황에도 유연하게 대처하고, 이전에 보지 못한 유형의 질문에도 일관된 논리 흐름을 제시할 수 있게 됩니다.
3. **OpenAI 내부 모델 개발 노하우 공유**:  
   
   GPT-4, O1 시리즈와 같은 최첨단 모델을 개발할 때 OpenAI는 내부적으로 강화 학습 기법을 활용해왔습니다. 이제 그 동일한 RL 파이프라인을 고객과 연구 커뮤니티에 개방하여, 모두가 고급 AI 개발 기술에 접근할 수 있게 됩니다.

---

실제 적용 사례: 희소질환 유전자 변이 분석
------------------------

이번 라이브 데모에서는 `샌프란시스코 Berkeley Lab`의 연구자들과 협력한 사례가 소개되었습니다. **유전자 변이로 인한 희소질환 진단**은 다음과 같은 과제가 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/67696edc-5e61-4ac2-a693-bb33069621cf/image.png)

* **복잡한 의학적 전문성 필요**: 특정 질환을 유발하는 유전적 변이를 판별하려면 광범위한 생물학적 지식과 임상 정보가 필요합니다.
* **불완전하고 산발적인 증상 정보 처리**: 환자의 증상은 모두 명확하지 않고, 조건별로 증상이 배제되는 경우(Absent Symptoms)도 많습니다.

이러한 고난이도 작업에 RFT를 활용한 결과, 이전까지 모델이 단순 모방 학습으로는 해결하기 어려웠던 **“추론적 의사결정”**을 모델이 스스로 개선하는 모습을 보였습니다.

### Demo

* 실제 사례에서 O1-mini 모델(기본적으로 더 작고 저렴한 모델)을 RFT로 강화 학습한 결과, 전문 분야 생물정보학적 문제에 대해 O1 모델 수준, 혹은 그 이상으로 추론 정확도가 개선되었습니다. (**Demo 내용**)

![](https://velog.velcdn.com/images/euisuk-chung/post/da3c7c66-3877-44b7-bda2-16bfbcf0b5c3/image.png)

데모에서 강화학습 기반 파인튜닝(Reinforcement Fine-Tuning, RFT)을 위해 사용되는 학습 데이터셋에 대해 다음과 같이 설명하고 있습니다.

1. **데이터 구조**:  
   
   파인튜닝에 활용하는 데이터는 **JSONL(JSON Lines)** 형식으로 구성됩니다. 각 줄은 하나의 학습 예제를 나타내며, 한 줄에 하나의 JSON 객체가 들어 있습니다. 즉, 여러 개의 “문제-정답” 쌍이 줄 단위로 독립적으로 정리되어 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/a2005bf0-c3d7-4240-b65c-0be395578da9/image.png)

2. **예제 구성 요소** (위 그림 참고):  
   
   각 예제(데이터 포인트)는 다음과 같은 정보를 담습니다.
   
   * **입력 정보(Case Report)**: 예를 들어, 환자의 나이, 증상, 특정 증상이 결핍(Absent)된 상황 등 도메인 특화된 상세한 컨텍스트를 제공합니다.
   * **지시사항(Instructions)**: 모델이 해야 할 일을 명시합니다. 예를 들어 “주어진 증상을 유발할 수 있는 유전자 목록을 가장 가능성 높은 순서로 제시하라”와 같은 형태입니다.
   * **정답(Correct Answer)**: 해당 입력 상황에서 실제로 정답으로 간주되는 타깃 결과를 제공합니다. 모델을 평가하는 기준으로 활용되며, 학습 시에는 모델 출력에 대한 보상(점수) 계산에 사용됩니다.
3. **학습 방식**:  
   
   이 JSONL 파일 내 각 예제는 강화학습 기반 파인튜닝 과정에서 “문제-지시사항-정답” 형태로 모델에게 제시됩니다. 모델은 지시사항에 따른 답을 내놓고, 그 답이 정답과 얼마나 일치하는지를 기반으로 점수가 매겨집니다. 이 점수(보상)를 통해 모델은 점차 더 나은 추론 경로를 찾고, 전문적인 문제에도 효율적으로 대응할 수 있도록 학습합니다.

> 🤔 (심화) **어떻게 학습하는가?**  
> 
> 모델이 처음으로 **해당 질문**(예: *“이런 증상을 유발하는 유전자를 순위별로 나열하고 설명하라”*)에 대한 답변을 내놓았을 때, 그 답변 결과를 바탕으로 강**화학습 기반 파인튜닝(Reinforcement Fine-Tuning, RFT) 과정**을 거치게 됩니다.

이 과정은 크게 다음 단계로 요약할 수 있습니다.

> **1. 모델 출력 생성**:  
> 
> 모델은 주어진 입력(증상, 부재 증상, 지시사항)을 바탕으로 가능한 유전자 후보 리스트를 예측합니다.
> 
> * 이는 아직 학습 완료된 최종 상태가 아니라, 현재 파인튜닝 과정 중 모델이 가진 파라미터로부터 생성된 첫 번째 응답입니다.  
>   
>   ![](https://velog.velcdn.com/images/euisuk-chung/post/f7269b04-88b8-4ad2-9836-26d1212155e0/image.png)

> **2. 채점(Grading)을 통한 보상 계산**:  
> 
> RFT에서는 모델 출력에 대해 주어진 **Grader(채점기)**가 정답과의 일치도를 점수로 환산해줍니다. 이 점수(보상)를 통해 모델은 “어떻게 문제를 접근해야 정답에 가깝게 갈 수 있는지”를 학습합니다.
> 
> * 만약 모델이 정답 유전자를 첫 번째 후보로 정확히 제안했다면 높은 보상을 줍니다(예: 보상 점수 1에 근접).
> * 정답 유전자가 상위 다섯 번째 안에만 들어있다면 중간 정도 보상을, 정답을 전혀 못 맞추면 낮은 보상을 부여하는 식으로 스코어를 매깁니다.
> * 만약 모델이 `Training set`에서만 쓸 수 있는 특수한 단서를 암기했다면, `Validation set`에서는 그 단서가 작용하지 않아 낮은 보상을 받게 됩니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/a23a749d-0e52-4c93-b742-5c9f9205903f/image.png)

> **3. 강화학습 알고리즘을 통한 파라미터 업데이트**:  
> 
> 채점 결과(보상 점수)를 바탕으로 모델 파라미터를 업데이트하는 강화학습 알고리즘이 작동합니다. 모델이 정답에 가까운 추론 과정을 밟을수록 모델 파라미터는 그러한 추론 패턴을 강화하고, 불필요하거나 오류를 야기하는 추론 경로는 점차 억제되도록 조정됩니다.
> 
> * 이 과정은 정책경사정책(Policy Gradient)나 Proximal Policy Optimization(PPO) 같은 RL 기법을 활용할 수 있습니다.
> * 핵심은 모델이 단순히 데이터로부터 “정답 문장”을 암기하는 것이 아니라, “이 문제를 어떻게 접근해야 하는지”에 대한 추론 전략을 스스로 개선하도록 하는 것입니다.

> **4. 반복 학습 및 성능 개선**:  
> 
> 첫 번째 답변에 대해 파라미터가 갱신된 후, 모델은 두 번째, 세 번째, …, 수많은 예제들에 걸쳐 반복적으로 같은 과정을 거칩니다.
> 
> * 각 예제마다 보상을 통해 모델의 추론 전략이 점진적으로 향상됩니다.

💬 **KEY-POINT**

**전통적인 지도학습(Supervised Learning) 파인튜닝**은 주어진 입력에 대응하는 정답 출력을 흉내 내는 데 초점을 둡니다. 이 경우 모델은 특정 패턴을 암기하거나, **학습 데이터에만 특화된 규칙을 형성할 가능성**이 높습니다.

반면, **RFT(Reinforcement Fine-Tuning)**에서는 모델이 단순히 정답을 따라 하는 것이 아닌, **“정답을 유도하는 추론 과정 자체”**를 강화학습 기법을 통해 개선하게 됩니다. 이는 모델이 데이터 전반에 내재한 `논리적 패턴`, `인과 관계`, `추론 전략` 등을 파악하도록 유도하기 때문에 **새로운 데이터(Validation set)에서도 해당 추론 전략을 적용**할 수 있게 됩니다.

* 예를 들어, Day 2의 데모에서 사용된 희귀질환 유전자 예측 문제에서는 `Train 데이터`와 `Validation 데이터`에 등장하는 **실제 유전자 목록이 겹치지 않도록 구성**되었습니다.
* 이는 모델이 특정 질환-유전자 대응 관계를 단순 암기하기 어렵게 만들고, 대신 **질환 증상의 특징을 분석하는 일반적인 추론 방법을 학습하도록 유도**합니다.

💬 **성능 변화 정리**  

![](https://velog.velcdn.com/images/euisuk-chung/post/04022d64-7569-4605-906c-1c7e8572b5a5/image.png)

* **Top-1 정확도 개선**: RFT(Reinforcement Fine-Tuning)를 적용한 후, 모델이 "가장 가능성 높은 원인 유전자"를 첫 번째 후보로 정확히 지목하는 비율이 기존 O1-mini 대비 크게 상승하였습니다. 이는 모델이 단순한 패턴 학습을 넘어, 주어진 문제에 대해 더 명확하고 직접적인 추론 전략을 형성했음을 보여줍니다.
* **Top-5 정답 포함도 향상**: 모델이 상위 5개 후보 내에 실제 정답 유전자를 포함시킬 확률 역시 현저히 증가하였습니다. 이로써 모델은 이전보다 풍부한 추론 경로를 탐색하고, 답변의 다양성과 정확성을 함께 향상시키는 능력을 갖추게 되었음을 의미합니다.

이러한 성능 개선은 모델이 단순히 과거 정답을 암기하거나 특정 패턴을 기계적으로 재현하는 것이 아니라, **"어떻게 생각해야 하는가"에 대한 추론 능력을 강화 학습을 통해 획득한 결과**라고 할 수 있습니다.

---

RFT가 유용한 분야
-----------

RFT는 단순 질의응답을 넘어, 다음과 같은 고난이도 전문 분야에서 특히 돋보입니다.

1. **의료·생명과학**: 희귀 질환 유전자 분석, 복잡한 진단 프로토콜 최적화.
2. **법률**: 특정 법령 판례 분석 및 근거 제시, 계약서 내 리스크 식별.
3. **금융**: 복잡한 금융 규제 해석, 알고리즘 트레이딩 전략 개선.
4. **엔지니어링 & 산업 응용**: 복잡한 설계 문제, 대규모 코딩 테스트 생성 및 솔루션 검증.
5. **AI 안전성(Safety) 및 학술 연구**: 특수한 기준을 만족하는 모델 성능 개선.

이런 영역에서 모델은 곧 전문 지식을 체화한 고급 어시스턴트로 진화할 것입니다.

---

RFT Research Program
--------------------

OpenAI는 RFT의 잠재력을 최대화하기 위해 **Alpha 프로그램**을 확장하고 있습니다. 이 프로그램은 다음과 같은 대상에게 추천됩니다.

* **연구기관 및 대학교**: 특정 전공 분야 문제에 최적화된 AI 연구 도구 구축.
* **기업 및 산업 파트너**: 복잡한 내재적 업무 프로세스를 개선하고 자동화.
* **ML 엔지니어·데이터 사이언티스트**: 소규모 데이터로 고정밀 모델을 개발하고자 하는 전문가.

프로그램 참여자는 RFT API에 대한 사전 액세스를 받아 다양한 도메인별 태스크를 모델에게 학습시켜볼 수 있습니다. 또한 피드백을 통해 향후 RFT 공식 출시 시점에 더욱 안정적이고 강력한 API를 제공할 수 있게 됩니다.

> **참여 방법**: 다음 링크(<https://openai.com/form/rft-research-program/>)를 통해 지원서를 제출할 수 있습니다. OpenAI에서 제한된 수의 파트너를 선별하여, RFT 알파 테스트 기회를 준다고 합니다.

---

마무리
---

RFT의 등장은 AI를 단순한 질문-응답 시스템에서 “특정 전문 영역에 대한 진짜 ‘추론 파트너’”로 한 단계 끌어올리는 이정표입니다. 이는 표면적 정보 복제에서 벗어나, 모델이 실제 세계의 복잡한 문제에 맞게 reasoning capability를 스스로 강화하는 혁신적 변화입니다. 향후 RFT를 이용해 모든 연구자, 기업, 기관은 자신만의 **전문 모델 에코시스템**을 구축할 수 있을 것입니다.

내일(12 Days of OpenAI: Day 3)에는 어떤 혁신을 보여줄지 기대됩니다. 매일 새로 공개되는 신기술을 통해 OpenAI는 명확한 비전을 제시하고 있습니다. AI가 단순 도우미를 넘어, 전문성·정밀성·안정성을 갖춘 차세대 파트너로 성장하는 그 과정에 함께 해보시기 바랍니다.

