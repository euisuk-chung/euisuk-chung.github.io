---
title: "[Day 9] OpenAI o1 및 개발자를 위한 새로운 도구"
date: "2024-12-18"
tags:
  - "OpenAI"
  - "chatGPT"
year: "2024"
---

# [Day 9] OpenAI o1 및 개발자를 위한 새로운 도구

원본 게시글: https://velog.io/@euisuk-chung/Day-9-OpenAI-o1-and-dev-tools



안녕하세요! 오늘은 **12 Days of OpenAI** 시리즈의 아홉 번째 날에 오신 것을 환영합니다!

오늘은 ChatGPT에 추가된 강력한 새로운 기능인 "**OpenAI o1 및 개발자를 위한 새로운 도구**"를 소개합니다.

블로그 작성 시점에는 영상이 안 올라왔지만, 현재는 다시 정상 공개된 것 확인했습니다. (~~내가 재촉해서 그런가? ㅎㅎ~~)

다음 링크에서 확인하실 수 있습니다:

* 링크: <https://youtu.be/14leJ1fg4Pw>

**OpenAI o1 및 개발자를 위한 새로운 도구 소개**
---------------------------------

OpenAI는 AI 애플리케이션을 구축하는 개발자들을 위해 더욱 강력한 모델과 유연한 도구들을 공개했습니다. 이번 업데이트는 성능, 사용자 맞춤화, 비용 효율성을 개선하는 데 중점을 두었으며, 주요 내용은 다음과 같습니다:

* **o1 모델 API**: 함수 호출, 개발자 메시지, 구조화된 출력, 비전 기능 추가 ([링크](https://platform.openai.com/docs/models#o1))
* **WebRTC 지원 및 실시간 API 업데이트**: GPT-4o 오디오의 60% 가격 인하 및 GPT-4o mini 지원 ([링크](https://platform.openai.com/docs/guides/realtime))
* **Preference Fine-Tuning을 통한 모델 맞춤화 기능**: 사용자의 선호도에 따라 모델을 조정하는 새로운 기법([링크](https://platform.openai.com/docs/guides/fine-tuning#preference))
* **Go 및 Java SDK의 베타 출시** ([링크](https://platform.openai.com/docs/libraries))

이를 통해 개발자들은 더욱 빠르게 혁신적인 제품을 만들고, 다양한 도메인에서 AI 활용을 확장할 수 있게 되었습니다.

---

### 1. **OpenAI o1: 새로운 고성능 추론 모델**

**OpenAI o1**은 복잡한 다단계 작업을 높은 정확도로 처리할 수 있는 추론 모델로, **이제 API에서 사용이 가능**합니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/22c8ba32-ca0c-4fcb-999e-16ecfffd2cea/image.png)

**o1 모델**은 이전에 출시된 *o1-preview*의 정식 릴리즈 버전으로, 고객 지원 자동화, 공급망 의사 결정 최적화, 복잡한 금융 트렌드 예측과 같은 에이전트 애플리케이션을 구축하는 데 사용될 수 있습니다. (o1-preview를 개선한 **`o1-2024-12-17`** 버전이 발표)

> (참고) **o1 explained**  
> 
> OpenAI는 최근 새로운 AI 모델인 o1을 발표했습니다. 이 모델은 복잡한 문제를 해결하기 위해 내부적으로 '**사고의 사슬(chain of thought)**'을 생성하여 심층적인 추론을 수행합니다. 이를 통해 수학, 코딩, 과학적 추론 등에서 뛰어난 성능을 발휘하며, 복잡한 문제를 해결하는 데 특히 유용합니다.
> 
> * **추론 토큰(reasoning token)**은 모델이 내부적으로 생성하는 '생각'을 위한 특별한 토큰입니다.
>   + 이러한 추론 토큰을 활용하여 모델은 복잡한 문제를 단계별로 분석하고, 최종 응답을 생성하기 전에 심층적인 사고 과정을 거칩니다. 이를 통해 모델의 추론 능력이 향상되며, 복잡한 문제에 대한 정확한 답변을 도출할 수 있습니다.
> * **출력 토큰(output token)**은 모델이 최종적으로 생성하는 응답의 구성 요소입니다.
>   + o1 모델은 응답을 생성하기 전에 내부적으로 추론 토큰을 활용하여 심층적인 사고 과정을 거치므로, 최종 출력 토큰의 품질과 정확성이 향상됩니다.이러한 과정은 모델이 복잡한 문제를 해결하는 데 있어 더 나은 성능을 발휘하도록 돕습니다.

아래는 이해를 돕기 위한 참고 이미지입니다:

![](https://velog.velcdn.com/images/euisuk-chung/post/9782b4e7-8c65-4dfe-a970-5786a8d0e4be/image.png)

> **Image Reference**: reasoning model, Gökhan Gerdan ([Medium Post](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fbimser-tech%2Fopenai-o1-and-how-it-works-0eb270efc341&psig=AOvVaw1skuQuvurpjbWsiXaH3e4B&ust=1734569419132000&source=images&cd=vfe&opi=89978449&ved=0CBcQjhxqFwoTCNDxluKMsIoDFQAAAAAdAAAAABAR))

![](https://velog.velcdn.com/images/euisuk-chung/post/53e9f8c1-be40-46fe-817b-2dd324e3f90a/image.png)

> **Image Reference**: Token Outputs, ArtificialAnlys ([X Post](https://x.com/ArtificialAnlys))

이번 API 릴리스는 함수 호출, 개발자 메시지, 구조화된 출력, 비전 기능을 지원하여 개발자들이 보다 유연하고 강력한 AI 애플리케이션을 개발할 수 있도록 돕습니다. (English Explanation is from OpenAI Blog Post)

* **함수 호출(Function Calling)**: o1 모델은 외부 API나 데이터 소스와의 상호작용을 통해 더욱 복잡한 작업을 자동화할 수 있습니다. 이를 통해 챗봇이 실시간으로 데이터를 조회하거나, 사용자의 요청에 따라 다양한 API를 호출할 수 있습니다. (*Seamlessly connect o1 to external data and APIs.*)
* **구조화된 출력(Structured Outputs)**: JSON Schema와 같은 맞춤형 형식에 맞게 데이터를 출력할 수 있어, 데이터 처리와 통합이 훨씬 더 수월해졌습니다. 복잡한 애플리케이션에서도 사용이 간편합니다. (*Generate responses that reliably adhere to your custom JSON Schema.*)
  
  + 아래처럼 원하는 출력 스키마를 정의해주면,  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/24446003-2450-469c-a757-196a845e46ee/image.png)
  + 해당 조건에 맞게 출력이 나오는 것을 확인할 수 있습니다:  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/d1e97b15-fc15-450a-aca6-eedf97f1cf43/image.png)
* **개발자 메시지(Developer Messages)**: 모델의 행동, 말투, 스타일을 세부적으로 설정할 수 있어, 다양한 컨텍스트에 맞게 모델의 출력을 맞춤화할 수 있습니다. (*Specify instructions or context for the model to follow, such as defining tone, style and other behavioral guidance.*)
* **비전 기능(Vision Capabilities)**: 이미지 기반 추론이 가능해져 과학, 의료, 제조, 코딩 등 다양한 산업 분야에서 활용할 수 있습니다. (*Reason over images to unlock many more applications in science, manufacturing, or coding, where visual inputs matter.*)
  
  + 예를 들어, 텍스트 양식을 스캔하고 오류를 자동으로 검출하는 데 사용될 수 있습니다.  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/dd6730a8-8077-4ed6-945b-066281fc88f9/image.png)
  + 이번 데모에서는 세금 신고 양식의 잘못된 수식과 표준 공제 값을 자동으로 찾아주었으며, 양식의 여러 페이지에서 데이터를 참조해 정확한 계산을 수행했습니다. (Function 기능 사용)  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/4e028f3c-4d83-42ba-b8cb-632727dd642e/image.png)
  + 이 기능은 문서 처리, 시각적 검토, 오류 검출 등 다양한 실무에 즉시 적용할 수 있습니다.

* **낮은 지연 시간(Lower latency)**: 평균적으로 o1-preview보다 60% 적은 토큰을 사용하면서도 빠르고 정확한 응답을 제공합니다. 실시간 애플리케이션에서도 효율적으로 작동합니다. (*o1 uses on average 60% fewer reasoning tokens than o1-preview for a given request.*)

추가로, **`reasoning_effort`** API 파라미터를 통해 모델의 추론 수준을 제어할 수 있습니다.

이 기능은 추론의 깊이를 조정함으로써 빠른 응답이 필요한 경우와 복잡한 작업이 필요한 경우 모두를 지원합니다.

**벤치마크 성능**:

* `o1-2024-12-17`은 다양한 벤치마크 테스트에서 이전 모델을 능가하는 성능을 기록하며, 특히 수학, 코딩 및 시각적 영역에서 큰 개선을 보였습니다.  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/bcbc1889-0f9e-4874-b07c-b3b20294402b/image.png)
* 각각의 기능들에 대해서도 테스트를 수행해보았다고 합니다.  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/11cc52fb-a485-4569-939d-46130a6e70e5/image.png)

![](https://velog.velcdn.com/images/euisuk-chung/post/2109191c-c730-4169-9a70-43886f199dfa/image.png)

> 위 표에서 SO는 Strcutured Output의 줄임말임\*

이러한 성능 개선을 통해 o1 모델은 복잡한 지식 기반 작업과 실시간 시뮬레이션, 코딩 및 학술 연구에 활용될 수 있습니다. 앞으로 다양한 분야의 AI 개발에 중요한 도구로 자리잡을 것으로 보입니다.

---

### 2. **실시간 API 개선**

![](https://velog.velcdn.com/images/euisuk-chung/post/aa74b929-3cb6-40dc-84b4-1be45fd3d19e/image.png)

**실시간 API**는 낮은 지연 시간을 바탕으로 인터랙티브한 경험을 제공합니다. `음성 비서`, `실시간 번역기`, `가상 튜터`와 같은 응용 프로그램에서 유용하게 사용할 수 있으며, 다음과 같은 새로운 기능이 추가되었습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/b70756a0-c78d-4adb-8876-711888ad5059/image.png)

> 출처 : 실시간 API(<https://platform.openai.com/docs/guides/realtime>)

#### **WebRTC 지원**

**WebRTC**(Web Real-Time Communication, *웹 애플리케이션과 사이트가 중간자 없이 브라우저 간에 오디오나 영상 미디어를 포착하고 마음대로 스트림 할 뿐 아니라, 임의의 데이터도 교환할 수 있도록 하는 기술*) 지원을 통해 실시간 음성 및 데이터를 처리하는 애플리케이션을 더욱 쉽게 구축할 수 있습니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/e1297378-cc6a-41b0-8436-73d49212e455/image.png)

> 출처 : 실시간 API(<https://platform.openai.com/docs/guides/realtime>)

브라우저, 모바일, IoT 디바이스 등 다양한 환경에서 실시간 음성 상호작용이 가능해졌습니다. WebRTC는 연결 성능이 뛰어나면서도 안정적인 음성 스트리밍을 지원합니다.

> (참고) 본 블로그의 데모 코드는 [OpenAI 블로그](https://openai.com/index/o1-and-new-tools-for-developers/)의 Demo 코드를 사용하고 있습니다. 영상의 Demo 코드는 이미지로 첨부드립니다.  
> 
> ![](https://velog.velcdn.com/images/euisuk-chung/post/4828e706-2d87-41a7-87da-8e882bdd2784/image.png)
> 
> * 위 코드를 간략하게 설명하면 HTML 기반 WebRTC(Web Real-Time Communication) 코드로, PeerConnection(Peer-to-Peer) 연결을 통해 중간 서버를 거치지 않고 브라우저 간 직접 통신을 가능하게 합니다.  
>   
>   ![](https://velog.velcdn.com/images/euisuk-chung/post/bb4f0814-9af0-439e-98dc-87581c94344d/image.png)

* 또한, 다른 데모에서는 '**아기 사슴 장난감**'에 WebRTC를 적용한 소형 하드웨어 장치를 부착하여 실시간 대화 가능한 인형을 만들었습니다 ㅋㅋ (커엽)  
  
  ![](https://velog.velcdn.com/images/euisuk-chung/post/36e4536e-91f4-4942-b075-b14506abd98d/image.png)
  + 사슴 인형는 동전 크기의 마이크로컨트롤러를 사용해 AI 모델과 연결되었으며, 간단한 설정만으로도 대화형 기능을 작동시킬 수 있습니다. 이러한 사례는 웨어러블 디바이스, 가정용 AI 비서, 스마트 카메라 등 다양한 하드웨어 활용 가능성을 보여줍니다.  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/f5013ea5-5215-451d-adc0-f2c757985d43/image.png)

**예시 코드 (JavaScript):**

```
async function createRealtimeSession(localStream, remoteAudioEl, token) {
    const pc = new RTCPeerConnection();
    pc.ontrack = e => remoteAudioEl.srcObject = e.streams[0];
    pc.addTrack(localStream.getTracks()[0]);
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);
    const resp = await fetch('https://api.openai.com/v1/realtime', {
        method: 'POST',
        headers: { Authorization: `Bearer ${token}`, 'Content-Type': 'application/sdp' },
        body: offer.sdp
    });
    await pc.setRemoteDescription({ type: 'answer', sdp: await resp.text() });
    return pc;
}
```
#### **새로운 GPT-4o 및 GPT-4o mini 실시간 스냅샷 버전**

OpenAI는 gpt-4o를 `gpt-4o-realtime-preview-2024-12-17`으로 실시간 API 베타 버전으로 출시했습니다.

* 이 업데이트는 개선된 음성 품질, 특히 숫자 입력의 신뢰성을 높이고, 비용을 절감합니다.
* 효율성 개선 덕분에 오디오 토큰 가격을 `60%` 낮춰 입력 토큰당 `$40/1M`, 출력 토큰당 `$80/1M`으로 인하했습니다.
* 캐시된 오디오 입력 비용은 87.5% 감소해 입력 토큰당 `$2.50/1M`으로 낮아졌습니다.

또한 GPT-4o mini를 `gpt-4o-mini-realtime-preview-2024-12-17`으로 실시간 API 베타에 추가했습니다.

* GPT-4o mini는 가장 비용 효율적인 소형 모델로, GPT-4o와 동일한 풍부한 음성 경험을 제공합니다.
* GPT-4o mini의 오디오 가격은 입력 토큰당 `$10/1M`, 출력 토큰당 `$20/1M`입니다.
* 텍스트 토큰 가격은 입력 토큰당 `$0.60/1M`, 출력 토큰당 `$2.40/1M`으로 설정되었으며, 캐시된 오디오 및 텍스트 모두 토큰당 `$0.30/1M`의 비용이 발생합니다.

이 스냅샷은 실시간 API와 Chat Completions API에서 각각 `gpt-4o-audio-preview-2024-12-17` 및 `gpt-4o-mini-audio-preview-2024-12-17`으로 제공됩니다.

#### **응답 제어 기능 강화**

OpenAI는 실시간 API를 통해 뛰어난 음성 기반 경험을 제공하기 위해 다음 기능을 출시했습니다:

* **동시 외부 응답 처리**: 사용자 음성 상호작용을 방해하지 않고 콘텐츠 분류나 모더레이션 같은 백그라운드 작업을 실행할 수 있습니다.
* **사용자 정의 입력 컨텍스트**: 모델 입력으로 포함할 대화를 지정합니다. 예를 들어, 사용자의 마지막 발언만 확인하거나 과거 응답을 다시 활용할 수 있습니다.
* **응답 타이밍 제어**: 서버 측 음성 활동 감지(VAD)를 사용하여 응답을 자동으로 트리거하지 않고, 계정 세부 정보 등 필요한 데이터를 수집한 후 수동으로 음성 응답을 시작할 수 있습니다.
* **세션 길이 증가**: 최대 세션 길이가 15분에서 30분으로 늘어났습니다.

---

### 3. **Preference Fine-Tuning: 사용자 맞춤 모델 학습**

**Preference Fine-Tuning**은 주관적인 평가 작업에 특화된 **새로운 모델 학습 기법**입니다.

![](https://velog.velcdn.com/images/euisuk-chung/post/fed7fcb2-fe8b-4eef-bd81-c2059cb461d4/image.png)

* 기존의 `Supervised Fine-Tuning`과 `Preference Fine-Tuning`은 다음과 같은 차이점을 가집니다.

#### **Supervised Fine-Tuning**

1. **목표**
   * Supervised Fine-Tuning은 명확히 정답이 있는 작업에서 입력과 출력 쌍을 학습하도록 설계되었습니다.
   * 예를 들어, 기계 번역, 코드 포맷팅, 또는 수학 문제 풀이처럼 정답이 명확하게 정의된 문제를 해결하는 데 적합합니다.
2. **학습 데이터**
   * 입력 데이터에 대해 명시적인 출력 정답을 제공하는 데이터셋이 필요합니다.
   * 예를 들어, 영어 문장을 입력하면 번역된 한국어 문장을 출력하는 데이터쌍이 필요합니다.
3. **적용 분야**
   * 번역 시스템: 영어-한국어 번역, 코드 변환(예: Python에서 Java로 변환).
   * 정확한 답변을 요구하는 QA 시스템.
   * 수학 공식 계산기 등.

#### **Preference Fine-Tuning**

1. **목표**
   
   * Preference Fine-Tuning은 사용자의 선호도를 모델에 반영하는 데 중점을 둡니다.
   * 주어진 여러 출력 중에서 선호하는 응답과 그렇지 않은 응답을 모델이 학습하도록 하여 더 바람직한 출력 결과를 도출합니다.
   * 이는 창의적 글쓰기, 콘텐츠 필터링 등 주관적 판단이 중요한 작업에서 유용합니다.
2. **학습 데이터**
   
   * A/B 테스트 형식으로 선호하는 출력(A)과 비선호하는 출력(B)을 짝지어 학습 데이터를 구성합니다.
   * 모델은 A와 B의 차이점을 학습하여 선호도 기반 출력을 생성하도록 조정됩니다.  
     
     ![](https://velog.velcdn.com/images/euisuk-chung/post/1804cf16-fef7-426d-a930-72eecfc1cb20/image.png)  
     
     ![](https://velog.velcdn.com/images/euisuk-chung/post/47fabd79-00ec-4d82-869c-f5c71e868425/image.png)
3. **적용 분야**
   
   * 창의적 글쓰기: 더 매끄럽고 인간적인 문체로 글을 작성.
   * 요약 작업: 사용자의 요구에 맞는 정보를 강조하는 요약.
   * 고객 지원: 고객의 톤과 스타일에 맞춘 답변 생성.
   * 콘텐츠 필터링: 특정 기준에 따라 부적합한 콘텐츠를 제거.

> **차이점**
> 
> * **Supervised Fine-Tuning**은 정답이 명확히 정의된 작업에 적합하며, 모델의 출력 정확도를 높이는 데 사용됩니다.
> * **Preference Fine-Tuning**은 정답이 아닌 '선호'를 반영하는 것이 핵심으로, 결과물의 주관적 품질을 높이고, 사용자 요구에 맞춘 출력을 제공합니다.

* **정리**
  
  | **특징** | **Supervised Fine-Tuning** | **Preference Fine-Tuning** |
  | --- | --- | --- |
  | **목표** | 정답 출력을 학습 | 선호하는 출력을 강화하고 비선호 출력 감소 |
  | **학습 데이터** | 입력-출력 쌍 | 선호/비선호 응답 쌍 (A/B 테스트 등) |
  | **주요 적용 분야** | 코드 포맷, 정답이 명확한 작업 | 창의적 글쓰기, 요약 등 주관적 평가 작업 |

* 이 방법을 사용하면 사용자의 선호에 맞게 모델을 미세 조정할 수 있어 창의적인 글쓰기, 요약, 번역 등에서 더 나은 결과를 기대할 수 있다고 강조합니다.
  
  + 예를 들어, **Rogo AI**(<https://rogo.ai/>)는 금융 분석가를 위한 AI 어시스턴트를 구축하면서 `Preference Fine-Tuning`을 활용했습니다. Rogo AI의 목표는 사용자의 질의 내용을 재작성 및 리팩터링해 더욱 정확하고 유의미한 답변을 제공하는 것이었습니다.
  + 초기에는 Supervised Fine-Tuning을 시도했지만, 성능이 기본 모델을 넘어서지 못했습니다. 그러나 Preference Fine-Tuning을 적용한 결과, 내부 벤치마크 정확도가 75%에서 80% 이상으로 개선되었습니다.  
    
    ![](https://velog.velcdn.com/images/euisuk-chung/post/dd345026-9ec5-4d9a-808d-457a966dbb75/image.png)
  + 이 사례를 통해 Preference Fine-Tuning이 사용자 피드백이 중요한 분야에서 큰 효과를 발휘한다는 것을 확인할 수 있으며, 고객 지원, 카피라이팅, 창의적 글쓰기 등 다양한 영역에서의 활용이 기대됩니다.

---

### 4. **Go 및 Java SDK 베타 출시**

OpenAI는 Python, Node.js에 이어 **Go**와 **Java**를 지원하는 공식 SDK를 새롭게 발표했습니다.

* **Go SDK**: 동시성 처리와 확장성이 뛰어나, 서버 애플리케이션 개발에 최적화되었습니다.
* **Java SDK**: 엔터프라이즈급 애플리케이션을 위한 강력한 타입 시스템과 유틸리티를 제공합니다.

**Go SDK 간단한 예시**:

다음은 Go SDK를 사용해 AI 모델과 상호작용하는 간단한 애플리케이션 예제입니다. 이 코드는 사용자가 입력한 텍스트를 바탕으로 하이쿠를 생성하는 프로그램입니다.

```
package main

import (
    "context"
    "fmt"
    "github.com/openai/go-openai"
)

func main() {
    client := openai.NewClient()
    ctx := context.Background()

    prompt := "Write me a haiku about Golang."

    completion, err := client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
        Messages: openai.F([]openai.ChatCompletionMessageParamUnion{
            openai.UserMessage(prompt),
        }),
        Model: openai.F(openai.ChatModelGPT4o),
    })

    if err != nil {
        fmt.Println("Error:", err)
        return
    }

    fmt.Println("Generated Haiku:", completion.Choices[0].Message.Content)
}
```

**Java SDK 간단한 예시**:

Java SDK를 활용해 간단한 챗봇 애플리케이션을 만드는 예제입니다. 사용자의 질문에 AI가 응답을 반환합니다.

```
import com.openai.api.OpenAi;
import com.openai.api.models.ChatCompletion;
import com.openai.api.models.ChatCompletionMessage;

import java.util.Arrays;

public class OpenAIExample {
    public static void main(String[] args) {
        OpenAi openAi = new OpenAi(System.getenv("OPENAI_API_KEY"));

        ChatCompletion completion = openAi.chatCompletions().create(
                chatCompletion -> chatCompletion
                        .model("gpt-4o")
                        .messages(Arrays.asList(
                                new ChatCompletionMessage("user", "Explain the concept of multithreading in Java.")))
        );

        System.out.println("Response: " + completion.choices().get(0).message().content());
    }
}
```

이제 다양한 언어 환경에서 OpenAI의 강력한 기능을 손쉽게 활용할 수 있습니다. 위의 예시를 기반으로 개발자들은 다양한 애플리케이션을 빠르게 구축할 수 있으며, 서버 애플리케이션, 대화형 챗봇 등 여러 활용 사례에 적용할 수 있습니다.

---

결론
--

이번 업데이트를 통해 OpenAI는 AI 개발자들에게 더욱 강력하고 유연한 환경을 제공합니다. 새로운 **o1 모델**, **WebRTC 기반 실시간 API**, **Preference Fine-Tuning**, **Go 및 Java SDK** 개선을 통해 다양한 혁신적 AI 애플리케이션 구축이 가능해졌습니다.

더 많은 정보와 가이드는 OpenAI **API 문서**에서 확인할 수 있습니다. OpenAI의 이러한 노력은 AI 개발의 문턱을 낮추고, 새로운 기술 도입을 더욱 가속화할 것입니다.

그리고 2024 DevDay에 유튜브 영상도 올렸다고 하는군요!! 그것도 리뷰하러 가야겠습니다!! 그럼 이만!! 총총총🏃‍♂️

내일도 기대해주세요!

